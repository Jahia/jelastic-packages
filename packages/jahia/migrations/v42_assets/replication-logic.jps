type: update
version: 1.5
id: gluster_cluster
baseUrl: https://raw.githubusercontent.com/jelastic-jps/glusterfs/master
description:
  short: GlusterFS Cluster Replication Logic
name: GlusterFS Cluster Replication Logic

permanent: true

targetNodes:
  nodeGroup: storage

globals:
  replicatedPath: /data
  replicatedVolume: data

onAfterStart:
  - cmd[storage]: mount -a; exportfs -ra;

onAfterServiceScaleOut[storage]:
  - forEach(newnode:event.response.nodes):
      - addBrickToVolume:
          address: ${@newnode.address}
          id: ${@newnode.id}

onBeforeScaleIn[storage]:
  - forEach(removednode:event.response.nodes):
      removeBrickFromVolume:
        address: ${@removednode.address}
        id: ${@removednode.id}

onAfterRedeployContainer[storage]:
  - cmd[storage]: /bin/systemctl start glusterd.service && /bin/systemctl enable glusterd.service;
  - cmd[${nodes.storage.master.id}]: set -o pipefail; gluster --log-level=TRACE --log-file=/dev/stdout volume start ${globals.replicatedVolume} force | tee -a /var/log/glusterfs/glusterd.log;
  - cmd[${nodes.storage.master.id}]: gluster volume set ${globals.replicatedVolume} network.ping-timeout 3;
  - getnodes: ${event.response.responses.join(nodeid,)}
  - forEach(node:response.nodes):
      - mountDirectoryToVolume:
          id: ${@node.id}
          address: ${@node.address}
      - addAutoMount:
          id: ${@node.id}
          address: ${@node.address}

onAfterClone:
  if ('${fn.compareEngine(6.1.1):}' < 0):
    reinstallGlusterFSLogic:
      clonedEnv: ${event.response.env.envName}

onAfterCloneAllNodes:
  reinstallGlusterFSLogic:
    clonedEnv: ${event.params.targetenv}
      
onBeforeMigrate:
  - umountFuze: ${nodes.storage.join(id,)}
  - cmd[storage]: sed -i '/glusterfs/d' /etc/fstab; rm -rf ${globals.replicatedPath}

onAfterMigrate:
  initiateGlusterFSCluster

onBeforeStop:
  - umountFuze: ${nodes.storage.join(id,)}

actions:
  initiateGlusterFSCluster:
    - umountFuze: ${nodes.storage.join(id,)}
    - environment.file.read:
        nodeId: ${nodes.storage.master.id}
        path: /etc/exports
        user: root
    - environment.file.write:
        nodeGroup: storage
        path: /etc/exports
        user: root
        body: ${response.body}
    - forEach(clusternode:nodes.storage):
        cleanupNode:
          id: ${@clusternode.id}
    - forEach(clusternode:nodes.storage):
        enableGlusterFS:
          id: ${@clusternode.id}   
    - prepareVolumeBricks          
    - initiateVolume             
    - forEach(clusternode:nodes.storage):          
        - mountDirectoryToVolume:    
            id: ${@clusternode.id}   
            address: ${@clusternode.address}      
        - addAutoMount:    
            id: ${@clusternode.id}   
            address: ${@clusternode.address}
        - fixExistingMounts:    
            id: ${@clusternode.id}  
        - checkStatus:    
            id: ${@clusternode.id}  

  cleanupNode:
    - cmd[${this.id}]: |-
        service glusterd stop; GLUSTER_PROCESS=$(ps aux|grep gluster|grep -v grep|awk '{print $2}'); 
        [ -n "${GLUSTER_PROCESS}" ] && kill -9 ${GLUSTER_PROCESS}; sed -i '/glusterfs/d' /etc/fstab; rm -rf /var/lib/glusterd/{vols,ss_brick,peers};
      user: root
      
  enableGlusterFS:              
    - cmd[${this.id}]: |-
        /bin/systemctl enable glusterd.service; /bin/systemctl start glusterd.service; mkdir -p /glustervolume ${globals.replicatedPath}; 
        echo -e "/glustervolume\n/var/lib/glusterd/\n/var/log/glusterfs\n${globals.replicatedPath}\n/etc/exports" >> /etc/jelastic/redeploy.conf;
        sed -i '/^$/d' /etc/exports;
      user: root

  addPeer:
    - cmd[${nodes.storage.master.id}]: |-
        counter=0
        while true
        do
            counter2=0
            while [[ "$counter2" -le 20 ]]; do sleep 1; gluster peer status && break; counter2=$((counter2+1));done
            set -o pipefail; gluster --log-file=/dev/stdout peer probe ${this.address} | tee -a /var/log/glusterfs/glusterd.log; res=$?
            [[ "$res" -eq 0 ]] && exit 0
            [[ "$counter" -gt 10 ]] && { echo "peer is not available"; exit 1; }
            counter=$((counter+1))
        done

  prepareVolumeBricks: 
    forEach(clusternode:nodes.storage):
      if (${@clusternode.id} != ${nodes.storage.master.id}):
        addPeer:
            address: ${@clusternode.address}

  initiateVolume:
    - cmd[${nodes.storage.master.id}]: |-
        let "NUMBER_OF_BRICKS=$(gluster peer status |grep 'Number of Peers'|awk '{print $4}') + 1"; 
        BRICKS_ADDRESSES=$(gluster peer status|grep Hostname| awk '{print $2}'); BRICKS_STRING="${nodes.storage.master.address}:/glustervolume"; 
        for i in ${BRICKS_ADDRESSES}; do BRICKS_STRING="${BRICKS_STRING} ${i}:/glustervolume"; done;
        counter=1
        while true
        do
            echo "Volume create operation - attempt ${counter}" | tee -a /var/log/glusterfs/glusterd.log;
            gluster volume create ${globals.replicatedVolume} replica ${NUMBER_OF_BRICKS} transport tcp ${BRICKS_STRING} force | tee -a /var/log/glusterfs/glusterd.log; res=$?
            [[ "$res" -eq 0 ]] && { echo "Volume create operation succeeded" | tee -a /var/log/glusterfs/glusterd.log; break; } 
            [[ "$counter" -gt 3 ]] && { echo "Volume create operation failed" | tee -a /var/log/glusterfs/glusterd.log; exit 1; }
            sleep 1
            counter=$((counter+1))
        done
        gluster volume start ${globals.replicatedVolume};
        gluster volume set ${globals.replicatedVolume} network.ping-timeout 3;
        if [ -f /etc/jelastic/clients/${globals.replicatedVolume} ]; then gluster volume set ${globals.replicatedVolume} auth.allow $(cat /etc/jelastic/clients/data|tr ' ' ','); else gluster volume set ${globals.replicatedVolume} auth.allow 127.0.0.1; fi;
      user: root
      
  mountDirectoryToVolume:
    - cmd[${this.id}]: |-
        dataBackupDir=$(mktemp -d)
        [ -n "$(ls -A /${globals.replicatedPath})" ] && { shopt -s dotglob; mv ${globals.replicatedPath}/* ${dataBackupDir}/; shopt -u dotglob; };
        mount.glusterfs localhost:/${globals.replicatedVolume} ${globals.replicatedPath}
        chmod 777 ${globals.replicatedPath}
        [ -n "$(ls -A /${dataBackupDir})" ] && { shopt -s dotglob; mv ${dataBackupDir}/* ${globals.replicatedPath}/; shopt -u dotglob; };
        rm -rf ${dataBackupDir}
      user: root
      
  addNewNodeToVolume:
    - cmd[${nodes.storage.master.id}]: |-
        let "NUMBER_OF_BRICKS=$(gluster peer status |grep 'Number of Peers'|awk '{print $4}') + 1"; 
        counter=0
        while true
        do
        set -o pipefail; gluster volume add-brick ${globals.replicatedVolume} replica ${NUMBER_OF_BRICKS} ${this.address}:/glustervolume force 2>&1 | tee -a /var/log/glusterfs/glusterd.log; res=$?
        [[ "$res" -eq 0 ]] && { echo "Volume add-brick succeeded"; exit 0; } 
        [[ "$counter" -gt 10 ]] && { echo "Volume add-brick failed"; exit 1; }
        sleep 1
        counter=$((counter+1))
        done
      user: root

  addAutoMount:
    - cmd[${this.id}]: |-
        sed -i '/glusterfs/d' /etc/fstab;
        echo "localhost:/${globals.replicatedVolume}  ${globals.replicatedPath}   glusterfs   defaults,_netdev,noauto,x-systemd.automount  0 0" >> /etc/fstab;
        
  addBrickToVolume:
    - addPeerToAuthAllow:
        address: ${this.address}
    - enableGlusterFS:
        id: ${this.id}
    - addPeer:
        address: ${this.address}
    - addNewNodeToVolume:
        address: ${this.address}
    - mountDirectoryToVolume:
        id: ${this.id}
        address: ${this.address}
    - addAutoMount:
        id: ${this.id}
        address: ${this.address}
    - fixExistingMounts:
        id: ${this.id}
    - removePeerFromAuthAllow:
        address: ${this.address}
        
  fixExistingMounts:
    - cmd[${this.id}]: |-
        LINE_NUMBER=0
        while read LINE; do let LINE_NUMBER++; FSID=$(cat /proc/sys/kernel/random/uuid); grep -q '^\"${globals.replicatedPath}' <<< $LINE && sed -i "${LINE_NUMBER}s/)$/,fsid=${FSID})/" /etc/exports || true; done < /etc/exports;
        exportfs -ra;
  checkStatus:
    - cmd[${this.id}]: |-
        gluster volume status;
        
  removeBrickFromVolume:
    - umountFuze: ${this.id}
    - cmd[${nodes.storage.master.id}]: |-
        NUMBER_OF_BRICKS=$(gluster peer status |grep 'Number of Peers'|awk '{print $4}'); 
        yes 2>/dev/null | gluster volume remove-brick ${globals.replicatedVolume} replica ${NUMBER_OF_BRICKS} ${this.address}:/glustervolume force; 
        gluster peer detach ${this.address} && sleep 1;
        
  umountFuze:
    - cmd[${this}]: systemctl stop $(systemd-escape -p --suffix=automount ${globals.replicatedPath}); umount -l ${globals.replicatedPath} || true;
    
  addPeerToAuthAllow:
    - cmd[${nodes.storage.master.id}]: |-
        CURRENT_AUTH_ALLOW=$(gluster volume info ${globals.replicatedVolume}|grep 'auth.allow'|awk '{print $2}');
        gluster volume set ${globals.replicatedVolume} auth.allow ${CURRENT_AUTH_ALLOW},${this.address}
        
  removePeerFromAuthAllow:
    - cmd[${nodes.storage.master.id}]: |-
        AUTH_ALLOW=$(gluster volume info ${globals.replicatedVolume}|grep 'auth.allow'|awk '{print $2}'|sed "s/,${this.address}//g");
        gluster volume set ${globals.replicatedVolume} auth.allow ${AUTH_ALLOW}
  
  reinstallGlusterFSLogic:
    install:
      jps: ${baseUrl}/replication-logic.jps?_r=${fn.random}
      envName: ${this.clonedEnv}
      settings:
        nodeGroup: storage
        replicatedPath: /data
        replicatedVolume: data

  getnodes:
    ids: ${this}
    script: |
        ids = String(ids).split(',');

        let resp = api.env.control.GetEnvInfo('${env.envName}', session);
        if (resp.result != 0) return resp;

        let nodes = resp.nodes.filter(node => (ids.includes(String(node.id))));

        return { result: 0, nodes: nodes };
