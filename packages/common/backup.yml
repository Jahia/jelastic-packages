---
type: update
version: 1.5
name: Jahia - Snapshots an environment and stores it in the appropriate object storage
logo: ../../assets/common/jahia-logo-70x70.png
id: jahia-backup-to-bucket

globals:
  timestamp: ${settings.timestamp}
  argTimestamp: '-t "${globals.timestamp}"'
  logAction: "${settings.backtype} Backup"
  logsPath: "/var/log/jelastic-packages/backup.log"
  lowerDBConnection: 100000
  jahiaCustomConfFiles:
    /data/digital-factory-data/repository/indexing_configuration.xml

mixins:
  - ../../mixins/common.yml
  - ../../mixins/jahia.yml
  - ../../mixins/elasticsearch.yml
  - ../../mixins/mariadb.yml

onInstall:
  - getEnvCloudProvider
  - getEnvRegion
  - getBackrestAwsAccessKey
  - if ("${globals.provider}" == "ovh"):
      getOvhBackupCredentials
  - else: # aws
      setGlobals:
          __secret__backupAccessKeyId: ${globals.__secret__backrestAwsAccessKeyId}
          __secret__backupSecretAccessKey: ${globals.__secret__backrestAwsSecretAccessKey}
  - setupBackupRCloneConfig:
      provider: ${globals.provider}
      region: ${globals.region}

  - cmd[${nodes.cp.first.id}]: |-
      cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST)
      echo $cluster_role
  - setGlobals:
      clusterRole: ${response.out}

  - getBucketName:
      region: ${globals.region}
      uid: ${env.uid}
      clusterRole: ${globals.clusterRole}
      cloudProvider: ${globals.provider}

  - if ('${globals.timestamp}' == ''):
      - setTimestamp
  - setGlobals:
      backupName: ${settings.backup_name}_${globals.timestamp}_${settings.backtype}

  - if (nodes.proc):  # Jahia
      - chooseTheBestDatabaseNodeToBackup
      - clearJelasticLogs:
          target: proc
          user: tomcat
          logsPath: ${globals.logsPath}
      - clearJelasticLogs:
          target: ${globals.dbBackupNode}
          user: mysql
          logsPath: ${globals.logsPath}
      - clearJelasticLogs:
          target: bl
          user: haproxy
          logsPath: ${globals.logsPath}
      - installBackupTools:
          target: ${globals.dbBackupNode}
          logAction: ${globals.logAction}
      - installBackupTools:
          target: proc
          logAction: ${globals.logAction}
      - if (nodes.bl):  # Haproxy
          - installBackupTools:
              target: bl
              logAction: ${globals.logAction}
      - backupJahia
      - backupMariadb: ${globals.dbBackupNode}
      - if (nodes.bl):  # Haproxy
          - backupHaproxy
      - addMetadata: proc
      - rotateBackups: proc
  - else:  # Jcustomer
      - clearJelasticLogs:
          target: ${nodes.cp.first.id}
          user: root
          logsPath: ${globals.logsPath}
      - installBackupTools:
          target: ${nodes.cp.first.id}
          logAction: ${globals.logAction}
      - backupElasticsearch
      - backupJcustomer
      - addMetadata: ${nodes.cp.first.id}
      - rotateBackups: ${nodes.cp.first.id}
      - rotateElasticsearchBackup

actions:
  setTimestamp:
    - script: |
          var diso = new Date()
          var m = diso.getMonth() + 1
          var H = diso.getHours()
          var M = diso.getMinutes()
          var d = diso.getDate()
          if(m.toString().length != 2) {m = "0" + m}
          if(H.toString().length != 2) {H = "0" + H}
          if(M.toString().length != 2) {M = "0" + M}
          if(d.toString().length != 2) {d = "0" + d}
          var timestamp = diso.getFullYear() + "-" + m + "-" + d + "T" + H + ":" + M + ":00"
          return {'result': 0, 'timestamp': timestamp}
    - setGlobals:
        timestamp: ${response.timestamp}
        argTimestamp: '-t "${response.timestamp}"'

  backupJahia:
    - enableFullReadOnlyOnCluster
    - cmd[${nodes.sqldb.first.id}]: |-
        mysql jahia -e "OPTIMIZE TABLE JR_J_JOURNAL" >>${globals.logsPath} 2>&1
        mysql jahia -sNe "select REVISION_ID from JR_J_LOCAL_REVISIONS where JOURNAL_ID='processing.${nodes.proc.first.id}'"
    - cmd[proc]: |-
        python3 -c 'with open("/data/digital-factory-data/repository/revisionNode", "wb") as file: file.write((${response.out}).to_bytes(8, byteorder="big", signed=False))'
        touch /data/digital-factory-data/backup-with-JCR-index
        tarfile=digital-factory-data.tar.gz
        cd jelastic_backup
        rm -f $tarfile
        tar -cz -H posix -f $tarfile -C / data/digital-factory-data/ 2>>${globals.logsPath} || echo "ERROR"
    - set:
        tarCmdRes: "${response.out}"
    - disableFullReadOnlyOnCluster
    - if ("${tarCmdRes}" == "ERROR"):
        return:
          type: error
          message: "An error occurred during Jahia backup (data)"
    - cmd [proc]: |-
        tarfile=jahia-custom-conf-files.tar
        cd jelastic_backup
        rm -f $tarfile
        tar -cf $tarfile --files-from /dev/null
        # keep existing file only
        files_to_save=($(for f in ${globals.jahiaCustomConfFiles}; do [ -f $f ] && echo -n "$f "; done))
        if [ ${#files_to_save[@]} -eq 0 ]; then
          rm -f $tarfile
        else
          tar -uPf $tarfile ${files_to_save[@]} 2>>${globals.logsPath} || { echo "ERROR" >&2; exit 0; }
          gzip $tarfile 2>>${globals.logsPath} || echo "ERROR" >&2
        fi
    - if ("${response.errOut}" != ""):
        return:
          type: error
          message: "An error occurred during jahia backup (custom configuration files)"
    - cmd [proc]: |-
        file=jahia-env-vars
        cd jelastic_backup
        rm -f $file $file.gz
        grep -E "^(jahia|tomcat)_cfg_" /.jelenv | grep -v "jahia_cfg_mvnPath" > $file 2>>${globals.logsPath} || echo "ERROR" >&2
        gzip $file 2>>${globals.logsPath} || echo "ERROR" >&2
    - if ("${response.errOut}" != ""):
        return:
          type: error
          message: "An error occurred during jahia backup (environment variables)"
    - cmd [proc]: |-
        tarfile=digital-factory-config.tar.gz
        cd jelastic_backup
        rm -f $tarfile
        tar -cz -H posix -f digital-factory-config.tar.gz -C / opt/tomcat/conf/digital-factory-config/ 2>>${globals.logsPath} || echo "ERROR" >&2
    - if ("${response.errOut}" != ""):
        return:
          type: error
          message: "An error occurred during jahia backup (config)"
    - cmd [proc]: |-
        __secret__ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
        __secret__SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
        export RCLONE_S3_ACCESS_KEY_ID="$__secret__ACCESS_KEY_ID"
        export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__SECRET_ACCESS_KEY"
        archives=(
          digital-factory-data.tar.gz
          digital-factory-config.tar.gz
          jahia-env-vars.gz
          jahia-custom-conf-files.tar.gz
        )
        cd jelastic_backup
        rclone copy . $(for file in ${archives[@]}; do echo -n "--include=$file "; done) backup:${globals.bucketName}/${globals.backupName}/ 2>>${globals.logsPath} || { echo "ERROR" >&2; exit 0; }
        rm -f ${archives[@]}
    - if ("${response.errOut}" != ""):
        return:
          type: error
          message: "An error occurred during jahia backup (archives upload)"

  addMetadata:
    - cmd [${this}]: |-
        __secret__AWS_ACCESS_KEY_ID="${globals.__secret__backrestAwsAccessKeyId}"
        __secret__AWS_SECRET_ACCESS_KEY="${globals.__secret__backrestAwsSecretAccessKey}"
        export AWS_ACCESS_KEY_ID="$__secret__AWS_ACCESS_KEY_ID"
        export AWS_SECRET_ACCESS_KEY="$__secret__AWS_SECRET_ACCESS_KEY"
        __secret__RCLONE_ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
        __secret__RCLONE_SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
        export RCLONE_S3_ACCESS_KEY_ID="$__secret__RCLONE_ACCESS_KEY_ID"
        export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__RCLONE_SECRET_ACCESS_KEY"
        region=${globals.region}
        if [ "${globals.provider}" != "aws" ]; then
          aws_region='eu-west-1'
        else
          aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST);
        fi;
        export AWS_DEFAULT_REGION="$aws_region"

        cd jelastic_backup
        if [ "${this}" == "proc" ]; then  # Jahia
          size=$(rclone size --json backup:${globals.bucketName}/${globals.backupName} | jq '.bytes')
        else  #jCustomer
          size=1
        fi
        lock_sec_max=300
        meta_bucket_name="jc${globals.clusterRole}backupmetadata"
        lockfile="$(echo ${globals.bucketName} | sed -r 's/^[[:alpha:]]+([[:digit:]]+).+$/\1/')_backup_metadata.json.lock"
        getLock(){
          if [ -z "$1" ]; then
            locked_until=$(date -d "+${lock_sec_max}sec" +"%s")
          else
            locked_until=$1
          fi
          echo "${envName} $(date -d "+${lock_sec_max}sec" +"%s")" > /tmp/${lockfile}
          if (( $(( ${locked_until} - $(date +"%s") )) > 0 )); then
            rclone copy --ignore-existing --error-on-no-transfer /tmp/${lockfile} backup:${meta_bucket_name} 2>&1
            if [ $? -ne 0 ]; then
              read locker locked_until < <(RCLONE_S3_REGION=eu-west-1 rclone cat backup:${meta_bucket_name}/${lockfile})
              echo "$(date) - waiting for ${locker} to release his lock (max to $(date -d @${locked_until}))" >> ${globals.logsPath}
              sleep 1
              getLock ${locked_until}
            else
              echo "$(date) - the lock file is uploaded" >> ${globals.logsPath}
            fi
          else
            echo "$(date) - we take control from now" >> ${globals.logsPath}
            rclone copy /tmp/${lockfile} backup:${meta_bucket_name} 2>&1
          fi
        }
        getLock
        python3 backrest.py -a addmeta --bucketname ${globals.bucketName} --backupname ${settings.backup_name} ${globals.argTimestamp} -m ${settings.backtype} --displayname '${env.displayName}' --size $size 2>>${globals.logsPath} || exit 1
        rclone delete backup:${meta_bucket_name}/${lockfile} 2>&1
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred during backup's metadata definition. ${response.errOut}"

  rotateBackups:
    if ("${settings.backtype}" == "auto"):
      - cmd [${this}]: |-
          __secret__ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
          __secret__SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
          export RCLONE_S3_ACCESS_KEY_ID="$__secret__ACCESS_KEY_ID"
          export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__SECRET_ACCESS_KEY"

          __secret__AWS_ACCESS_KEY_ID="${globals.__secret__backrestAwsAccessKeyId}"
          __secret__AWS_SECRET_ACCESS_KEY="${globals.__secret__backrestAwsSecretAccessKey}"
          export AWS_ACCESS_KEY_ID="$__secret__AWS_ACCESS_KEY_ID" AWS_SECRET_ACCESS_KEY="$__secret__AWS_SECRET_ACCESS_KEY"

          cd jelastic_backup
          backups_to_keep=$(( ${settings.retention}+1 ))
          backups_to_delete=$(rclone lsd backup:${globals.bucketName} | awk '{print $NF}' | grep "^${env.envName}-scheduled.*auto" | sort -r | tail -n +$backups_to_keep)
          echo "$backups_to_delete" > /tmp/backups_to_rotate
          lock_sec_max=300
          meta_bucket_name="jc${globals.clusterRole}backupmetadata"
          lockfile="$(echo ${globals.bucketName} | sed -r 's/^[[:alpha:]]+([[:digit:]]+).+$/\1/')_backup_metadata.json.lock"
          getLock(){
            if [ -z "$1" ]; then
              locked_until=$(date -d "+${lock_sec_max}sec" +"%s")
            else
              locked_until=$1
            fi
            echo "${envName} $(date -d "+${lock_sec_max}sec" +"%s")" > /tmp/${lockfile}
            if (( $(( ${locked_until} - $(date +"%s") )) > 0 )); then
              rclone copy --ignore-existing --error-on-no-transfer /tmp/${lockfile} backup:${meta_bucket_name} 2>&1
              if [ $? -ne 0 ]; then
                read locker locked_until < <(RCLONE_S3_REGION=eu-west-1 rclone cat backup:${meta_bucket_name}/${lockfile})
                echo "$(date) - waiting for ${locker} to release his lock (max to $(date -d @${locked_until}))" >> ${globals.logsPath}
                sleep 1
                getLock ${locked_until}
              else
                echo "$(date) - the lock file is uploaded" >> ${globals.logsPath}
              fi
            else
              echo "$(date) - we take control from now" >> ${globals.logsPath}
              rclone copy /tmp/${lockfile} backup:${meta_bucket_name} 2>&1
            fi
          }
          getLock
          while IFS= read -r backup_name; do
            if [ ! -z "$backup_name" ]; then # Just makes sure we are NEVER going to purge an entire bucket
              timestamp=$(echo "$backup_name"|sed 's/.*_\(.*\)_auto/\1/')
              python3 backrest.py -a delmeta --bucketname ${globals.bucketName} --backupname ${settings.backup_name} -t $timestamp -m ${settings.backtype} --displayname '${env.displayName}' 2>>${globals.logsPath} || exit 1
              rclone purge backup:${globals.bucketName}/$backup_name 2>&1
            fi
          done <<<"$(cat /tmp/backups_to_rotate)"
          rclone delete backup:${meta_bucket_name}/${lockfile} 2>&1
          rm /tmp/backups_to_rotate
      - if ("${response.errOut}" != ""):
          - return:
              type: error
              message: "An error occurred during backup rotation. ${response.errOut}"

  backupElasticsearch:
    - cmd[${nodes.cp.first.id}]: |-
        curl -s \
          -u $UNOMI_ELASTICSEARCH_USERNAME:$UNOMI_ELASTICSEARCH_PASSWORD \
          https://$UNOMI_ELASTICSEARCH_ADDRESSES | python -c "import sys, json; print(json.load(sys.stdin)['version']['number'])"
    - setGlobals:
        esVersion: ${response.out}
    - getECAdminCredentials

    - cmd[${nodes.cp.first.id}]: |-
        # Create the bucket if doesn't exists
        __secret__AWS_ACCESS_KEY_ID="${globals.__secret__backrestAwsAccessKeyId}"
        __secret__AWS_SECRET_ACCESS_KEY="${globals.__secret__backrestAwsSecretAccessKey}"
        export AWS_ACCESS_KEY_ID="$__secret__AWS_ACCESS_KEY_ID" AWS_SECRET_ACCESS_KEY="$__secret__AWS_SECRET_ACCESS_KEY"
        cd jelastic_backup
        python3 elasticsearch.py --bucketname ${globals.bucketName} --backupname ${settings.backup_name} --cloudprovider=${globals.provider} --operation=backup 2>>${globals.logsPath} || exit 1
    - if ("${globals.provider}" == "aws"):
        - set:
            region: ${globals.region}
    - else:
        - set:
            region: "eu-west-1"
    - setAwsSnapshotRepository:
        repositoryName: ${env.shortdomain}
        backupName: ${settings.backup_name}
        region: ${this.region}
        account: ${globals.bucketName}
        logsPath: ${globals.logsPath}
        __secret__awsAccessKeyId: ${globals.__secret__backrestAwsAccessKeyId}
        __secret__awsSecretAccessKey: ${globals.__secret__backrestAwsSecretAccessKey}

    # start backup
    - cmd[${nodes.cp.first.id}]: |-
        output_file=$(mktemp)
        __secret__ec_admin_credentials=${globals.__secret__ecAdminCredentials}
        timestamp=$(echo "${globals.timestamp}"| awk '{print tolower($0)}')
        return_code=$(curl -sS -o $output_file -w '%{http_code}' \
          -H 'Content-Type: application/json' \
          -u $__secret__ec_admin_credentials \
          -XPUT "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/${timestamp}_${settings.backtype}?wait_for_completion=true" \
          -d'{
            "indices": "'${UNOMI_ELASTICSEARCH_INDEXPREFIX}'-*,-global-geonameentry*",
            "include_global_state": false
          }')
        exit=0
        if [ $return_code -ne 200 ]; then
          cat $output_file | tee -a ${globals.logsPath}
          exit=1
        fi
        if [[ "$(cat $output_file  | jq -r .snapshot.state)" != "SUCCESS" ]]; then
          cat $output_file | tee -a ${globals.logsPath}
          exit=1
        fi
        rm -f $output_file
        exit $exit
    - if ("${response.out}" != ""):
        - return:
            type: error
            message: "An error occurred during the backup process."

  rotateElasticsearchBackup:
    - getECAdminCredentials
    - cmd[${nodes.cp.first.id}]: |-
        __secret__ec_admin_credentials=${globals.__secret__ecAdminCredentials}
        export AWS_ACCESS_KEY_ID="${globals.__secret__backrestAwsAccessKeyId}" AWS_SECRET_ACCESS_KEY="${globals.__secret__backrestAwsSecretAccessKey}"
        cd jelastic_backup
        # filter and format snapshot list
        snapshots=$(curl -s \
          -H 'Content-Type: application/json' \
          -u $__secret__ec_admin_credentials \
          -XPUT "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/_all?pretty" \
          | grep "\"snapshot\"" | grep "${settings.backtype}" |awk '{ print substr($3,2,length($3)-3)}')
        delete_count=$(( $(wc -l<<<"$snapshots") - ${settings.retention} ))
        if [ $delete_count -lt 1 ]; then
          exit 0;
        fi
        lock_sec_max=300
        meta_bucket_name="jc${globals.clusterRole}backupmetadata"
        lockfile="$(echo ${globals.bucketName} | sed -r 's/^[[:alpha:]]+([[:digit:]]+).+$/\1/')_backup_metadata.json.lock"
        getLock(){
          if [ -z "$1" ]; then
            locked_until=$(date -d "+${lock_sec_max}sec" +"%s")
          else
            locked_until=$1
          fi
          echo "${envName} $(date -d "+${lock_sec_max}sec" +"%s")" > /tmp/${lockfile}
          if (( $(( ${locked_until} - $(date +"%s") )) > 0 )); then
            rclone copy --ignore-existing --error-on-no-transfer /tmp/${lockfile} backup:${meta_bucket_name} 2>&1
            if [ $? -ne 0 ]; then
              read locker locked_until < <(RCLONE_S3_REGION=eu-west-1 rclone cat backup:${meta_bucket_name}/${lockfile})
              echo "$(date) - waiting for ${locker} to release his lock (max to $(date -d @${locked_until}))" >> ${globals.logsPath}
              sleep 1
              getLock ${locked_until}
            else
              echo "$(date) - the lock file is uploaded" >> ${globals.logsPath}
            fi
          else
            echo "$(date) - we take control from now" >> ${globals.logsPath}
            rclone copy /tmp/${lockfile} backup:${meta_bucket_name} 2>&1
          fi
        }
        getLock
        snapshots_to_delete=$(head -n $delete_count <<<"$snapshots")
        for snapshot in $snapshots_to_delete;  do
          timestamp=$(echo $snapshot |awk '{print toupper(substr($0,0,19))}')
          python3 backrest.py -a delmeta --bucketname ${globals.bucketName} --backupname ${settings.backup_name} -t $timestamp
          if [ $? -eq 0 ]; then
            curl \
              -u $__secret__ec_admin_credentials \
              -XDELETE "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/$snapshot"
            echo "snapshot $snapshot deleted" | tee -a ${globals.logsPath}
          fi
        done
        rclone delete backup:${meta_bucket_name}/${lockfile} 2>&1

  backupHaproxy:
    - cmd [${nodes.bl.first.id}]: |-
        __secret__ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
        __secret__SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
        export RCLONE_S3_ACCESS_KEY_ID="$__secret__ACCESS_KEY_ID"
        export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__SECRET_ACCESS_KEY"

        HAPROXY_CONF_DIR=/etc/haproxy/haproxy.cfg.jahia
        HAPROXY_CUSTOMER_CONF_DIR_NAME=customer.configuration.d
        HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME=customer.errorpages.d
        HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME=http-errors.cfg
        HAPROXY_JAHIA_CONF=$HAPROXY_CONF_DIR/jahia-cloud.cfg
        HAPROXY_CUSTOMER_CONF_DIR=$HAPROXY_CONF_DIR/$HAPROXY_CUSTOMER_CONF_DIR_NAME
        HAPROXY_CUSTOMER_ERROR_PAGES_CONF=$HAPROXY_CONF_DIR/$HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME
        HAPROXY_CUSTOMER_ERROR_PAGES_DIR=$HAPROXY_CONF_DIR/$HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME

        cd jelastic_backup
        cp -a $HAPROXY_CUSTOMER_CONF_DIR .
        cp -a $HAPROXY_CUSTOMER_ERROR_PAGES_CONF $HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME
        cp -a $HAPROXY_CUSTOMER_ERROR_PAGES_DIR .
        tar -cz -H posix -f haproxy.tar.gz $HAPROXY_CUSTOMER_CONF_DIR_NAME $HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME $HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME

        rclone copy haproxy.tar.gz backup:${globals.bucketName}/${globals.backupName}/ 2>&1 || exit 1

        rm -rf haproxy.tar.gz $HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME $HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME $HAPROXY_CUSTOMER_CONF_DIR_NAME

    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred during haproxy backup."

  backupMariadb:
    - cmd[${this}]: |-
        __secret__ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
        __secret__SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
        export RCLONE_S3_ACCESS_KEY_ID="$__secret__ACCESS_KEY_ID"
        export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__SECRET_ACCESS_KEY"
        BACKUP_DIR=/tmp/database-backup
        BACKUP_NAME=database.tar

        rm -rf $BACKUP_DIR
        sudo -u mysql mkdir $BACKUP_DIR
        GALERA_INFOS=""
        if [ "$(mysql -sNe  "show global status like 'wsrep_ready'"|grep ON)" != "" ]; then
          GALERA_INFOS="--galera-info"
        fi
        sudo -u mysql ionice -n 5 mariabackup --backup --target-dir=$BACKUP_DIR $GALERA_INFOS --compress --compress-threads=2 --user mysql 2>> ${globals.logsPath} || exit 1
        cd jelastic_backup
        tar -H posix -C "$BACKUP_DIR" -cf $BACKUP_NAME . 2>> ${globals.logsPath}
        rclone copy $BACKUP_NAME backup:${globals.bucketName}/${globals.backupName}/ 2>&1 || exit 1
        rm -rf $BACKUP_DIR $BACKUP_NAME
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred during database backup."

  chooseTheBestDatabaseNodeToBackup:
    - if (nodes.sqldb.length > 1):
        # choose the node having the lowest number of openened connections
        - forEach(i:nodes.sqldb):
            chooseIfLessConnections: ${@i.id}
    - else:
        - setGlobals:
            dbBackupNode: ${nodes.sqldb.first.id}

  chooseIfLessConnections:
    - cmd[${this}]: mysql -sNe "select count(host) from information_schema.processlist"
    - if (${response.out} < ${globals.lowerDBConnection}):
        - setGlobals:
            lowerDBConnection: ${response.out}
            dbBackupNode: ${this}
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred during database backup."

  backupJcustomer:
    - cmd [${nodes.cp.first.id}]: |-
        __secret__ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
        __secret__SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
        export RCLONE_S3_ACCESS_KEY_ID="$__secret__ACCESS_KEY_ID"
        export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__SECRET_ACCESS_KEY"

        cd jelastic_backup
        echo "Backup unomi env vars" >> ${globals.logsPath}
        grep -P '^UNOMI_(?!ELASTICSEARCH)' /.jelenv | grep -v "^UNOMI_ELASTICSEARCH_" > jcustomer-env-vars 2>>${globals.logsPath}
        gzip jcustomer-env-vars 2>>${globals.logsPath} || exit 1

        rclone copy jcustomer-env-vars.gz backup:${globals.bucketName}/${globals.backupName}/ 2>&1 || exit 1

        rm -f jcustomer-env-vars.gz
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred during environment variables backup. ${response.errOut}"

settings:
  fields:
    - name: backup_name
      type: string
      caption: Backup Name
      vtype: text
      required: true
    - name: timestamp
      caption: timestamp in format %Y-%m-%dT%H:%M:00
      required: false
      type: string
    - name: retention
      caption: how many backup do you want to keep
      type: string
      default: 15
    - name: backtype
      caption: is this a manual or auto backup
      type: string
      default: manual
