---
type: update
version: 1.5
name: Jahia - Restores an environment from a bucket
logo: ../../assets/common/jahia-logo-70x70.png
id: jahia-restore-from-bucket

globals:
  logAction: "Restore"
  logsPath: "/var/log/jelastic-packages/restore.log"
  DS_in_DB: false

mixins:
  - ../../mixins/common.yml
  - ../../mixins/elasticsearch.yml
  - ../../mixins/jahia.yml
  - ../../mixins/jcustomer.yml
  - ../../mixins/mariadb.yml

onInstall:
  - muteDatadogHost:
      target: "*"
      duration: 240 # 4h
  - muteEnvWideMonitors:
      duration: 240 # 4h
  - setGlobalRepoRootUrl
  - getBackrestAwsAccessKey

  - script: |
        var region = '${settings.region_source}'
        return {'result': 0, 'resp': region.replace(/(\W|_)+/g, '')}
  - setGlobals:
      wc_region_source: ${response.resp}
      regionRealName_source: ${settings.region_source}
      cloudProvider_source: ${settings.cloud_source}
      provider: ${settings.cloud_source}
      region: ${settings.region_source}

  # TODO: Remove me, I'm ugly. Dirty hack to allow ovh internal backups older than v3.22.0 to be restore.
  # Remove the following if block
  - if (("${globals.provider}" == "ovh") && ("${settings.timestamp}" < "2023-02-08T07:00:00")):
      setGlobals:
        provider: aws
        region: eu-west-1
        oldOvhRegion: true # Used in getBucketName, should be remove there too

  - getBucketName:
      region: ${globals.region}
      uid: ${settings.uid_source}
      clusterRole: ${settings.envrole_source}
      cloudProvider: ${globals.provider}

  - if ("${globals.provider}" == "ovh"):
      getOvhBackupCredentials
  - else: # aws
      setGlobals:
          __secret__backupAccessKeyId: ${globals.__secret__backrestAwsAccessKeyId}
          __secret__backupSecretAccessKey: ${globals.__secret__backrestAwsSecretAccessKey}
  - setupBackupRCloneConfig:
      provider: ${globals.provider}
      region: ${globals.region}
  - setGlobals:
      backupName: ${settings.backup_name}_${settings.timestamp}_${settings.backtype}

  - if (nodes.proc):  # Jahia
      - muteDatadogSynthetics:
          duration: 240 # 4h
      - clearJelasticLogs:
          target: bl
          user: haproxy
          logsPath: ${globals.logsPath}
      - clearJelasticLogs:
          target: "proc, cp"
          user: tomcat
          logsPath: ${globals.logsPath}
      - clearJelasticLogs:
          target: "sqldb"
          user: mysql
          logsPath: ${globals.logsPath}
      - installBackupTools:
          target: proc,cp,sqldb
          logAction: ${globals.logAction}
      - if (nodes.bl):
          - installBackupTools:
              target: bl
              logAction: ${globals.logAction}
      - getJahiaVersion
      - getInitialJexperienceStatus
      - getInitialAugSearchStatus
      - saveOtherModulesConfig
      - restoreJahia
      - if (nodes.bl):
          - restoreHaproxy
      - unmuteDatadogSynthetics
  - else:  # Jcustomer
      - clearJelasticLogs:
          target: cp
          user: root
          logsPath: ${globals.logsPath}
      - installBackupTools:
          target: cp
          logAction: ${globals.logAction}
      - restoreElasticsearch
      - restoreJcustomer
      - checkJcustomerHealthWhenStarting: cp
  - unmuteDatadogHost:
      target: "*"


actions:
  getInitialJexperienceStatus:
    - setGlobals:
        jexperienceActiveBeforeRestore: false
    - checkModule:
        moduleSymname: jexperience
    - if ("${globals.moduleState}" == "started"):
        - cmd[proc]: |-
            echo "jexperience/${globals.runningVersion} active before restore" >> ${globals.logsPath}
        - setGlobals:
            jexperienceActiveBeforeRestore: true
            jexperienceVersionBeforeRestore: ${globals.runningVersion}
        - checkModule:
            moduleSymname: kibana-dashboards-provider
        - setGlobals:
            kibanaDashboardsVersionBeforeRestore: ${globals.runningVersion}
        - checkModule:
            moduleSymname: jexperience-dashboards
        - setGlobals:
            jexperienceDashboardsVersionBeforeRestore: ${globals.runningVersion}
        # We save Kibana dashboards provider configuration
        - cmd [proc,cp]: |-
            cp -p /data/digital-factory-data/karaf/etc/org.jahia.modules.kibana_dashboards_provider.cfg /root
        # We save jexperience configuration
        - cmd [proc,cp]: |-
            cp -p /data/digital-factory-data/karaf/etc/org.jahia.modules.jexperience.settings-global.cfg /root

  getInitialAugSearchStatus:
    # Saves AS module version and status (installed/started) in /tmp/as-state if augsearch variable present and set to true in nodegroup data
    - cmd[proc]: |-
        [ -f /tmp/as-state ] && echo "true" || echo "false"
    - setGlobals:
        augSearchActiveBeforeRestore: ${response.out}
    # if augSearchActiveBeforeRestore is True, it means that the file was already saved at a previous restore that failed
    # so there is nothing to do because we want to keep it
    - if (! ${globals.augSearchActiveBeforeRestore}):
        - isAugSearchEnabled
        - if (${globals.isAugSearchEnabled}):
            - checkModule:
                moduleSymname: augmented-search
            - if ("${globals.moduleState}" == "uninstalled"):
                return:
                  type: error
                  message: "augsearch variable value is true, but augmented-search module is not installed. Don't know what to do. Exiting"
            - set:
                asModuleState: ${globals.moduleState}
                asModuleVersion: ${globals.runningVersion:[globals.mostRecentInstalledVersion]}
            - checkModule:
                moduleSymname: database-connector
            - set:
                databaseModuleVersion: ${globals.runningVersion:[globals.mostRecentInstalledVersion]}
            - checkModule:
                moduleSymname: elasticsearch-connector
            - set:
                elasticsearchModuleVersion: ${globals.runningVersion:[globals.mostRecentInstalledVersion]}

            - cmd[proc]: |-
                echo '''
                  {
                    "as-status":"${this.asModuleState}",
                    "as-version":"${this.asModuleVersion}",
                    "database-connector-version":"${this.databaseModuleVersion}",
                    "elasticsearch-connector-version":"${this.elasticsearchModuleVersion}"
                  }
                ''' > /tmp/as-state
            - setGlobals:
                augSearchActiveBeforeRestore: true
    - if (${globals.augSearchActiveBeforeRestore}):
        # store augmented search/ and db/es connector modules status and versions in globals for future usage
        - cmd[proc]: |-
            cat /tmp/as-state
        - script: |-
            return {"result": 0, "onAfterReturn": {setGlobals: {modulesState: ${response.out.toJSON()} } } }
        - if ("${globals.modulesState.as-status}" == "started"):
            setGlobals:
              asShouldBeStarted: true
        - else:
            setGlobals:
              asShouldBeStarted: false

  saveOtherModulesConfig:
    # This action aims at saving various modules' config files in karaf/etc
    # (and that don't need specific actions). Any config file can be added to the cfg_files list
    #
    # Screeb: /data/digital-factory-data/karaf/etc/org.jahia.services.env.cfg
    - cmd[proc,cp]: |-
        cfg_files=(
          "/data/digital-factory-data/karaf/etc/org.jahia.services.env.cfg"
        )
        for file in ${cfg_files[@]}; do
          if [ -f $file ]; then
            cp -p $file /root
          fi
        done

  restoreOtherModulesConfig:
    # This action will restore config files from various modules saved in the saveOtherModulesConfig action
    # to the karaf/etc folder. The cfg_files list should contain the same files as in saveOtherModulesConfig
    #
    # Screeb: /root/org.jahia.services.env.cfg
    - log: "Initial Screeb and Kibana dashboard config has been restored"
    - cmd [proc, cp]: |-
        tmp_file=$(mktemp)
        prov_file=/data/digital-factory-data/patches/provisioning/restore-modules-configurations.yaml
        cfg_files=(
          "/root/org.jahia.services.env.cfg"
        )
        for file in ${cfg_files[@]}; do
          if [ -f $file ]; then
            echo '- installConfiguration: "file:$file"' \
              >> $tmp_file
          fi
        done
        if [[ $(wc -l $tmp_file | cut -d" " -f1) != 0 ]]; do
          chown tomcat: $prov_file
          mv $tmp_file $prov_file
        else
          rm -f $tmp_file
        fi

  reconfigureJExperience:
    - log: "jExperience was active before the restore, the original conf needs to be restored"
    - cmd [proc, cp]: |-
        tmp_file=$(mktemp)
        prov_file=/data/digital-factory-data/patches/provisioning/restore-jexperience-configuration.yaml
        cfg_files=(
          "/root/org.jahia.modules.kibana_dashboards_provider.cfg"
          "/root/org.jahia.modules.jexperience.settings-global.cfg"
        )
        for file in ${cfg_files[@]}; do
          if [ -f $file ]; then
            echo '- installConfiguration: "file:$file"' \
              >> $tmp_file
          fi
        done
        if [[ $(wc -l $tmp_file | cut -d" " -f1) != 0 ]]; do
          chown tomcat: $prov_file
          mv $tmp_file $prov_file
        else
          rm -f $tmp_file
        fi

  installOrUpgradeJExprienceModules:
    - cmd [proc]: |-
        provisioning_folder=/data/digital-factory-data/patches/provisioning
        provisioning_filename=jcustomer-restore-module-provisioning-file.yaml
        provisioning_file=$provisioning_folder/333.01-$provisioning_filename

        curl -fSsLo $provisioning_file ${globals.repoRootUrl}/assets/jahia/$provisioning_filename || exit 1

        sed -i \
            -e "s;__JEXPERIENCE_VERSION__;${globals.jexperienceVersionBeforeRestore};g" \
            -e "s;__KIBANA_DASHBOARDS_VERSION__;${globals.kibanaDashboardsVersionBeforeRestore};g" \
            -e "s;__JEXPERIENCE_DASHBOARDS_VERSION__;${globals.jexperienceDashboardsVersionBeforeRestore};g" \
            $provisioning_file

  prepareJExperienceRestore:
    - reconfigureJExperience
    # If the version of jExperience restored from the backup is not the same as the original one,
    # then we need to install the right version
    - log: "The version of jExperience pre-restore has to be installed: ${globals.jexperienceVersionBeforeRestore}"
    - cmd[proc]: |-
        echo "Re-install jExperience module to the pre-restore version: v${globals.jexperienceVersionBeforeRestore}" >> ${globals.logsPath}
    - log: "Install jExperience v${globals.jexperienceVersionBeforeRestore}"
    - installOrUpgradeJExprienceModules

  finishJExperienceRestore:
    # We make sure that jExperience modules are running
    - checkModule:
        moduleSymname: jexperience
    - if ("${globals.moduleState}" != "started"):
        - return:
            type: error
            message: "jExperience is not running after restoring the backup, please check."
    - checkModule:
        moduleSymname: kibana-dashboards-provider
    - if ("${globals.moduleState}" != "started"):
        - return:
            type: error
            message: "Kibana Dashboards Provider is not running after restoring the backup, please check."
    - checkModule:
        moduleSymname: jexperience-dashboards
    - if ("${globals.moduleState}" != "started"):
        - return:
            type: error
            message: "jExperience Dashboards is not running after restoring the backup, please check."

  reconfigureAS:
    # This action will drop groovy scripts in the "patch" folder to reconfigure AS
    # while Jahia is stopped (mandatory for some depending modules)
    - getECDeploymentEndpoints
    - cmd [proc]: |-
        patches_folder=/data/digital-factory-data/patches/groovy
        groovy_file=as-recreate-es-connection.groovy
        esc_recreate_groovy_file=$patches_folder/444.10-$groovy_file
        graphql_file=as-update-es-connection.graphql
        es_hostname=$(echo ${globals.es_endpoint} | sed 's/https:\/\/\(.*\):.*/\1/g')
        es_port=443
        es_user=${env.envName}
        __secret__es_password=${globals.__secret__elasticsearch_password}

        curl -fSsLo ${esc_recreate_groovy_file} ${globals.repoRootUrl}/assets/jahia/$groovy_file || exit 1
        curl -fSsLo $patches_folder/444.20-$graphql_file ${globals.repoRootUrl}/assets/jahia/$graphql_file || exit 1

        sed -i \
             -e "s;\(.*ES_HOSTNAME.*\)PLACEHOLDER\(.*\);\1${es_hostname}\2;" \
             -e "s;\(.*ES_PORT.*\)PLACEHOLDER\(.*\);\1${es_port}\2;" \
             -e "s;\(.*ES_USER.*\)PLACEHOLDER\(.*\);\1${es_user}\2;" \
             -e "s;\(.*ES_PASSWORD.*\)PLACEHOLDER\(.*\);\1${__secret__es_password}\2;" \
             $esc_recreate_groovy_file
        chown -R tomcat:tomcat $patches_folder

  installOrUpgradeASModules:
    # This action will drop a provisioning file in the "provisioning" folder to install (or upgrade)
    # the right versions of the modules required by AS
    - cmd [proc]: |-
        provisioning_folder=/data/digital-factory-data/patches/provisioning
        provisioning_filename=as-restore-modules-provisioning-file.yaml
        provisioning_file=$provisioning_folder/444.01-$provisioning_filename

        curl -fSsLo $provisioning_file ${globals.repoRootUrl}/assets/jahia/$provisioning_filename || exit 1

        sed -i \
             -e "s;__DB_CONNECTOR_VERSION__;${globals.modulesState.database-connector-version};g" \
             -e "s;__ES_CONNECTOR_VERSION__;${globals.modulesState.elasticsearch-connector-version};g" \
             -e "s;__AS_VERSION__;${globals.modulesState.as-version};g" \
             $provisioning_file
        chown tomcat:tomcat $provisioning_file

  prepareAugSearchRestore:
    - setGlobals:
        __secret__elasticsearch_password: ${fn.password(20)}
    - env.control.AddContainerEnvVars [cp, proc]:
      vars:
        JAHIA_ELASTICSEARCH_PASSWORD: ${globals.__secret__elasticsearch_password}
    - cmd[cp, proc]: |-
        systemctl restart datadog-agent
    - createESAccount4AS
    - reconfigureAS
    - installOrUpgradeASModules
    - purgeEnvIndicesInES

  checkESConnection:
    # This action makes sure that the ES connection has been updated by the groovy script during Jahia startup
    - cmd [proc]: |-
        es_user=${env.envName}
        es_hostname=$(echo ${globals.es_endpoint} | sed 's/https:\/\/\(.*\):.*/\1/g')
        es_conn_name=jahia-cloud_augmented-search
        __secret__pat_token="${globals.__secret__pat_token}"

        res=$(curl -fLSs -XGET localhost:8080/modules/dbconn/elasticsearch/status/$es_conn_name \
              -H "Authorization: APIToken $__secret__API_TOKEN" \
              -H 'Origin: http://localhost:8080' \
              -H 'Content-Type: application/json' \
              | jq -r ".success")
        if [[ "$res" == "" ]]; then
          echo "Error when trying to get the status" >&2
          exit 1
        fi

        if [[ "$es_hostname" != "$(echo $res | jq -r .host)" ]] || [[ "$es_user" != "$(echo $res | jq -r .user)" ]]; then
          echo "The ES connection has not been updated, please check why the as-recreate-es-connection.groovy script failed" >&2
          exit 1
        fi

  finishASRestore:
    # We need Jahia to be running to make sure the ES connection has been restored
    # and trigger a full reindex of sites for Augmented Search.
    - checkESConnection
    - if (${globals.asShouldBeStarted}):
        - checkModule:
            moduleSymname: augmented-search
        - if ("${globals.moduleState}" != "started"):
            - return:
                type: error
                message: "Augmented Search is not running after restoring the backup, please check."
        - triggerAugSearchFullReindex
    - else:
        # Stops AS module if it was not running on the env before the restore
        - stopModules:
            modules: augmented-search
    - cmd[proc] : rm -f /tmp/as-state

  restoreJahia:
    - cmd [proc,cp]: |-
        __secret__ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
        __secret__SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
        export RCLONE_S3_ACCESS_KEY_ID="$__secret__ACCESS_KEY_ID"
        export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__SECRET_ACCESS_KEY"
        archives=(
          digital-factory-data.tar.gz
          digital-factory-config.tar.gz
          jahia-env-vars.gz
        )
        cd jelastic_backup
        rclone copy $(for file in ${archives[@]}; do echo "--include=$file "; done) backup:${globals.bucketName}/${globals.backupName}/ . 2>>${globals.logsPath} || echo "ERROR" >&2
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring jahia (archives download)"
    - cmd[proc]: |-
        __secret__ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
        __secret__SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
        export RCLONE_S3_ACCESS_KEY_ID="$__secret__ACCESS_KEY_ID"
        export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__SECRET_ACCESS_KEY"
        share_folder=/share
        mirror_folder=~/share_mirror/datastore

        # in case of backup with datastore on DB,
        # check that we really have a datastore archive to download
        if [[ -z "$(rclone lsf backup:${globals.bucketName}/${globals.backupName}/datastore.tar)" ]]; then
          echo "no datastore archive found on bucket" >> ${globals.logsPath}
          echo "DS_in_DB"
          exit 0
        fi

        # reset the local mirror folder
        rm -fr $mirror_folder
        mkdir -p $mirror_folder

        # download the datastore archive and extract it on the fly
        set -o pipefail
        rclone cat backup:${globals.bucketName}/${globals.backupName}/datastore.tar \
               --log-file=${globals.logsPath} \
          | tar xf - -C ~/share_mirror 2>> ${globals.logsPath}

        if (($?)); then
          echo "Something went wrong with tar or rclone" >> ${globals.logsPath}
          echo "ERROR" >&2
          exit 0
        fi

        # now refreshing the shared datastore
        rsync -a --delete $mirror_folder $share_folder --log-file=${globals.logsPath} 2>>${globals.logsPath}
    - if ("${response.errOut}" != ""):
        return:
          type: error
          message: "An error occurred while restoring jahia (datastore)"
    - else:
        - if ("${response.out}" == "DS_in_DB"):
            setGlobals:
              DS_in_DB: true
    - cmd [proc,cp]: service tomcat stop
    - cmd [proc,cp]: |-
        tarfile=digital-factory-data.tar.gz
        cd jelastic_backup
        rm -rf /data/digital-factory-data
        tar xf $tarfile -C / 2>>${globals.logsPath} || { echo "ERROR" >&2; exit 0; }
        rm -f $tarfile
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring Jahia (data)"
    - restoreOtherModulesConfig
    # If Augmented Search and/or jExperience need to be restored, then these actions need to be performed
    # before starting Jahia so the modules can start successfully when Jahia will start
    - if (${globals.jexperienceActiveBeforeRestore}):
        - prepareJExperienceRestore
    - if (${globals.augSearchActiveBeforeRestore}):
        - prepareAugSearchRestore
    - cmd [proc,cp]: |-
        tarfile=digital-factory-config.tar.gz
        cd jelastic_backup
        rm -rf /opt/tomcat/conf/digital-factory-config
        tar xf $tarfile -C / 2>>${globals.logsPath} || { echo "ERROR" >&2; exit 0; }
        rm -f $tarfile
        rm -f /data/digital-factory-data/repository/.lock
        if [ -f /data/digital-factory-data/backup-with-JCR-index ]; then
          rm -f /data/digital-factory-data/backup-with-JCR-index
        else
          # Legacy backup without JCR saved with FRO, add reindex flag to trigger reindex on restart
          sudo -u tomcat touch /data/digital-factory-data/repository/reindex
        fi
        sudo -u tomcat touch /data/digital-factory-data/safe-env-clone
        short_name=$(echo ${_ROLE}.$HOSTNAME | sed -r 's/^([a-Z]+)\.[a-Z]+([0-9]+)-.+$/\1.\2/' | tr 'A-Z' 'a-z')
        sed -e "s|^cluster.node.serverId.*|cluster.node.serverId = $short_name|" \
            -e "s|^jahia.session.jvmRoute.*|jahia.session.jvmRoute = $short_name|" \
            -i $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties \
            2>>${globals.logsPath} || echo "ERROR" >&2
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring Jahia (configuration)"
    - switchDatastoreGarbageCollectionToTimer
    # Content History purge job should only be enabled on the processing node
    - cmd [cp]: |-
        file=/opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-purge-jobs.xml
        if [ -f $file ]; then mv $file ${file}-disabled; fi
    - cmd [proc]: |-
        file=/opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-purge-jobs.xml
        if ! [ -f $file ]; then
          url=${globals.repoRootUrl}/assets/jahia/applicationcontext-purge-jobs.xml-disabled
          curl -fSsLo ${file} $url || exit 1
          chown tomcat:tomcat $file
        fi
    - cmd [proc,cp]: |-
        file=jahia-env-vars
        tmpfile=/tmp/$file
        cd jelastic_backup
        gunzip $file.gz 2>>${globals.logsPath} || { echo "ERROR" >&2; exit 0; }
        cp $file $tmpfile
        sed -i "/^jahia_cfg_mvnPath/d" $tmpfile
        sed -i "/^jahia_cfg_elasticsearch_prefix/d" $tmpfile
        sed -i "/^jahia_cfg_jahia_jackrabbit_datastore_path/d" $tmpfile
        grep "jahia_cfg_mvnPath" /.jelenv >> $tmpfile
        grep "jahia_cfg_elasticsearch_prefix" /.jelenv >> $tmpfile
        grep "jahia_cfg_jahia_jackrabbit_datastore_path" /.jelenv >> $tmpfile
        grep -vE "^(jahia|tomcat)_cfg_" /.jelenv >> $tmpfile
        cat $tmpfile > /.jelenv
        rm -f $file $tmpfile
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring Jahia (environment variables)"
    - cmd [cp]: |-
        sed -i "s|^processingServer.*|processingServer = false|g" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
    - restoreMariadb
    - cleanJRLocalRevisionsTable
    - getPatTokenAndKey
    - cmd [proc]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        groovy_file_path=/data/digital-factory-data/patches/groovy/pat.groovy

        # Clean up any possible remainder of previous script execution
        rm -f ${groovy_file_path}*

        # Replace Pat Token if needed and start tomcat
        echo """
          org.jahia.services.content.JCRTemplate.getInstance().doExecuteWithSystemSession({ session ->
            def service = org.jahia.osgi.BundleUtils.getOsgiService(\"org.jahia.modules.apitokens.TokenService\")
            def tokenDetails = service.verifyToken(\"$__secret__pat_token\", session)
            if (! tokenDetails) {
              def tokens = service.getTokensDetails(\"/users/root\",  session).toList()
              // Remove all tokens with a name starting with jahia-cloud-token_
              // LEGACY (jahia environment V19): also remove token named "Jahia Cloud Token" in case of restauring an old backup
              for (token in tokens) {
                if (token.name.startsWith(\"jahia-cloud-token_\") || token.name == \"Jahia Cloud Token\" ) {
                  service.deleteToken(token.key, session)
                }
              }
              service.tokenBuilder(\"/users/root\", \"jahia-cloud-token_admin_${env.envName}\", session)
                     .setToken(\"$__secret__pat_token\")
                     .setActive(true)
                     .create()
              session.save();
            }
          })
        """ > ${groovy_file_path}
        chown tomcat: $groovy_file_path

    - if (globals.DS_in_DB):
        - cmd[proc]: |-
            echo "This backup contains the datastore in the database" >> ${globals.logsPath}
            if [[ -d $jahia_cfg_jahia_jackrabbit_datastore_path ]]; then
              echo "This env is configured for datastore in filesystem, have to migrate it from database to filesystem" >> ${globals.logsPath}
            else
              echo "This env is configured for datastore in database, no migration to filesystem needed" >> ${globals.logsPath}
              exit 0
            fi

            # download and build the datastore migration script
            echo "download jackrabbit-datastore-migration's source from github" >> ${globals.logsPath}
            curl -fLSso /tmp/jackrabbit-datastore-migration-0.1.5.zip https://github.com/woonsan/jackrabbit-datastore-migration/archive/refs/tags/0.1.5.zip || exit 1
            cd /tmp
            echo "extract jackrabbit-datastore-migration source" >> ${globals.logsPath}
            unzip jackrabbit-datastore-migration-0.1.5.zip 2>> ${globals.logsPath}
            echo "build jackrabbit-datastore-migration" >> ${globals.logsPath}
            cd /tmp/jackrabbit-datastore-migration-0.1.5
            $(find /opt/apache-maven-*/bin -type f -name mvn) clean install -DskipTests >> ${globals.logsPath} 2>&1

            echo "delete and recreate /share/datastore" >> ${globals.logsPath}
            rm -fr /share/datastore
            sudo -u tomcat mkdir /share/datastore

            # migration conf file setup
            echo "create jackrabbit-datastore-migration's config file" >> ${globals.logsPath}
            cat > /tmp/ds_migration_config.yaml <<- EOF
            logging:
                level:
                    root: 'WARN'
                    com.github.woonsan.jackrabbit.migration.datastore: 'DEBUG'

            batch:
                minWorkers: '10'
                maxWorkers: '10'

            target:
                dataStore:
                    directBackendAccess: true
                    homeDir: 'tmp/target'
                    className: 'org.apache.jackrabbit.vfs.ext.ds.VFSDataStore'
                    params:
                        baseFolderUri: 'file:///share/datastore'
                        minRecordLength: '1024'

            source:
                dataStore:
                    homeDir: 'tmp/source'
                    className: 'org.apache.jackrabbit.core.data.db.DbDataStore'
                    params:
                        url: 'jdbc:mysql://${nodes.sqldb.first.intIP}:3306/jahia?useUnicode=true&characterEncoding=UTF-8&useSSL=false'
                        user: '${DB_USER}'
                        password: '${DB_PASSWORD}'
                        driver: 'com.mysql.jdbc.Driver'
                        databaseType: 'mysql'
                        minRecordLength: '1024'
                        maxConnections: '10'
                        copyWhenReading: 'false'
                        tablePrefix: ''
                        schemaObjectPrefix: 'JR_'
                        schemaCheckEnabled: 'true'
            EOF

            # start the migration
            cd /tmp
            set -o pipefail
            sudo -u tomcat java -Dloader.path=/opt/tomcat/lib/ \
                                -jar /tmp/jackrabbit-datastore-migration-0.1.5/target/jackrabbit-datastore-migration-0.1.5.jar \
                                --spring.config.location=/tmp/ds_migration_config.yaml \
              >> ${globals.logsPath} 2>&1

            if (($?)); then
              echo "something went wrong on datastore migration" >> ${globals.logsPath}
              echo "ERROR" >&2
            fi

            # truncate datastore table in database
            mysql -h ${nodes.sqldb.first.intIP} -u ${DB_USER} -p${DB_PASSWORD} jahia -e "TRUNCATE TABLE JR_DATASTORE"
        - if ("${response.errOut}" != ""):
            - return:
                type: error
                message: "An error occurred while restoring Jahia (datastore migration)"


    - cmd[proc]: |-
        service tomcat start

    - startupJahiaHealthCheck: proc

    - if (${globals.jexperienceActiveBeforeRestore}):
        - finishJExperienceRestore
    - else:
        - checkModule:
            moduleSymname: jexperience
        - if ("${globals.moduleState}" != "uninstalled"):
            - log: "jExperience was not running before restore so it has to be removed"
            - removeAndCleanJexperience

    - if (${globals.augSearchActiveBeforeRestore}):
        - finishASRestore
    - else:
        - checkModule:
            moduleSymname: augmented-search
        - if ("${globals.moduleState}" != "uninstalled"):
            - log: "Augmented Search was not running before restore so it has to be removed"
            - removeAndCleanAugmentedSearch

    - cmd [cp]: service tomcat start

    - startupJahiaHealthCheck: cp
    # Upgrade dx-clustering module if needed
    - upgradeDxClusteringTo8109
    # Clean up /root folder with modules configuration files
    - cmd [proc, cp]: rm -f /root/*.cfg

  restoreElasticsearch:
    - script: |-
        // Extract only bucketname in case of foreign restore
        var account="${globals.bucketName}";
        account=account.split(" ")[0];
        return {'result': 0,'account': account}
    - setGlobals:
        account: ${response.account}
        region: ${globals.regionRealName_source}

    - cmd[${nodes.cp.first.id}]: |-
        curl -s \
          -u $UNOMI_ELASTICSEARCH_USERNAME:$UNOMI_ELASTICSEARCH_PASSWORD \
          https://$UNOMI_ELASTICSEARCH_ADDRESSES | python -c "import sys, json; print(json.load(sys.stdin)['version']['number'])" 2>>${globals.logsPath} || exit 1
    - setGlobals:
        esVersion: ${response.out}
    - getECAdminCredentials

    - if ("${globals.cloudProvider_source}" == "aws"):
        - set:
            region: ${globals.region}
    - else:
        - set:
            region: "eu-west-1"
    - setAwsSnapshotRepository:
        repositoryName: ${env.shortdomain}
        backupName: ${settings.backup_name}
        region: ${this.region}
        account: ${globals.account}
        logsPath: ${globals.logsPath}
        __secret__awsAccessKeyId: ${globals.__secret__backrestAwsAccessKeyId}
        __secret__awsSecretAccessKey: ${globals.__secret__backrestAwsSecretAccessKey}
        readonly: "true"

    - vaultSecretReadKeyB64:
        secretPath: paas/envs-common/cloud-ec-infra-circleci-api
        secretKey: token
    - set:
        __secret__circleCiToken: ${globals.__secret__vaultSecretData}
    - if ("HideThisLine" && "${globals.__secret__papiToken.print()}" == ""):
        getPapiInfoAll
    - getCloudConf
    - cmd[${nodes.cp.first.id}]: |-
        source /metadata_from_HOST
        if [ -z $JEL_ENV_ROLE ]; then
          echo "JEL_ENV_ROLE variable is not defined in /metadata_from_HOST"
          exit 1
        fi

        output_file=$(mktemp)
        __secret__ec_admin_credentials=${globals.__secret__ecAdminCredentials}
        timestamp=$(echo "${settings.timestamp}"| awk '{print tolower($0)}')
        snapshot_state=$(curl -sS -u $__secret__ec_admin_credentials \
          "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/${timestamp}_${settings.backtype}" | jq -r ".snapshots[0].state")
        if [[ "$snapshot_state" != "SUCCESS" ]]; then
          echo "The Elasticsearch snapshot you are trying to restore is not complete, aborting" >&2
          exit 1
        fi

        __secret__PAPI_TOKEN="${globals.__secret__papiToken}"
        export PAPI_TOKEN="$__secret__PAPI_TOKEN"
        export PAPI_HOSTNAME="${globals.papiHostname}"
        export PAPI_ENV_ID="${globals.papiEnvId}"
        export PAPI_API_VERSION="${globals.papiApiVersion}"

        environment=$(papi.py -X GET "paas-environment/$PAPI_ENV_ID")
        ec_deployment_id=$(echo $environment | jq -r .ec_deployment_id)
        ec_deployment=$(papi.py -X GET "ec-deployment/$ec_deployment_id")
        ec_deployment_mutualized=$(echo $ec_deployment | jq -r .mutualized)
        ec_deployment_min_score=$(echo $ec_deployment | jq -r .min_score)
        if [ "$ec_deployment_min_score" = "null" ]; then
          ec_deployment_min_score=0
        fi

        dev_dedicated=15
        dev_mutualized=20
        preprod_dedicated=15
        preprod_mutualized=20
        prod_dedicated=10
        prod_mutualized=15

        if [ "$ec_deployment_mutualized" = true ]; then
          eval ratio=\$${JEL_ENV_ROLE}_mutualized
        else
          eval ratio=\$${JEL_ENV_ROLE}_dedicated
        fi

        # Get the total shards and score before the restore
        current_total_shards=$(curl -sS -u $__secret__ec_admin_credentials "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_cluster/stats" | jq -r '.indices.shards.total')
        current_score=$(( (current_total_shards/ratio) + ((current_total_shards%ratio) > 0 )))

        curl -sSo /dev/null \
          -u $__secret__ec_admin_credentials \
          -X DELETE "https://$UNOMI_ELASTICSEARCH_ADDRESSES/${UNOMI_ELASTICSEARCH_INDEXPREFIX}-*"

        # Calculate the new total shards and score from the snapshot
        temp_total_shards=$(curl -sS -u $__secret__ec_admin_credentials "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_cluster/stats" | jq -r '.indices.shards.total')
        shards_to_be_added=$(curl -sS -u $__secret__ec_admin_credentials "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/${timestamp}_${settings.backtype}" | jq -r '.snapshots[0].shards.total')
        new_total_shards=$(( temp_total_shards + shards_to_be_added ))
        new_score=$(( (new_total_shards/ratio) + ((new_total_shards%ratio) > 0 )))

        # Update PAPI and trigger CircleCI pipeline only if new score is greater
        if [ "$new_score" -gt "$current_score" ] && [ "$new_score" -gt "$ec_deployment_min_score" ]; then
          organization_id=$(echo $environment | jq -r .paas_organization_id)
          ec_deployment_name=$(echo $ec_deployment | jq -r .name)
          papi.py -X PUT "ec-deployment/$ec_deployment_id" -d '{"score":'$new_score'}' >> ${globals.logsPath}

          __secret__CCToken=$(echo -n ${this.__secret__circleCiToken} | base64 -d)

          generate_payload() {
          if [[ "${globals.cloud_conf.ec.tag}" =~ ^v[0-9]+(\.[0-9]+)*$ ]]; then
            branch_or_tag="tag"
          else
            branch_or_tag="branch"
          fi
          cat << EOF
          {
            "$branch_or_tag": "${globals.cloud_conf.ec.tag}",
            "parameters": {
              "workflow": "terraform_apply",
              "env": "$JEL_ENV_ROLE",
              "papi_organization_id": $organization_id,
              "papi_instance_id": $ec_deployment_id,
              "papi_instance_name": "$ec_deployment_name"
            }
          }
        EOF
          }

          response=$(curl -fLSs -XPOST "https://circleci.com/api/v2/project/github/Jahia/cloud-ec-infra/pipeline" \
                        -H "Circle-Token: $__secret__CCToken" --header 'content-type: application/json' --data "$(generate_payload)")
          if [ $? -ne 0 ]; then
            echo "Pipeline trigger failed: $response"
            exit 1
          fi

          pipelineId=$(echo $response | jq -r .id)
          sleep 1  # it seems to take a few milliseconds for the pipeline to really be created in CCI...
          response=$(curl -LSs "https://circleci.com/api/v2/pipeline/$pipelineId/workflow" -H "Circle-Token: $__secret__CCToken")
          if [ "$(echo $response | jq -r '.items[0].status')" == "null" ]; then
            echo "No workflow created on CircleCI, aborting. Pipeline ID: $pipelineId. Response: $response" >&2
            exit 1
          fi

          # Wait for max 10 minutes for EC deployment update to finish then restore the snapshot anyways
          timeout=600 # 10 minutes
          sleep_interval=20
          workflow_status="running"
          while [ "$workflow_status" = "running" ]; do
            sleep $sleep_interval
            response=$(curl -fLSs "https://circleci.com/api/v2/pipeline/$pipelineId/workflow" -H "Circle-Token: $__secret__CCToken")
            if [ $? -eq 0 ]; then
              new_workflow_status=$(echo $response | jq -r '.items[0].status')
              if [ "$new_workflow_status" != "null" ]; then
                workflow_status=$new_workflow_status
              fi
            fi
            if [ $timeout -lt 1 ]; then
              break
            fi
            ((timeout-=$sleep_interval))
          done
        fi

        # Check if we restore a backup of an existing env present on the same EC deployment
        # If so, aliases will be excluded from restore to prevent conflicts errors
        prefix=$(curl -s -u $__secret__ec_admin_credentials -XGET "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/${timestamp}_${settings.backtype}" | jq '.snapshots[0].indices[0]' -r | sed 's/\(.*__jc\).*/\1/')
        existing_index=$(curl -su $__secret__ec_admin_credentials "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_cat/indices?format=json"| jq --arg prefix $prefix '.[] | select(.index | contains($prefix))|.index' -r)
        if [ "$existing_index" = "" ] ; then
          echo "Aliases included in restore" >> ${globals.logsPath}
          include_aliases=true
        else
          echo "Aliases excluded from restore"  >> ${globals.logsPath}
          include_aliases=false
        fi

        return_code=$(curl -sS -o $output_file -w '%{http_code}' \
          -H 'Content-Type: application/json' \
          -u $__secret__ec_admin_credentials \
          -XPOST "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/${timestamp}_${settings.backtype}/_restore" \
          -d'{
            "indices": "-global-geonameentry*,-context-geonameentry*",
            "rename_pattern": "(.*)(__[a-z0-9]+|^context)-(.*)",
            "rename_replacement": "'${UNOMI_ELASTICSEARCH_INDEXPREFIX}'-$3",
            "include_aliases": "'${include_aliases}'"
          }')
        exit=0
        if [ $return_code -ne 200 ]; then
          cat $output_file | tee -a ${globals.logsPath}
          exit=1
        fi

        if [ "$prefix" =  "${UNOMI_ELASTICSEARCH_INDEXPREFIX}" ]; then
          # Retoration on the same environment
          rm $output_file
          exit 0
        fi

        # needed for foreign restore:
        #   - checks aliases name and rename them (delete and create in fact)
        #   - fix the references to the original environment for the lifecycle_policy
        while read alias_name index_name; do

          if ($include_aliases); then
            if [[ "$alias_name" == "$prefix"* ]] && [[ "$index_name" == "${UNOMI_ELASTICSEARCH_INDEXPREFIX}"* ]]; then
              # Need to be renamed (deleted and recreated)
              rename_alias=true
              new_alias_index_name=$index_name
            else
              # We don't care about this alias
              continue
            fi
          else
            if [[ "$alias_name" != "$prefix"* ]]; then
              # We don't care about this alias
              continue
            else
              # Alias of the original env, we need to create a similar alias for the current env
              rename_alias=false
              new_alias_index_name=$(echo $index_name | sed -r 's/.+__jc(-.+)/'${UNOMI_ELASTICSEARCH_INDEXPREFIX}'\1/')
            fi
          fi

          new_alias_name="$(echo $alias_name | sed -r 's/.+__jc(-.+)/'${UNOMI_ELASTICSEARCH_INDEXPREFIX}'\1/')"
          if $rename_alias; then
            echo "index $index_name: alias is $alias_name while it should be $new_alias_name" >> ${globals.logsPath}
            return_code=$(curl -sS -o $output_file -w '%{http_code}' \
              -H 'Content-Type: application/json' \
              -u $__secret__ec_admin_credentials \
              -XDELETE "https://$UNOMI_ELASTICSEARCH_ADDRESSES/${index_name}/_alias/${alias_name}"
            )
            if [ $return_code -ne 200 ]; then
              cat $output_file | tee -a ${globals.logsPath}
              exit=1
            else
              echo "index $index_name: alias is $alias_name is now deleted" >> ${globals.logsPath}
            fi
          fi

          return_code=$(curl -sS -o $output_file -w '%{http_code}' \
            -H 'Content-Type: application/json' \
            -u $__secret__ec_admin_credentials \
            -XPOST "https://$UNOMI_ELASTICSEARCH_ADDRESSES/${new_alias_index_name}/_alias/${new_alias_name}"
          )
          if [ $return_code -ne 200 ]; then
            cat $output_file | tee -a ${globals.logsPath}
            exit=1
          else
            echo "index $new_alias_index_name: alias $new_alias_name is now created" >> ${globals.logsPath}
          fi

          return_code=$(curl -sS -o $output_file -w '%{http_code}' \
            -H 'Content-Type: application/json' \
            -u $__secret__ec_admin_credentials \
            -XPUT "https://$UNOMI_ELASTICSEARCH_ADDRESSES/${new_alias_index_name}/_settings" \
            -d'{
                 "settings": {
                   "index": {
                     "lifecycle": {
                         "name": "'${UNOMI_ELASTICSEARCH_INDEXPREFIX}'-unomi-rollover-policy",
                         "rollover_alias": "'${UNOMI_ELASTICSEARCH_INDEXPREFIX}'-'$(echo $new_alias_index_name| sed -r "s/.+-(session|event)-[0-9]+/\1/")'"
                     }
                   }
                 }
               }'
          )
          if [ $return_code -ne 200 ]; then
            cat $output_file | tee -a ${globals.logsPath}
            exit=1
          else
            echo "index $index_name: lifecycle settings fixed" >> ${globals.logsPath}
          fi
        done < <(curl -s -u $__secret__ec_admin_credentials "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_cat/aliases" | awk '$2 !~ "global-geonameentry" && substr($1,1,1) != "." {print $1, $2}')

        rm -f $output_file
        exit $exit

    - if ("${response.out}" != ""):
        - return:
            type: error
            message: "An error occurred during the backup restore process."

  restoreHaproxy:
    - cmd [bl]: |-
        __secret__ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
        __secret__SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
        export RCLONE_S3_ACCESS_KEY_ID="$__secret__ACCESS_KEY_ID"
        export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__SECRET_ACCESS_KEY"
        BACKUP_FILE=haproxy.tar.gz
        CUSTOMER_RULES_FILE=customer_rules.cfg
        HAPROXY_CONF_DIR=/etc/haproxy/haproxy.cfg.jahia
        HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME=customer.errorpages.d
        HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME=http-errors.cfg
        HAPROXY_CONF=$HAPROXY_CONF_DIR/jahia-cloud.cfg
        HAPROXY_CUSTOMER_CONF_DIR_NAME=customer.configuration.d
        HAPROXY_CUSTOMER_CONF_DIR=$HAPROXY_CONF_DIR/$HAPROXY_CUSTOMER_CONF_DIR_NAME
        HAPROXY_CUSTOMER_ERROR_PAGES_CONF=$HAPROXY_CONF_DIR/$HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME
        HAPROXY_CUSTOMER_ERROR_PAGES_DIR=$HAPROXY_CONF_DIR/$HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME

        cd jelastic_backup
        rclone copy backup:${globals.bucketName}/${globals.backupName}/haproxy.tar.gz . 2>&1 || exit 1
        rm -f $HAPROXY_CUSTOMER_CONF_DIR/* $HAPROXY_CUSTOMER_ERROR_PAGES_DIR/* $HAPROXY_CUSTOMER_ERROR_PAGES_CONF 2>/dev/null
        tar -xzf $BACKUP_FILE

        # Rules
        mv $HAPROXY_CUSTOMER_CONF_DIR_NAME/* $HAPROXY_CUSTOMER_CONF_DIR 2>/dev/null

        if [ -d $HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME ]; then
          mv $HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME/* $HAPROXY_CUSTOMER_ERROR_PAGES_DIR/ 2>/dev/null
          mv $HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME $HAPROXY_CUSTOMER_ERROR_PAGES_CONF 2>/dev/null
        else
          echo "Legacy haproxy backup without error page customization. Fetching default config" >> ${globals.logsPath}
          mkdir -p $HAPROXY_CUSTOMER_ERROR_PAGES_DIR
          curl -fLSso $HAPROXY_CUSTOMER_ERROR_PAGES_DIR/502.http ${globals.repoRootUrl}/assets/haproxy/502.http || exit 1
          curl -fLSso $HAPROXY_CUSTOMER_ERROR_PAGES_CONF ${globals.repoRootUrl}/assets/haproxy/http-errors.cfg || exit 1
          chown -R haproxy: $HAPROXY_CUSTOMER_ERROR_PAGES_DIR
        fi

        rm -rf $HAPROXY_CUSTOMER_CONF_DIR_NAME $BACKUP_FILE $HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME $HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME

        # Replace JSESSIONID and use HAProxy dedicated cookie
        if (grep -qw 'JSESSIONID' $HAPROXY_CONF); then
          cookie_line1="use_backend proc if { cook(SERVERID),lower -m beg s${nodes.proc.first.id} }"
          cookie_line2="cookie SERVERID insert nocache httponly secure"
          sed -i "/use_backend proc.*/c \ \ \ \ $cookie_line1" $HAPROXY_CONF
          sed -i "/cookie JSESSIONID.*/c \ \ \ \ $cookie_line2" $HAPROXY_CONF
        fi
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring haproxy configuration."
    - cmd [bl]: service haproxy restart

  restoreMariadb:
    - cmd[sqldb]: |-
        __secret__ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
        __secret__SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
        export RCLONE_S3_ACCESS_KEY_ID="$__secret__ACCESS_KEY_ID"
        export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__SECRET_ACCESS_KEY"

        cd jelastic_backup
        rclone copy backup:${globals.bucketName}/${globals.backupName}/database.tar . || exit 1
        ls database.tar || exit 0
        service mysql stop
        if (pgrep mysqld > /dev/null); then
          pkill -e -9 mysqld
        fi
        rm -rf /var/lib/mysql/*
        mv database.tar /var/lib/mysql/
        cd /var/lib/mysql/
        tar -xf database.tar 2>>${globals.logsPath}
        rm -f database.tar
        sudo -u mysql mariabackup --decompress --parallel=2 --remove-original --target-dir . 2>>${globals.logsPath} || exit 1

        # now some magic in case of restore from a previous mariadb 10.x version
        db_version=$(rpm -qi MariaDB-server | awk '$1=="Version" {match($NF, /^[0-9]+\.[0-9]+/, v); print v[0]}')
        echo "node's MariaDB version is $db_version" >> ${globals.logsPath}
        backup_version=$(awk '$1=="server_version" {match($NF, /^[0-9]+\.[0-9]+/, v); print v[0]}' /var/lib/mysql/xtrabackup_info)
        echo "backup's MariaDB version is $backup_version" >> ${globals.logsPath}
        if [[ $db_version != $backup_version ]]; then
          # The backup isn't from the same installed mariadb server version.
          # We will install docker and use a $backup_version MariaDB image
          echo "backup's MariaDB version and node's MariaDB version are not the same, we need to prepare the restored files using a mariadb:$backup_version image" >> ${globals.logsPath}
          dnf install -y yum-utils
          yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
          dnf install -y docker-ce docker-ce-cli containerd.io
          systemctl start docker
          docker run --rm -v /var/lib/mysql/:/var/lib/mysql:rw --entrypoint=mariabackup mariadb:${backup_version} --prepare --target-dir /var/lib/mysql 2>>${globals.logsPath}
          systemctl stop docker
          chown -R mysql:mysql /var/lib/mysql
          dnf remove -y docker-ce docker-ce-cli containerd.io yum-utils
          rm -rf /etc/yum.repos.d/docker-ce.repo /var/lib/docker
        else
          sudo -u mysql mariabackup --prepare --target-dir . 2>>${globals.logsPath} || exit 1
        fi

    - if (nodes.sqldb.length > 1):
        - cmd[sqldb]: |-
            seqno=$(cat /var/lib/mysql/xtrabackup_galera_info|cut -d':' -f 2)
            uuid=$(cat /var/lib/mysql/xtrabackup_galera_info|cut -d':' -f 1)
            echo "# GALERA saved state" > /var/lib/mysql/grastate.dat
            echo "version: 2.1" >> /var/lib/mysql/grastate.dat
            echo "uuid:    $uuid" >> /var/lib/mysql/grastate.dat
            echo "seqno:   $seqno" >> /var/lib/mysql/grastate.dat
            chown mysql: /var/lib/mysql/grastate.dat

        - cmd[${nodes.sqldb.first.id}]: |-
            echo "safe_to_bootstrap: 1" >> /var/lib/mysql/grastate.dat
        - startGaleraNode: ${nodes.sqldb.first.id}

        - forEach(i:nodes.sqldb):
            if (${@i.id} != ${nodes.sqldb.first.id}):
              - cmd[${@i.id}]: |-
                  echo "safe_to_bootstrap: 0" >> /var/lib/mysql/grastate.dat
              - startGaleraNode: ${@i.id}
    - else:
        startGaleraNode: sqldb


    - api: env.control.GetContainerEnvVarsByGroup
      nodeGroup: proc

    - cmd[${nodes.sqldb.first.id}]: |-
        __secret__PROXYSQL_MONITORING_PASSWORD="${response.object.PROXYSQL_MONITORING_PASSWORD}"
        # reset jahia user and datadog user password
        EXISTING_JAHIA_USER=$(mysql -sNe "select user from mysql.user where user like 'jahia-db-%'")
        mysql -e "DROP USER '${EXISTING_JAHIA_USER}'@'%'; flush privileges"
        mysql -e "CREATE USER '${DB_USER}'@'%' identified by '${DB_PASS}'"
        mysql -e "grant all privileges on jahia.* to '${DB_USER}'@'%'"
        mysql -e "set password for 'datadog'@'localhost' = PASSWORD('${DB_USER_DATADOG}')"
        mysql -e "set password for 'proxysql'@'%' = PASSWORD('${__secret__PROXYSQL_MONITORING_PASSWORD}')"
        mysql -e "flush privileges"

        if (( $(mysql -Nse "select count(user) from mysql.user where user='datadog' and host='%'") == 0 )); then
          mysql -e "CREATE USER 'datadog'@'%' IDENTIFIED BY '${DB_USER_DATADOG}'"
          mysql -e "GRANT SELECT ON jahia.JR_J_LOCAL_REVISIONS TO 'datadog'@'%'"
        else
          mysql -e "set password for 'datadog'@'%' = PASSWORD('${DB_USER_DATADOG}')"
          mysql -e "flush privileges"
        fi

        # Now recreate trigger(s)
        # dump trigger(s):
        mysqldump --no-data --no-create-info --skip-routines --triggers --add-drop-trigger --compact jahia > /tmp/triggers_dump.sql
        # change DEFINER(s) if matching /^jahia-db-[0-9]+$/
        sed -i -r "s|(DEFINER=\`)jahia-db-[0-9]+|\1$DB_USER|" /tmp/triggers_dump.sql
        # let's import this
        mysql jahia < /tmp/triggers_dump.sql
        rm /tmp/triggers_dump.sql

  restoreJcustomer:
    - cmd [cp]: |-
        __secret__ACCESS_KEY_ID="${globals.__secret__backupAccessKeyId}"
        __secret__SECRET_ACCESS_KEY="${globals.__secret__backupSecretAccessKey}"
        export RCLONE_S3_ACCESS_KEY_ID="$__secret__ACCESS_KEY_ID"
        export RCLONE_S3_SECRET_ACCESS_KEY="$__secret__SECRET_ACCESS_KEY"

        cd jelastic_backup
        rclone copy backup:${globals.bucketName}/${globals.backupName}/jcustomer-env-vars.gz .
        ls jcustomer-env-vars.gz || exit 0
    - if ("${response.errOut}" == ""):
        - cmd [cp]: |-
            cd jelastic_backup
            gunzip jcustomer-env-vars.gz
            echo "Restoring jcustomer env vars" >> ${globals.logsPath}
            # Exclude UNOMI_ELASTICSEARCH env vars if backup pre EC release
            grep -Ev '(^UNOMI_ELASTICSEARCH|^UNOMI_THIRDPARTY_PROVIDER1_)' jcustomer-env-vars > /tmp/jcustomer-env-vars
            rm -f jcustomer-env-vars
            grep -vP '^UNOMI_(?!ELASTICSEARCH|THIRDPARTY_PROVIDER1)' /.jelenv  >> /tmp/jcustomer-env-vars
            cat /tmp/jcustomer-env-vars > /.jelenv
            rm -f /tmp/jcustomer-env-vars
        - if ("${response.errOut}" != ""):
            - return:
                type: error
                message: "An error occurred while restoring jcustomer environment variables. ${response.errOut}"
        - checkIfReindexationIsNeeded
        - api: env.control.RestartNodes
          envName: ${env.envName}
          nodeGroup: cp
    - else:
        log: "No env var backup available"

  checkIfReindexationIsNeeded:
    - install:
        jps: "${globals.repoRootUrl}/packages/jcustomer/reindexation.yml"

settings:
  fields:
    - name: backup_name
      type: string
      caption: Backup Name
      vtype: text
      required: true
    - name: cloud_source
      type: string
      caption: cloud source ?
      required: true
    - name: region_source
      type: string
      caption: region_source ?
      required: true
    - name: uid_source
      type: string
      caption: uid_source ?
      required: true
    - name: envrole_source
      type: list
      caption: envrole_source ?
      values:
        dev: dev
        preprod: preprod
        prod: prod
      required: true
    - name: timestamp
      caption: timestamp in format %Y-%m-%dT%H:%M:00
      required: true
      type: string
    - name: backtype
      caption: is this a manual or auto backup
      type: string
      default: manual
      required: true
