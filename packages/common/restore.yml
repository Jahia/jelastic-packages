---
type: update
version: 1.5
name: Jahia - Restores an environment from a bucket
logo: ../../assets/common/jahia-logo-70x70.png
id: jahia-restore-from-bucket

globals:
  logAction: "Restore"
  logsPath: "/var/log/jelastic-packages/restore.log"

mixins:
  - ../../mixins/common.yml
  - ../../mixins/elasticsearch.yml
  - ../../mixins/jahia.yml
  - ../../mixins/jcustomer.yml
  - ../../mixins/mariadb.yml

onInstall:
  - muteDatadogHost:
      target: "*"
      duration: 240 # 4h
  - muteEnvWideMonitors:
      duration: 240 # 4h
  - setGlobalRepoRootUrl
  - getBackrestAwsAccessKey
  - if(settings.cloud_source):
      - if(settings.region_source):
          - script: |
                var region = '${settings.region_source}'
                return {'result': 0, 'resp': region.replace(/(\W|_)+/g, '')}
          - setGlobals:
              wc_region_source: ${response.resp}
          - if(settings.envrole_source):
              - if(settings.uid_source):
                  - setGlobals:
                      bucketname: jc${settings.envrole_source}${settings.uid_source}${globals.wc_region_source} -F ${settings.cloud_source},${settings.region_source},${settings.envrole_source}
              - setGlobals:
                  regionRealName_source: ${settings.region_source}
                  cloudProvider_source: ${settings.cloud_source}

  - if(settings.source_env):
      - envSource
      - setGlobals:
          bucketname: jc${globals.envRole_source}${env.uid}${globals.region_source} -F ${globals.cloudProvider_source},${globals.regionRealName_source},${settings.envrole_source}
  - if(!settings.source_env):
      - if(!settings.envrole_source):
          - setGlobals:
              bucketname: jc${cluster_role}${env.uid}${env_region}

  - if (nodes.proc):  # Jahia
      - muteDatadogSynthetics:
          duration: 240 # 4h
      - clearJelasticLogs:
          target: bl
          user: haproxy
          logsPath: ${globals.logsPath}
      - clearJelasticLogs:
          target: "proc, cp"
          user: tomcat
          logsPath: ${globals.logsPath}
      - clearJelasticLogs:
          target: "sqldb"
          user: mysql
          logsPath: ${globals.logsPath}
      - installBackupTools:
          target: proc,cp,sqldb
          logAction: ${globals.logAction}
      - if (nodes.bl):
          - installBackupTools:
              target: bl
              logAction: ${globals.logAction}
      - getJahiaVersion
      - isVersionHigherOrEqual:
          a: ${globals.jahiaVersion}
          b: 8.0.0.0
          res: jahia8
      - getInitialJexperienceStatus
      - getInitialAugSearchStatus
      - if (${globals.jahia8}):
          - saveOtherModulesConfig
      - restoreJahia
      - if (nodes.bl):
          - restoreHaproxy
      - unmuteDatadogSynthetics
  - else:  # Jcustomer
      - clearJelasticLogs:
          target: cp
          user: root
          logsPath: ${globals.logsPath}
      - installBackupTools:
          target: cp
          logAction: ${globals.logAction}
      - restoreElasticsearch
      - restoreJcustomer
      - checkJcustomerHealthWhenStarting: cp
  - unmuteDatadogHost:
      target: "*"


actions:
  getInitialJexperienceStatus:
    - setGlobals:
        jexperienceActiveBeforeRestore: false
    - checkModule:
        moduleSymname: jexperience
    - if ("${globals.moduleState}" == "started"):
        - cmd[proc]: |-
            echo "jexperience/${globals.runningVersion} active before restore" >> ${globals.logsPath}
        - setGlobals:
            jexperienceActiveBeforeRestore: true
            jexperienceVersionBeforeRestore: ${globals.runningVersion}
        - if (${globals.jahia8}):
            - checkModule:
                moduleSymname: kibana-dashboards-provider
            - setGlobals:
                kibanaDashboardsVersionBeforeRestore: ${globals.runningVersion}
            - checkModule:
                moduleSymname: jexperience-dashboards
            - setGlobals:
                jexperienceDashboardsVersionBeforeRestore: ${globals.runningVersion}
            # We save Kibana dashboards provider configuration
            - cmd [proc,cp]: |-
                cp -p /data/digital-factory-data/karaf/etc/org.jahia.modules.kibana_dashboards_provider.cfg /root
              user: root
        # We save jexperience configuration
        - cmd [proc,cp]: |-
            cp -p /data/digital-factory-data/karaf/etc/org.jahia.modules.jexperience.settings-global.cfg /root
          user: root

  getInitialAugSearchStatus:
    # Saves AS module version and status (installed/started) in /tmp/as-state if augsearch variable present and set to true in nodegroup data
    - cmd[proc]: |-
        [ -f /tmp/as-state ] && echo "true" || echo "false"
    - setGlobals:
        augSearchActiveBeforeRestore: ${response.out}
    # if augSearchActiveBeforeRestore is True, it means that the file was already saved at a previous restore that failed
    # so there is nothing to do because we want to keep it
    - if (! ${globals.augSearchActiveBeforeRestore}):
        - isAugSearchEnabled
        - if (${globals.isAugSearchEnabled}):
            - checkModule:
                moduleSymname: augmented-search
            - if ("${globals.moduleState}" == "uninstalled"):
                return:
                  type: error
                  message: "augsearch variable value is true, but augmented-search module is not installed. Don't know what to do. Exiting"
            - set:
                asModuleState: ${globals.moduleState}
                asModuleVersion: ${globals.runningVersion:[globals.mostRecentInstalledVersion]}
            - checkModule:
                moduleSymname: database-connector
            - set:
                databaseModuleVersion: ${globals.runningVersion:[globals.mostRecentInstalledVersion]}
            - checkModule:
                moduleSymname: elasticsearch-connector
            - set:
                elasticsearchModuleVersion: ${globals.runningVersion:[globals.mostRecentInstalledVersion]}

            - cmd[proc]: |-
                echo '''
                  {
                    "as-status":"${this.asModuleState}",
                    "as-version":"${this.asModuleVersion}",
                    "database-connector-version":"${this.databaseModuleVersion}",
                    "elasticsearch-connector-version":"${this.elasticsearchModuleVersion}"
                  }
                ''' > /tmp/as-state
            - setGlobals:
                augSearchActiveBeforeRestore: true
    - if (${globals.augSearchActiveBeforeRestore}):
        # store augmented search/ and db/es connector modules status and versions in globals for future usage
        - cmd[proc]: |-
            cat /tmp/as-state
        - script: |-
            return {"result": 0, "onAfterReturn": {setGlobals: {modulesState: ${response.out.toJSON()} } } }
        - if ("${globals.modulesState.as-status}" == "started"):
            setGlobals:
              asShouldBeStarted: true
        - else:
            setGlobals:
              asShouldBeStarted: false

  saveOtherModulesConfig:
    # This action aims at saving various modules' config files in karaf/etc
    # (and that don't need specific actions). Any config file can be added to the cfg_files list
    #
    # Screeb: /data/digital-factory-data/karaf/etc/org.jahia.services.env.cfg
    - cmd[proc,cp]: |-
        cfg_files=(
          "/data/digital-factory-data/karaf/etc/org.jahia.services.env.cfg"
        )
        for file in ${cfg_files[@]}; do
          if [ -f $file ]; then
            cp -p $file /root
          fi
        done
      user: root

  restoreOtherModulesConfig:
    # This action will restore config files from various modules saved in the saveOtherModulesConfig action
    # to the karaf/etc folder. The cfg_files list should contain the same files as in saveOtherModulesConfig
    #
    # Screeb: /root/org.jahia.services.env.cfg
    - log: "Initial Screeb and Kibana dashboard config has been restored"
    - cmd[proc, cp]: |-
        cfg_files=(
          "/root/org.jahia.services.env.cfg"
        )
        for file in ${cfg_files[@]}; do
          if [ -f $file ]; then
            mv $file /data/digital-factory-data/karaf/etc
          fi
        done
      user: root

  reconfigureJExperience:
    - log: "jExperience was active before the restore, the original conf needs to be restored"
    - if (${globals.jahia8}):
        - cmd [proc, cp]: |-
            mv /root/org.jahia.modules.kibana_dashboards_provider.cfg /data/digital-factory-data/karaf/etc
          user: root
    - cmd [proc, cp]: |-
        mv /root/org.jahia.modules.jexperience.settings-global.cfg /data/digital-factory-data/karaf/etc
      user: root

  installOrUpgradeJExprienceModules:
    - cmd [proc]: |-
        provisioning_folder=/data/digital-factory-data/patches/provisioning
        provisioning_filename=jcustomer-restore-module-provisioning-file.yaml
        provisioning_file=$provisioning_folder/333.01-$provisioning_filename

        curl -fSsLo $provisioning_file ${globals.repoRootUrl}/assets/jahia/$provisioning_filename || exit 1

        sed -i \
            -e "s;__JEXPERIENCE_VERSION__;${globals.jexperienceVersionBeforeRestore};g" \
            -e "s;__KIBANA_DASHBOARDS_VERSION__;${globals.kibanaDashboardsVersionBeforeRestore};g" \
            -e "s;__JEXPERIENCE_DASHBOARDS_VERSION__;${globals.jexperienceDashboardsVersionBeforeRestore};g" \
            $provisioning_file

  prepareJExperienceRestore:
    - reconfigureJExperience
    # If the version of jExperience restored from the backup is not the same as the original one,
    # then we need to install the right version (only for Jahia 8 since we are using provisioning)
    - if (${globals.jahia8}):
        - log: "The version of jExperience pre-restore has to be installed: ${globals.jexperienceVersionBeforeRestore}"
        - cmd[proc]: |-
            echo "Re-install jExperience module to the pre-restore version: v${globals.jexperienceVersionBeforeRestore}" >> ${globals.logsPath}
        - log: "Install jExperience v${globals.jexperienceVersionBeforeRestore}"
        - installOrUpgradeJExprienceModules

  finishJExperienceRestore:
    # For Jahia 7, since the provisioning doesn't work, we have to reinstall/upgrade
    # jExperience using the API (if needed)
    - if (!${globals.jahia8}):
        - installOrUpgradeModule:
            moduleSymname: jexperience
            moduleVersion: ${globals.jexperienceVersionBeforeRestore}
            moduleGroupId: org.jahia.modules
            moduleRepository: marketing-factory-releases
    - else:
        # For Jahia 8 we just make sure that jExperience modules are running
        - checkModule:
            moduleSymname: jexperience
        - if ("${globals.moduleState}" != "started"):
            - return:
                type: error
                message: "jExperience is not running after restoring the backup, please check."
        - checkModule:
            moduleSymname: kibana-dashboards-provider
        - if ("${globals.moduleState}" != "started"):
            - return:
                type: error
                message: "Kibana Dashboards Provider is not running after restoring the backup, please check."
        - checkModule:
            moduleSymname: jexperience-dashboards
        - if ("${globals.moduleState}" != "started"):
            - return:
                type: error
                message: "jExperience Dashboards is not running after restoring the backup, please check."

  reconfigureAS:
    # This action will drop groovy scripts in the "patch" folder to reconfigure AS
    # while Jahia is stopped (mandatory for some depending modules)
    - getECDeploymentEndpoints
    - cmd [proc]: |-
        patches_folder=/data/digital-factory-data/patches/groovy
        groovy_file=as-recreate-es-connection.groovy
        esc_recreate_groovy_file=$patches_folder/444.10-$groovy_file
        graphql_file=as-update-es-connection.graphql
        es_hostname=$(echo ${globals.es_endpoint} | sed 's/https:\/\/\(.*\):.*/\1/g')
        es_port=443
        es_user=${env.envName}
        __secret__es_password=${globals.__secret__elasticsearch_password}

        curl -fSsLo ${esc_recreate_groovy_file} ${globals.repoRootUrl}/assets/jahia/$groovy_file || exit 1
        curl -fSsLo $patches_folder/444.20-$graphql_file ${globals.repoRootUrl}/assets/jahia/$graphql_file || exit 1

        sed -i \
             -e "s;\(.*ES_HOSTNAME.*\)PLACEHOLDER\(.*\);\1${es_hostname}\2;" \
             -e "s;\(.*ES_PORT.*\)PLACEHOLDER\(.*\);\1${es_port}\2;" \
             -e "s;\(.*ES_USER.*\)PLACEHOLDER\(.*\);\1${es_user}\2;" \
             -e "s;\(.*ES_PASSWORD.*\)PLACEHOLDER\(.*\);\1${__secret__es_password}\2;" \
             $esc_recreate_groovy_file

  installOrUpgradeASModules:
    # This action will drop a provisioning file in the "provisioning" folder to install (or upgrade)
    # the right versions of the modules required by AS
    - cmd [proc]: |-
        provisioning_folder=/data/digital-factory-data/patches/provisioning
        provisioning_filename=as-restore-modules-provisioning-file.yaml
        provisioning_file=$provisioning_folder/444.01-$provisioning_filename

        curl -fSsLo $provisioning_file ${globals.repoRootUrl}/assets/jahia/$provisioning_filename || exit 1

        sed -i \
             -e "s;__DB_CONNECTOR_VERSION__;${globals.modulesState.database-connector-version};g" \
             -e "s;__ES_CONNECTOR_VERSION__;${globals.modulesState.elasticsearch-connector-version};g" \
             -e "s;__AS_VERSION__;${globals.modulesState.as-version};g" \
             $provisioning_file

  prepareAugSearchRestore:
    - setGlobals:
        __secret__elasticsearch_password: ${fn.password(20)}
    - createESAccount4AS
    - reconfigureAS
    # We are using provisioning to install the right version of the modules so it only works
    # with Jahia 8
    - if (${globals.jahia8}):
        - installOrUpgradeASModules
    - purgeEnvIndicesInES

  checkESConnection:
    # This action makes sure that the ES connection has been updated by the groovy script during Jahia startup
    - cmd [proc]: |-
        es_user=${env.envName}
        es_hostname=$(echo ${globals.es_endpoint} | sed 's/https:\/\/\(.*\):.*/\1/g')
        es_conn_name=jahia-cloud_augmented-search
        __secret__pat_token="${globals.__secret__pat_token}"

        res=$(curl -fLSs -XGET localhost:8080/modules/dbconn/elasticsearch/status/$es_conn_name \
              -H "Authorization: APIToken $__secret__API_TOKEN" \
              -H 'Origin: http://localhost:8080' \
              -H 'Content-Type: application/json' \
              | jq -r ".success")
        if [[ "$res" == "" ]]; then
          echo "Error when trying to get the status" >&2
          exit 1
        fi

        if [[ "$es_hostname" != "$(echo $res | jq -r .host)" ]] || [[ "$es_user" != "$(echo $res | jq -r .user)" ]]; then
          echo "The ES connection has not been updated, please check why the as-recreate-es-connection.groovy script failed" >&2
          exit 1
        fi

  finishASRestore:
    # We need Jahia to be running to make sure the ES connection has been restored
    # and trigger a full reindex of sites for Augmented Search.
    #
    # Since the provisioning doesn't work for Jahia 7, we need to reinstall modules and restore
    # ES connection/AS conf the old way, using the API, which is why we do it once Jahia is running
    # Unlike Jahia 8, since we don't know if the modules were installed and running in the backup and they are
    # not installed with the provisioning file, we can't fully trust the groovy scripts to make sure that
    # the ES connector and AS connection have been updated, so we can rely on the modules' state, if they are running
    # it means the groovy scripts could be run and we don't need to reconfigure ES/AS (I know it's super complex but
    # Jahia 7 will soon be dropped so...)
    - if (! ${globals.jahia8}):
        - set:
            reconfigureES: false
            reconfigureAS: false
        - checkModule:
            moduleSymname: elasticsearch-connector
        - if ("${globals.moduleState}" != "started"):
            - set:
                reconfigureES: true
        - checkModule:
            moduleSymname: augmented-search
        - if ("${globals.moduleState}" != "started"):
            - set:
                reconfigureAS: true
        - installOrUpgradeModule:
            moduleSymname: database-connector
            moduleVersion: ${globals.modulesState.database-connector-version}
            moduleGroupId: org.jahia.modules
            moduleRepository: jahia-connector-enterprise-releases
        - installOrUpgradeModule:
            moduleSymname: elasticsearch-connector
            moduleVersion: ${globals.modulesState.elasticsearch-connector-version}
            moduleGroupId: org.jahia.modules
            moduleRepository: jahia-connector-enterprise-releases
        - installOrUpgradeModule:
            moduleSymname: augmented-search
            moduleVersion: ${globals.modulesState.as-version}
            moduleGroupId: org.jahia.modules
            moduleRepository: augmented-search-releases
        - if (${this.reconfigureES}):
            - removeDefaultESConnection
            - setDefaultESConnection
        - if (${this.reconfigureAS}):
            - setAugSearchESConnection
    - else:
        # We need to make sure that the ES connection has been successfully restored before triggering a full reindex
        - checkESConnection
    - if (${globals.asShouldBeStarted}):
        - checkModule:
            moduleSymname: augmented-search
        - if ("${globals.moduleState}" != "started"):
            - return:
                type: error
                message: "Augmented Search is not running after restoring the backup, please check."
        - triggerAugSearchFullReindex
    - else:
        # Stops AS module if it was not running on the env before the restore
        - stopModule:
            moduleSymname: augmented-search
    - cmd[proc] : rm -f /tmp/as-state

  restoreJahia:
    - cmd [proc,cp]: |-
        sudo service tomcat stop
        __secret__AWS_ACCESS_KEY_ID="${globals.__secret__backrestAwsAccessKeyId}"
        __secret__AWS_SECRET_ACCESS_KEY="${globals.__secret__backrestAwsSecretAccessKey}"
        export AWS_ACCESS_KEY_ID="$__secret__AWS_ACCESS_KEY_ID" AWS_SECRET_ACCESS_KEY="$__secret__AWS_SECRET_ACCESS_KEY"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f digital-factory-data.tar.gz 2>>${globals.logsPath} || exit 1
        sudo rm -rf /data/digital-factory-data
        sudo chown tomcat:tomcat /data
        tar xf digital-factory-data.tar.gz -C / 2>>${globals.logsPath} || exit 1
        rm digital-factory-data.tar.gz
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring jahia data. ${response.errOut}"

    - if (${globals.jahia8}):
        - restoreOtherModulesConfig

    # If Augmented Search and/or jExperience need to be restored, then these actions need to be performed
    # before starting Jahia so the modules can start successfully when Jahia will start
    - if (${globals.jexperienceActiveBeforeRestore}):
        - prepareJExperienceRestore
    - if (${globals.augSearchActiveBeforeRestore}):
        - prepareAugSearchRestore

    - cmd [proc,cp]: |-
        __secret__AWS_ACCESS_KEY_ID="${globals.__secret__backrestAwsAccessKeyId}"
        __secret__AWS_SECRET_ACCESS_KEY="${globals.__secret__backrestAwsSecretAccessKey}"
        export AWS_ACCESS_KEY_ID="$__secret__AWS_ACCESS_KEY_ID" AWS_SECRET_ACCESS_KEY="$__secret__AWS_SECRET_ACCESS_KEY"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f digital-factory-config.tar.gz 2>>${globals.logsPath} || exit 1
        rm -rf /opt/tomcat/conf/digital-factory-config
        tar xf digital-factory-config.tar.gz -C / 2>>${globals.logsPath} || exit 1
        chown tomcat:tomcat -R /opt/tomcat/conf
        rm -f digital-factory-config.tar.gz
        rm -f /data/digital-factory-data/repository/.lock
        # Add reindex flag to trigger reindex on restart
        touch /data/digital-factory-data/repository/reindex
        touch /data/digital-factory-data/safe-env-clone
        short_name=$(echo ${_ROLE}.$HOSTNAME | sed -r 's/^([a-Z]+)\.[a-Z]+([0-9]+)-.+$/\1.\2/' | tr 'A-Z' 'a-z')
        sed -e "s|^cluster.node.serverId.*|cluster.node.serverId = $short_name|" \
            -e "s|^jahia.session.jvmRoute.*|jahia.session.jvmRoute = $short_name|" \
            -i $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring jahia configuration. ${response.errOut}"
    # Content History purge job should only be enabled on the processing node
    - cmd [cp]: |-
        file=/opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-purge-jobs.xml
        if [ -f $file ]; then mv $file ${file}-disabled; fi
      user: tomcat
    - cmd [proc]: |-
        file=/opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-purge-jobs.xml
        if ! [ -f $file ]; then
          url=${globals.repoRootUrl}/assets/jahia/applicationcontext-purge-jobs.xml-disabled
          curl -fSsLo ${file} $url || exit 1
        fi
      user: tomcat

    - cmd [proc,cp]: |-
        __secret__AWS_ACCESS_KEY_ID="${globals.__secret__backrestAwsAccessKeyId}"
        __secret__AWS_SECRET_ACCESS_KEY="${globals.__secret__backrestAwsSecretAccessKey}"
        export AWS_ACCESS_KEY_ID="$__secret__AWS_ACCESS_KEY_ID" AWS_SECRET_ACCESS_KEY="$__secret__AWS_SECRET_ACCESS_KEY"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f jahia-env-vars.gz 2>>${globals.logsPath} || exit 1
        ls jahia-env-vars.gz || exit 0
    - if ("${response.errOut}" == ""):
        - cmd [proc,cp]: |-
            cd jelastic_backup
            gunzip jahia-env-vars.gz
            mv jahia-env-vars /tmp/
            sed -i "/^jahia_cfg_mvnPath/d" /tmp/jahia-env-vars
            sed -i "/^jahia_cfg_elasticsearch_prefix/d" /tmp/jahia-env-vars
            grep "jahia_cfg_mvnPath" /.jelenv >> /tmp/jahia-env-vars
            grep "jahia_cfg_elasticsearch_prefix" /.jelenv >> /tmp/jahia-env-vars
            grep -vE "^(jahia|tomcat)_cfg_" /.jelenv >> /tmp/jahia-env-vars
        - cmd [proc,cp]: |-
            cat /tmp/jahia-env-vars > /.jelenv
            rm -f /tmp/jahia-env-vars
          user: root
        - if ("${response.errOut}" != ""):
            - return:
                type: error
                message: "An error occurred while restoring jahia environment variables. ${response.errOut}"
    - else:
        log: "No env var backup available"

    - cmd [cp]: |-
        sed -i "s|^processingServer.*|processingServer = false|g" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
    - restoreMariadb
    - cleanJRLocalRevisionsTable
    - getPatTokenAndKey
    - cmd [proc]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        groovy_file_path=/data/digital-factory-data/patches/groovy/pat.groovy

        # Clean up any possible remainder of previous script execution
        rm -f ${groovy_file_path}*

        # Disable removeAbandoned in JDBC before the first start
        sed -E 's:(^\s+removeAbandoned=).*:\1"false":' \
            -i /opt/tomcat/webapps/ROOT/META-INF/context.xml

        # Replace Pat Token if needed and start tomcat
        jahia_major_version=$(echo "$DX_VERSION" | cut -d'.' -f1)
        if [ $jahia_major_version -eq 7 ]; then
          jahia_7_parameter=", null"
        else
          jahia_7_parameter=""
        fi
        echo """
          org.jahia.services.content.JCRTemplate.getInstance().doExecuteWithSystemSession({ session ->
            def service = org.jahia.osgi.BundleUtils.getOsgiService(\"org.jahia.modules.apitokens.TokenService\"$jahia_7_parameter)
            def tokenDetails = service.verifyToken(\"$__secret__pat_token\", session)
            if (! tokenDetails) {
              def tokens = service.getTokensDetails(\"/users/root\",  session).toList()
              // Remove all tokens with a name starting with jahia-cloud-token_
              // LEGACY (jahia environment V19): also remove token named "Jahia Cloud Token" in case of restauring an old backup
              for (token in tokens) {
                if (token.name.startsWith(\"jahia-cloud-token_\") || token.name == \"Jahia Cloud Token\" ) {
                  service.deleteToken(token.key, session)
                }
              }
              service.tokenBuilder(\"/users/root\", \"jahia-cloud-token_admin_${env.envName}\", session)
                     .setToken(\"$__secret__pat_token\")
                     .setActive(true)
                     .create()
              session.save();
            }
          })
        """ > ${groovy_file_path}

        sudo service tomcat start

    - startupJahiaHealthCheck: proc

    - if (${globals.jexperienceActiveBeforeRestore}):
        - finishJExperienceRestore
    - else:
        - checkModule:
            moduleSymname: jexperience
        - if ("${globals.moduleState}" != "uninstalled"):
            - log: "jExperience was not running before restore so it has to be removed"
            - removeAndCleanJexperience

    - if (${globals.augSearchActiveBeforeRestore}):
        - finishASRestore
    - else:
        - checkModule:
            moduleSymname: augmented-search
        - if ("${globals.moduleState}" != "uninstalled"):
            - log: "Augmented Search was not running before restore so it has to be removed"
            - removeDefaultESConnection
            - uninstallModule:
                moduleSymname: augmented-search

    - cmd[proc]: |-
        # re-enable removeAbandoned in JDBC
        sed -E 's:(^\s+removeAbandoned=).*:\1"true":' \
            -i /opt/tomcat/webapps/ROOT/META-INF/context.xml
        # restart tomcat
        sudo service tomcat restart
    - startupJahiaHealthCheck: proc

    - cmd[cp]: |-
        # Disable removeAbandoned in JDBC before the first start
        sed -E 's:(^\s+removeAbandoned=).*:\1"false":' \
            -i /opt/tomcat/webapps/ROOT/META-INF/context.xml
        sudo service tomcat start
    - startupJahiaHealthCheck: cp
    - cmd[cp]: |-
        # re-enable removeAbandoned in JDBC
        sed -E 's:(^\s+removeAbandoned=).*:\1"true":' \
            -i /opt/tomcat/webapps/ROOT/META-INF/context.xml
        # restart tomcat
        sudo service tomcat restart
    - startupJahiaHealthCheck: cp

  envSource:
    - script: |
          var envInfo = jelastic.env.control.getenvinfo('${settings.source_env}', session)
          for (var i = 0, n = envInfo.nodes; i < n.length; i++) {
            if (n[i].nodeGroup == 'cp') {
              var nodeID = n[i].id;
              break;
            }
          }
          var metadata = jelastic.env.file.read('${settings.source_env}', session, '/metadata_from_HOST', null, null, nodeID).body.toString()

          var re = /(\S|\n|\r)*JEL_REGION=(\S+)(\S|\n|\r)*/
          var regionRealName = metadata.replace(re, '$2')
          var region = regionRealName.replace(/(\W|_)+/g, '')

          var re = /(\S|\n|\r)*JEL_CLOUDPROVIDER=(\S+)(\S|\n|\r)*/
          var cloudProvider = metadata.replace(re, '$2')

          var re = /(\S|\n|\r)*JEL_AVAILABILITYZONE=(\S+)(\S|\n|\r)*/
          var az = metadata.replace(re, '$2')

          var re = /(\S|\n|\r)*JEL_ENV_ROLE=(\S+)(\S|\n|\r)*/
          var envRole = metadata.replace(re, '$2')

          return {'result': 0,
            'region': region,
            'regionRealName': regionRealName,
            'cloudProvider': cloudProvider,
            'az': az,
            'envRole': envRole}
    - setGlobals:
        region_source: ${response.region}
        regionRealName_source: ${response.regionRealName}
        cloudProvider_source: ${response.cloudProvider}
        az_source: ${response.az}
        envRole_source: ${response.envRole}

  restoreElasticsearch:
    - script: |-
        // Extract only bucketname in case of foreign restore
        var account="${globals.bucketname}";
        account=account.split(" ")[0];
        return {'result': 0,'account': account}
    - setGlobals:
        account: ${response.account}
        region: ${globals.regionRealName_source}

    - cmd[${nodes.cp.first.id}]: |-
        curl -s \
          -u $UNOMI_ELASTICSEARCH_USERNAME:$UNOMI_ELASTICSEARCH_PASSWORD \
          https://$UNOMI_ELASTICSEARCH_ADDRESSES | python -c "import sys, json; print(json.load(sys.stdin)['version']['number'])" 2>>${globals.logsPath} || exit 1
    - setGlobals:
        esVersion: ${response.out}
    - getECAdminCredentials

    - if ("${globals.cloudProvider_source}" == "aws" || "${globals.cloudProvider_source}" == "ovh"):
        - if ("${globals.cloudProvider_source}" == "aws"):
            - set:
                region: ${globals.region}
        - else:
            - set:
                region: "eu-west-1"
        - setAwsSnapshotRepository:
            repositoryName: ${env.shortdomain}
            backupName: ${settings.backup_name}
            region: ${this.region}
            account: ${globals.account}
            logsPath: ${globals.logsPath}
            __secret__awsAccessKeyId: ${globals.__secret__backrestAwsAccessKeyId}
            __secret__awsSecretAccessKey: ${globals.__secret__backrestAwsSecretAccessKey}
            readonly: "true"
    - else:
        - setAzureSnapshotRepository:
            repositoryName: ${env.shortdomain}
            __secret__awsAccessKeyId: ${globals.__secret__backrestAwsAccessKeyId}
            __secret__awsSecretAccessKey: ${globals.__secret__backrestAwsSecretAccessKey}
            operation: "restore"
            account: ${globals.account}
            backupName: ${settings.backup_name}
            logsPath: ${globals.logsPath}
            readonly: "true"

    - vaultSecretReadKeyB64:
        secretPath: paas/envs-common/cloud-ec-infra-circleci-api
        secretKey: token
    - set:
        __secret__circleCiToken: ${globals.__secret__vaultSecretData}
    - if ("HideThisLine" && "${globals.__secret__papiToken.print()}" == ""):
        getPapiInfoAll
    - getCloudConf
    - cmd[${nodes.cp.first.id}]: |-
        source /metadata_from_HOST
        if [ -z $JEL_ENV_ROLE ]; then
          echo "JEL_ENV_ROLE variable is not defined in /metadata_from_HOST"
          exit 1
        fi

        output_file=$(mktemp)
        __secret__ec_admin_credentials=${globals.__secret__ecAdminCredentials}
        timestamp=$(echo "${settings.timestamp}"| awk '{print tolower($0)}')
        snapshot_state=$(curl -sS -u $__secret__ec_admin_credentials \
          "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/${timestamp}_${settings.backtype}" | jq -r ".snapshots[0].state")
        if [[ "$snapshot_state" != "SUCCESS" ]]; then
          echo "The Elasticsearch snapshot you are trying to restore is not complete, aborting" >&2
          exit 1
        fi

        __secret__PAPI_TOKEN="${globals.__secret__papiToken}"
        export PAPI_TOKEN="$__secret__PAPI_TOKEN"
        export PAPI_HOSTNAME="${globals.papiHostname}"
        export PAPI_ENV_ID="${globals.papiEnvId}"
        export PAPI_API_VERSION="${globals.papiApiVersion}"

        environment=$(papi.py -X GET "paas-environment/$PAPI_ENV_ID")
        ec_deployment_id=$(echo $environment | jq -r .ec_deployment_id)
        ec_deployment=$(papi.py -X GET "ec-deployment/$ec_deployment_id")
        ec_deployment_mutualized=$(echo $ec_deployment | jq -r .mutualized)

        dev_dedicated=50
        dev_mutualized=70
        preprod_dedicated=50
        preprod_mutualized=70
        prod_dedicated=20
        prod_mutualized=50

        if [ "$ec_deployment_mutualized" = true ]; then
          eval ratio=\$${JEL_ENV_ROLE}_mutualized
        else
          eval ratio=\$${JEL_ENV_ROLE}_dedicated
        fi

        # Get the total shards and score before the restore
        current_total_shards=$(curl -sS -u $__secret__ec_admin_credentials "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_cluster/stats" | jq -r '.indices.shards.total')
        current_score=$(( (current_total_shards/ratio) + ((current_total_shards%ratio) > 0 )))

        curl -sSo /dev/null \
          -u $__secret__ec_admin_credentials \
          -X DELETE "https://$UNOMI_ELASTICSEARCH_ADDRESSES/${UNOMI_ELASTICSEARCH_INDEXPREFIX}-*"

        # Calculate the new total shards and score from the snapshot
        temp_total_shards=$(curl -sS -u $__secret__ec_admin_credentials "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_cluster/stats" | jq -r '.indices.shards.total')
        shards_to_be_added=$(curl -sS -u $__secret__ec_admin_credentials "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/${timestamp}_${settings.backtype}" | jq -r '.snapshots[0].shards.total')
        new_total_shards=$(( temp_total_shards + shards_to_be_added ))
        new_score=$(( (new_total_shards/ratio) + ((new_total_shards%ratio) > 0 )))

        # Update PAPI and trigger CircleCI pipeline only if new score is greater
        if [ "$new_score" -gt "$current_score" ]; then
          organization_id=$(echo $environment | jq -r .paas_organization_id)
          ec_deployment_name=$(echo $ec_deployment | jq -r .name)
          papi.py -X PUT "ec-deployment/$ec_deployment_id" -d '{"score":'$new_score'}'

          __secret__CCToken=$(echo -n ${this.__secret__circleCiToken} | base64 -d)

          generate_payload() {
          if [[ "${globals.cloud_conf.ec.tag}" =~ ^v[0-9]+(\.[0-9]+)*$ ]]; then
            branch_or_tag="tag"
          else
            branch_or_tag="branch"
          fi
          cat << EOF
          {
            "$branch_or_tag": "${globals.cloud_conf.ec.tag}",
            "parameters": {
              "workflow": "terraform_apply",
              "env": "$JEL_ENV_ROLE",
              "papi_organization_id": $organization_id,
              "papi_instance_id": $ec_deployment_id,
              "papi_instance_name": "$ec_deployment_name"
            }
          }
        EOF
          }

          response=$(curl -fLSs -XPOST "https://circleci.com/api/v2/project/github/Jahia/cloud-ec-infra/pipeline" \
                        -H "Circle-Token: $__secret__CCToken" --header 'content-type: application/json' --data "$(generate_payload)")
          if [ $? -ne 0 ]; then
            echo "Pipeline trigger failed: $response"
            exit 1
          fi

          pipelineId=$(echo $response | jq -r .id)
          sleep 1  # it seems to take a few milliseconds for the pipeline to really be created in CCI...
          response=$(curl -LSs "https://circleci.com/api/v2/pipeline/$pipelineId/workflow" -H "Circle-Token: $__secret__CCToken")
          if [ "$(echo $response | jq -r '.items[0].status')" == "null" ]; then
            echo "No workflow created on CircleCI, aborting. Pipeline ID: $pipelineId. Response: $response" >&2
            exit 1
          fi

          # Wait for max 10 minutes for EC deployment update to finish then restore the snapshot anyways
          timeout=600 # 10 minutes
          sleep_interval=20
          workflow_status="running"
          while [ "$workflow_status" = "running" ]; do
            sleep $sleep_interval
            response=$(curl -fLSs "https://circleci.com/api/v2/pipeline/$pipelineId/workflow" -H "Circle-Token: $__secret__CCToken")
            if [ $? -eq 0 ]; then
              new_workflow_status=$(echo $response | jq -r '.items[0].status')
              if [ "$new_workflow_status" != "null" ]; then
                workflow_status=$new_workflow_status
              fi
            fi
            if [ $timeout -lt 1 ]; then
              break
            fi
            ((timeout-=$sleep_interval))
          done
        fi

        return_code=$(curl -sS -o $output_file -w '%{http_code}' \
          -H 'Content-Type: application/json' \
          -u $__secret__ec_admin_credentials \
          -XPOST "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/${timestamp}_${settings.backtype}/_restore" \
          -d'{
            "indices": "-global-geonameentry*,-context-geonameentry*",
            "rename_pattern": "(.*)(__[a-z0-9]+|^context)-(.*)",
            "rename_replacement": "'${UNOMI_ELASTICSEARCH_INDEXPREFIX}'-$3"
          }')
        exit=0
        if [ $return_code -ne 200 ]; then
          cat $output_file | tee -a ${globals.logsPath}
          exit=1
        fi
        rm -f $output_file
        exit $exit
    - if ("${response.out}" != ""):
        - return:
            type: error
            message: "An error occurred during the backup restore process."

  restoreHaproxy:
    - cmd [bl]: |-
        __secret__AWS_ACCESS_KEY_ID="${globals.__secret__backrestAwsAccessKeyId}"
        __secret__AWS_SECRET_ACCESS_KEY="${globals.__secret__backrestAwsSecretAccessKey}"

        BACKUP_FILE=haproxy.tar.gz
        AUTH_BASIC=auth_basic
        CUSTOMER_RULES_FILE=customer_rules.cfg
        HAPROXY_CONF_DIR=/etc/haproxy/haproxy.cfg.jahia
        HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME=customer.errorpages.d
        HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME=http-errors.cfg
        HAPROXY_CONF=$HAPROXY_CONF_DIR/jahia-cloud.cfg
        HAPROXY_CUSTOMER_CONF_DIR_NAME=customer.configuration.d
        HAPROXY_CUSTOMER_CONF_DIR=$HAPROXY_CONF_DIR/$HAPROXY_CUSTOMER_CONF_DIR_NAME
        HAPROXY_CUSTOMER_ERROR_PAGES_CONF=$HAPROXY_CONF_DIR/$HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME
        HAPROXY_CUSTOMER_ERROR_PAGES_DIR=$HAPROXY_CONF_DIR/$HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME

        export AWS_ACCESS_KEY_ID="$__secret__AWS_ACCESS_KEY_ID" AWS_SECRET_ACCESS_KEY="$__secret__AWS_SECRET_ACCESS_KEY"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f haproxy.tar.gz 2>>${globals.logsPath}

        rm -f $HAPROXY_CUSTOMER_CONF_DIR/* $HAPROXY_CUSTOMER_ERROR_PAGES_DIR/* $HAPROXY_CUSTOMER_ERROR_PAGES_CONF 2>/dev/null
        tar -xzf $BACKUP_FILE
        # Rules
        mv $HAPROXY_CUSTOMER_CONF_DIR_NAME/* $HAPROXY_CUSTOMER_CONF_DIR 2>/dev/null

        if [ -d $HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME ]; then
          mv $HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME/* $HAPROXY_CUSTOMER_ERROR_PAGES_DIR/ 2>/dev/null
          mv $HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME $HAPROXY_CUSTOMER_ERROR_PAGES_CONF 2>/dev/null
        else
          echo "Legacy haproxy backup without error page customization. Fetching default config" >> ${globals.logsPath}
          mkdir -p $HAPROXY_CUSTOMER_ERROR_PAGES_DIR
          curl -fLSso $HAPROXY_CUSTOMER_ERROR_PAGES_DIR/502.http ${globals.repoRootUrl}/assets/haproxy/502.http || exit 1
          curl -fLSso $HAPROXY_CUSTOMER_ERROR_PAGES_CONF ${globals.repoRootUrl}/assets/haproxy/http-errors.cfg || exit 1
        fi

        # Basic auth
        old_auth_basic_disabled=1
        user=env-admin
        password=XXXXXXX
        if [ -s $AUTH_BASIC ]; then
          old_auth_basic_disabled=0
          user=$(awk '{print $1}' $AUTH_BASIC)
          password=$(awk '{print $2}' $AUTH_BASIC)
        fi
        rm -rf $HAPROXY_CUSTOMER_CONF_DIR_NAME $BACKUP_FILE $AUTH_BASIC $HAPROXY_CUSTOMER_ERROR_PAGES_DIR_NAME $HAPROXY_CUSTOMER_ERROR_PAGES_CONF_NAME
        # Basic auth
        sed -i "s;^\(\s*user \).*\( password \).*$;\1$user\2$password;" $HAPROXY_CONF
        auth_basic_disabled=$(grep -c  "#acl tools.*#HTTP_AUTH_BASIC" $HAPROXY_CONF)
        [ $old_auth_basic_disabled -eq $auth_basic_disabled ] && exit 0
        if [ $old_auth_basic_disabled -eq 1 ]; then
          sed -i "s;^\(\s*\)\(.*#HTTP_AUTH_BASIC.*\)$;\1#\2;" $HAPROXY_CONF
        else
          sed -i "s;^\(\s*\)#\+\(.*#HTTP_AUTH_BASIC.*\)$;\1\2;" $HAPROXY_CONF
        fi
        # Replace JSESSIONID and use HAProxy dedicated cookie
        if (grep -qw 'JSESSIONID' $HAPROXY_CONF); then
          cookie_line1="use_backend proc if { cook(SERVERID),lower -m beg s${nodes.proc.first.id} }"
          cookie_line2="cookie SERVERID insert nocache httponly secure"
          sed -i "/use_backend proc.*/c \ \ \ \ $cookie_line1" $HAPROXY_CONF
          sed -i "/cookie JSESSIONID.*/c \ \ \ \ $cookie_line2" $HAPROXY_CONF
        fi
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring haproxy configuration."
    - cmd [bl]: sudo service haproxy restart

  restoreMariadb:
    - cmd[sqldb]: |-
        __secret__AWS_ACCESS_KEY_ID="${globals.__secret__backrestAwsAccessKeyId}"
        __secret__AWS_SECRET_ACCESS_KEY="${globals.__secret__backrestAwsSecretAccessKey}"
        export AWS_ACCESS_KEY_ID="$__secret__AWS_ACCESS_KEY_ID" AWS_SECRET_ACCESS_KEY="$__secret__AWS_SECRET_ACCESS_KEY"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f database.tar 2>>${globals.logsPath}
        ls database.tar || exit 0

        sudo service mysql stop
        if (pgrep mysqld > /dev/null); then
          sudo pkill -e -9 mysqld
        fi
        rm -rf /var/lib/mysql/*
        cd /var/lib/mysql/
        mv /home/jelastic/jelastic_backup/database.tar .
        tar -xf database.tar 2>>${globals.logsPath}
        rm database.tar
        mariabackup --decompress --parallel=2 --remove-original --target-dir . 2>>${globals.logsPath} || exit 1
        mariabackup --prepare --target-dir . 2>>${globals.logsPath} || exit 1

    - if (nodes.sqldb.length > 1):
        cmd[${nodes.sqldb.first.id}]: |-
          echo "# GALERA saved state" > /var/lib/mysql/grastate.dat
          echo "version: 2.1" >> /var/lib/mysql/grastate.dat
          echo "seqno: -1" >> /var/lib/mysql/grastate.dat
          echo "safe_to_bootstrap: 1" >> /var/lib/mysql/grastate.dat

    - forEach(i:nodes.sqldb):
        startGaleraNode: ${@i.id}

    - cmd[${nodes.sqldb.first.id}]: |-
        # reset jahia user and datadog user password
        EXISTING_JAHIA_USER=$(mysql -sNe "select user from mysql.user where user like 'jahia-db-%'")
        mysql -e "DROP USER '${EXISTING_JAHIA_USER}'@'%'; flush privileges"
        mysql -e "CREATE USER '${DB_USER}'@'%' identified by '${DB_PASS}'"
        mysql -e "grant all privileges on jahia.* to '${DB_USER}'@'%'"
        mysql -e "set password for 'datadog'@'localhost' = PASSWORD('${DB_USER_DATADOG}')"
        mysql -e "flush privileges"

        if (( $(mysql -Nse "select count(user) from mysql.user where user='datadog' and host='%'") == 0 )); then
          mysql -e "CREATE USER 'datadog'@'%' IDENTIFIED BY '${DB_USER_DATADOG}'"
          mysql -e "GRANT SELECT ON jahia.JR_J_LOCAL_REVISIONS TO 'datadog'@'%'"
        else
          mysql -e "set password for 'datadog'@'%' = PASSWORD('${DB_USER_DATADOG}')"
          mysql -e "flush privileges"
        fi

        # Now recreate trigger(s)
        # dump trigger(s):
        mysqldump --no-data --no-create-info --skip-routines --triggers --add-drop-trigger --compact jahia > /tmp/triggers_dump.sql
        # change DEFINER(s) if matching /^jahia-db-[0-9]+$/
        sed -i -r "s|(DEFINER=\`)jahia-db-[0-9]+|\1$DB_USER|" /tmp/triggers_dump.sql
        # let's import this
        mysql jahia < /tmp/triggers_dump.sql
        rm /tmp/triggers_dump.sql

  restoreJcustomer:
    - cmd [cp]: |-
        __secret__AWS_ACCESS_KEY_ID="${globals.__secret__backrestAwsAccessKeyId}"
        __secret__AWS_SECRET_ACCESS_KEY="${globals.__secret__backrestAwsSecretAccessKey}"
        export AWS_ACCESS_KEY_ID="$__secret__AWS_ACCESS_KEY_ID" AWS_SECRET_ACCESS_KEY="$__secret__AWS_SECRET_ACCESS_KEY"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f jcustomer-env-vars.gz 2>>${globals.logsPath}
        ls jcustomer-env-vars.gz || exit 0
    - if ("${response.errOut}" == ""):
        - cmd [cp]: |-
            cd jelastic_backup
            gunzip jcustomer-env-vars.gz
            echo "Restoring jcustomer env vars" >> ${globals.logsPath}
            # Exclude UNOMI_ELASTICSEARCH env vars if backup pre EC release
            grep -Ev '(^UNOMI_ELASTICSEARCH|^UNOMI_THIRDPARTY_PROVIDER1_)' jcustomer-env-vars > /tmp/jcustomer-env-vars
            rm -f jcustomer-env-vars
            grep -vP '^UNOMI_(?!ELASTICSEARCH|THIRDPARTY_PROVIDER1)' /.jelenv  >> /tmp/jcustomer-env-vars
            cat /tmp/jcustomer-env-vars > /.jelenv
            rm -f /tmp/jcustomer-env-vars
          user: root
        - if ("${response.errOut}" != ""):
            - return:
                type: error
                message: "An error occurred while restoring jcustomer environment variables. ${response.errOut}"
        - checkIfReindexationIsNeeded
        - api: env.control.RestartNodes
          envName: ${env.envName}
          nodeGroup: cp
    - else:
        log: "No env var backup available"

  checkIfReindexationIsNeeded:
    - install:
        jps: "${globals.repoRootUrl}/packages/jcustomer/reindexation.yml"

settings:
  fields:
    - name: backup_name
      type: string
      caption: Backup Name
      vtype: text
      required: true
    - name: source_env
      type: envlist
      caption: backup from ?
      valueField: appid
      editable: true
    - name: cloud_source
      type: string
      caption: cloud source ?
    - name: region_source
      type: string
      caption: region_source ?
    - name: uid_source
      type: string
      caption: uid_source ?
    - name: envrole_source
      type: list
      caption: envrole_source ?
      values:
        dev: dev
        prod: prod
    - name: timestamp
      caption: timestamp in format %Y-%m-%dT%H:%M:00
      required: true
      type: string
    - name: backtype
      caption: is this a manual or auto backup
      type: string
      default: manual
