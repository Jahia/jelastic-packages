---
type: update
version: 1.5
name: Jahia - Restores an environment from a bucket
logo: ../../assets/common/jahia-logo-70x70.png
id: jahia-restore-from-bucket

globals:
  logAction: "Restore"
  logsPath: "/var/log/jelastic-packages/restore.log"

mixins:
  - ../../mixins/common.yml
  - ../../mixins/elasticsearch.yml
  - ../../mixins/jahia.yml
  - ../../mixins/jcustomer.yml
  - ../../mixins/mariadb.yml

onInstall:
  - muteDatadogHost:
      target: "*"
      duration: 240 # 4h
  - setGlobalRepoRootUrl
  - getBackrestAwsAccessKey
  - if(settings.cloud_source):
      - if(settings.region_source):
          - script: |
                var region = '${settings.region_source}'
                return {'result': 0, 'resp': region.replace(/(\W|_)+/g, '')}
          - setGlobals:
              wc_region_source: ${response.resp}
          - if(settings.envrole_source):
              - if(settings.uid_source):
                  - setGlobals:
                      bucketname: jc${settings.envrole_source}${settings.uid_source}${globals.wc_region_source} -F ${settings.cloud_source},${settings.region_source},${settings.envrole_source}
              - setGlobals:
                  regionRealName_source: ${settings.region_source}
                  cloudProvider_source: ${settings.cloud_source}

  - if(settings.source_env):
      - envSource
      - setGlobals:
          bucketname: jc${globals.envRole_source}${env.uid}${globals.region_source} -F ${globals.cloudProvider_source},${globals.regionRealName_source},${settings.envrole_source}
  - if(!settings.source_env):
      - if(!settings.envrole_source):
          - setGlobals:
              bucketname: jc${cluster_role}${env.uid}${env_region}

  - if (nodes.proc):  # Jahia
      - muteDatadogSynthetics:
          duration: 240 # 4h
      - clearJelasticLogs:
          target: bl
          user: haproxy
          logsPath: ${globals.logsPath}
      - clearJelasticLogs:
          target: "proc, cp"
          user: tomcat
          logsPath: ${globals.logsPath}
      - clearJelasticLogs:
          target: "sqldb"
          user: mysql
          logsPath: ${globals.logsPath}
      - installBackupTools:
          target: proc,cp,sqldb
          logAction: ${globals.logAction}
      - if (nodes.bl):
          - installBackupTools:
              target: bl
              logAction: ${globals.logAction}
      - initialJexperienceStatus
      - restoreJahia
      - if (nodes.bl):
          - restoreHaproxy
      - unmuteDatadogSynthetics
  - else:  # Jcustomer
      - clearJelasticLogs:
          target: cp
          user: root
          logsPath: ${globals.logsPath}
      - installBackupTools:
          target: cp
          logAction: ${globals.logAction}
      # The following action will be removed once all JC/AG environments are migrated to elastic cloud
      - restorePreECMigrationJCustomerEnv
      - restoreElasticsearch
      - restoreJcustomer
      - checkJcustomerHealthWhenStarting: cp
  - unmuteDatadogHost:
      target: "*"


actions:
  getJexperienceStatus:
  # Parameters:
  #   - step: on wich step this action is launched
    - cmd[proc]: |-
        module="dx:org.jahia.modules/jexperience"
        # following will output:
        #   - json-like with key "ACTIVE"=false if jexperience module isn't there or not in "ACTIVE" stat
        #   - a json-like string if a jexperience module is in "ACTIVE" stat '{"version": "x.y.z", "ACTIVE": true}'
        #   (cf: https://docs.osgi.org/javadoc/r3/constant-values.html for status codes)
        # Also feed globals.logPath with jexperience module found and their state
        awk -v moduleName="$module" -v logfile="${globals.logsPath}" \
          'NR=2 && $1 ~ moduleName { \
              split($1,modinfo,"/"); \
              getline; \
              printf "Found module %s v%s with status %s\n",modinfo[2],modinfo[3],$1 >> logfile; \
              if($1==32){ \
                version=modinfo[3]; \
                ACTIVE="true" \
              } \
            } \
            END{ \
              if(ACTIVE=="true"){ \
                printf "{\"version\": \"%s\", \"ACTIVE\": %s }",version,ACTIVE \
              } \
              else{ \
                printf "{\"ACTIVE\": false}" \
              } \
            }' \
          /data/digital-factory-data/bundles-deployed/*/bundle.info
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while fetching jExperience information ${this.step}: ${response.errOut}"
    # the following script is only here to transform the response string to a real json object
    - script: |-
        const rep = ${response.out.toJSON()}
        const result = {"result": 0}
        return Object.assign({}, result, rep)

  initialJexperienceStatus:
    - getJexperienceStatus:
        step: "before restore"
    - setGlobals:  # must be set for following conditionnal test, even if false
        jexperience_active: ${response.ACTIVE}
    - if (${globals.jexperience_active}):
        - setGlobals:
            jexperience_active_version: ${response.version}
        # we keep the jexperience conf in a barely safe spot
        - cmd[proc,cp]: |-
            cp -p /data/digital-factory-data/karaf/etc/org.jahia.modules.jexperience.settings-global.cfg /root
          user: root

  restoreJahia:
    - cmd [proc,cp]: |-
        ## [${globals.logAction}] - 2/5
        sudo service tomcat stop
        export AWS_ACCESS_KEY_ID="${globals.backrestAwsAccessKeyId}" AWS_SECRET_ACCESS_KEY="${globals.backrestAwsSecretAccessKey}"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f digital-factory-data.tar.gz 2>>${globals.logsPath} || exit 1
        sudo rm -rf /data/digital-factory-data
        sudo chown tomcat:tomcat /data
        tar xf digital-factory-data.tar.gz -C / 2>>${globals.logsPath} || exit 1
        rm digital-factory-data.tar.gz
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring jahia data. ${response.errOut}"
    # set default value for the management of jexperience
    - set:
        remove_restored_jexperience: false
        reinstall_jexperience: false
    # check is restored backup is also with jexperience active
    - getJexperienceStatus:
        step: "after digital-factory-data restore"
    - set:  # must be set for following conditionnal test, even if false
        restored_linked: ${response.ACTIVE}
    - if (${this.restored_linked}):
        - set:
            restored_active_version: ${response.version}
    - if (${globals.jexperience_active} && ${this.restored_linked}):
        # check is restored active jexperience module version is the same
        - if ("${globals.jexperience_active_version}" != "${this.restored_active_version}"):
            - cmd[proc]: |-
                echo "restored ACTIVE jexperience is v${this.restored_active_version} while v${globals.jexperience_active_version} was ACTIVE before the restore" >> ${globals.logsPath}
            - set:
                reinstall_jexperience: true  # jexperience need to be reinstalled with the initial version
        - else:
            - cmd[proc]: |-
                echo "restored ACTIVE jexperience is v${this.restored_active_version} as the ACTIVE module before the restore was" >> ${globals.logsPath}
    - elif (!${globals.jexperience_active} && ${this.restored_linked}):  # we don't have to keep the restored jexperience
        - set:
            remove_restored_jexperience: true
    - elif (${globals.jexperience_active} && !${this.restored_linked}):  # we have to reinstall jexperience module
        - set:
            reinstall_jexperience: true

    - cmd [proc,cp]: |-
        ## [${globals.logAction}] - 3/5
        export AWS_ACCESS_KEY_ID="${globals.backrestAwsAccessKeyId}" AWS_SECRET_ACCESS_KEY="${globals.backrestAwsSecretAccessKey}"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f digital-factory-config.tar.gz 2>>${globals.logsPath} || exit 1
        rm -rf /opt/tomcat/conf/digital-factory-config
        tar xf digital-factory-config.tar.gz -C / 2>>${globals.logsPath} || exit 1
        chown tomcat:tomcat -R /opt/tomcat/conf
        rm -f digital-factory-config.tar.gz
        rm -rf /data/digital-factory-data/repository/.lock /data/digital-factory-data/repository/workspace /data/digital-factory-data/repository/index
        touch /data/digital-factory-data/safe-env-clone
        short_name=$(echo ${_ROLE}.$HOSTNAME | sed -r 's/^([a-Z]+)\.[a-Z]+([0-9]+)-.+$/\1.\2/' | tr 'A-Z' 'a-z')
        sed -e "s|^cluster.node.serverId.*|cluster.node.serverId = $short_name|" \
            -e "s|^jahia.session.jvmRoute.*|jahia.session.jvmRoute = $short_name|" \
            -i $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring jahia configuration. ${response.errOut}"

    - cmd [proc,cp]: |-
        export AWS_ACCESS_KEY_ID="${globals.backrestAwsAccessKeyId}" AWS_SECRET_ACCESS_KEY="${globals.backrestAwsSecretAccessKey}"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f jahia-env-vars.gz 2>>${globals.logsPath} || exit 1
        ls jahia-env-vars.gz || exit 0
    - if ("${response.errOut}" == ""):
        - cmd [proc,cp]: |-
            cd jelastic_backup
            gunzip jahia-env-vars.gz
            mv jahia-env-vars /tmp/
            sed -i "/^jahia_cfg_mvnPath/d" /tmp/jahia-env-vars
            grep "jahia_cfg_mvnPath" /.jelenv >> /tmp/jahia-env-vars
            grep -vE "^(jahia|tomcat)_cfg_" /.jelenv >> /tmp/jahia-env-vars
        - cmd [proc,cp]: |-
            cat /tmp/jahia-env-vars > /.jelenv
            rm -f /tmp/jahia-env-vars
          user: root
        - if ("${response.errOut}" != ""):
            - return:
                type: error
                message: "An error occurred while restoring jahia environment variables. ${response.errOut}"
    - else:
        log: "No env var backup available"

    - cmd [cp]:
        - sed -i "s|^processingServer.*|processingServer = false|g" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
    - restoreMariadb
    - cleanJRLocalRevisionsTable
    - getPatTokenAndKey
    - cmd [proc]: |-
        ## [${globals.logAction}] - 5/5
        groovy_file_path=/data/digital-factory-data/patches/groovy/pat.groovy

        # Clean up any possible remainder of previous script execution
        rm -f ${groovy_file_path}*

        # Replace Pat Token if needed and start tomcat

        jahia_major_version=$(echo "$DX_VERSION" | cut -d'.' -f1)
        if [ $jahia_major_version -eq 7 ]; then
          jahia_7_parameter=", null"
        else
          jahia_7_parameter=""
        fi

        echo """
          org.jahia.services.content.JCRTemplate.getInstance().doExecuteWithSystemSession({ session ->
            def service = org.jahia.osgi.BundleUtils.getOsgiService(\"org.jahia.modules.apitokens.TokenService\"$jahia_7_parameter)
            def tokenExists = service.verifyToken(\"${globals.__secret__pat_token}\", session)
            if (! tokenExists) {
               def tokenDetails = service.getTokenDetails(\"/users/root\", \"Jahia Cloud Token\", session)
               service.deleteToken(tokenDetails.key, session)
               service.tokenBuilder(\"/users/root\", \"Jahia Cloud Token\", session)
                      .setToken(\"${globals.__secret__pat_token}\")
                      .setActive(true)
                      .create()
                  session.save();
            }
          })
        """ > ${groovy_file_path}

        sudo service tomcat start

    - checkPatGroovyScriptExecution
    - startupJahiaHealthCheck: proc

    - cmd [cp]:
        - sudo service tomcat start
    - startupJahiaHealthCheck: cp

    # if the env intialy don't have jexperience active,
    # then we clean the conf and nodegroups data
    - if (${this.remove_restored_jexperience}):
        - log: Remove env link
        - removeAndCleanJexperience
    - else:
        - log: Keep env link

    # if jexperience was intialy active, we restore the conf file
    - log: "jExperience was active before the restore: ${globals.jexperience_active}"
    - if (${globals.jexperience_active}):
        - log: "Initial jExperience conf needs to be restored"
        - cmd[proc, cp]: |-
            mv /root/org.jahia.modules.jexperience.settings-global.cfg /data/digital-factory-data/karaf/etc/org.jahia.modules.jexperience.settings-global.cfg
          user: root

    # if the restored jexperience isn't the same as the intiale one,
    # then we install the initial
    - log: "jExperience has to be reinstalled: ${this.reinstall_jexperience}"
    - if (${this.reinstall_jexperience}):
        - cmd[proc]: |-
            echo "Upgrade jexperience module to initial v${globals.jexperience_active_version} now" >> ${globals.logsPath}
        - log: "Install jExperience v${globals.jexperience_active_version}"
        - upgradeModule:
            moduleSymname: jexperience
            moduleVersion: ${globals.jexperience_active_version}

    # if the env intialy don't have jexperience active,
    # then we clean the conf and nodegroups data
    - log: "Restored jExperience has to be removed: ${this.remove_restored_jexperience}"
    - if (${this.remove_restored_jexperience}):
        - removeAndCleanJexperience

  envSource:
    - script: |
          var envInfo = jelastic.env.control.getenvinfo('${settings.source_env}', session)
          for (var i = 0, n = envInfo.nodes; i < n.length; i++) {
            if (n[i].nodeGroup == 'cp') {
              var nodeID = n[i].id;
              break;
            }
          }
          var metadata = jelastic.env.file.read('${settings.source_env}', session, '/metadata_from_HOST', null, null, nodeID).body.toString()

          var re = /(\S|\n|\r)*JEL_REGION=(\S+)(\S|\n|\r)*/
          var regionRealName = metadata.replace(re, '$2')
          var region = regionRealName.replace(/(\W|_)+/g, '')

          var re = /(\S|\n|\r)*JEL_CLOUDPROVIDER=(\S+)(\S|\n|\r)*/
          var cloudProvider = metadata.replace(re, '$2')

          var re = /(\S|\n|\r)*JEL_AVAILABILITYZONE=(\S+)(\S|\n|\r)*/
          var az = metadata.replace(re, '$2')

          var re = /(\S|\n|\r)*JEL_ENV_ROLE=(\S+)(\S|\n|\r)*/
          var envRole = metadata.replace(re, '$2')

          return {'result': 0,
            'region': region,
            'regionRealName': regionRealName,
            'cloudProvider': cloudProvider,
            'az': az,
            'envRole': envRole}
    - setGlobals:
        region_source: ${response.region}
        regionRealName_source: ${response.regionRealName}
        cloudProvider_source: ${response.cloudProvider}
        az_source: ${response.az}
        envRole_source: ${response.envRole}

  restoreElasticsearch:
    - script: |-
        // Extract only bucketname in case of foreign restore
        var account="${globals.bucketname}";
        account=account.split(" ")[0];
        return {'result': 0,'account': account}
    - setGlobals:
        account: ${response.account}
        region: ${globals.regionRealName_source}

    - cmd[${nodes.cp.first.id}]: |-
        curl -s \
          -u $UNOMI_ELASTICSEARCH_USERNAME:$UNOMI_ELASTICSEARCH_PASSWORD \
          https://$UNOMI_ELASTICSEARCH_ADDRESSES | python -c "import sys, json; print(json.load(sys.stdin)['version']['number'])" 2>>${globals.logsPath} || exit 1
    - setGlobals:
        esVersion: ${response.out}
    - getECAdminCredentials

    - if ("${globals.cloudProvider_source}" == "aws" || "${globals.cloudProvider_source}" == "ovh"):
        - if ("${globals.cloudProvider_source}" == "aws"):
            - set:
                region: ${globals.region}
        - else:
            - set:
                region: "eu-west-1"
        - setAwsSnapshotRepository:
            repositoryName: ${env.shortdomain}
            backupName: ${settings.backup_name}
            region: ${this.region}
            account: ${globals.account}
            logsPath: ${globals.logsPath}
            awsAccessKeyId: ${globals.backrestAwsAccessKeyId}
            awsSecretAccessKey: ${globals.backrestAwsSecretAccessKey}
    - else:
        - setAzureSnapshotRepository:
            repositoryName: ${env.shortdomain}
            awsAccessKeyId: ${globals.backrestAwsAccessKeyId}
            awsSecretAccessKey: ${globals.backrestAwsSecretAccessKey}
            operation: "restore"
            account: ${globals.account}
            backupName: ${settings.backup_name}
            logsPath: ${globals.logsPath}

    - cmd[${nodes.cp.first.id}]: |-
        ## [${globals.logAction}] - 4/5
        output_file=$(mktemp)
        ec_admin_credentials=${globals.ecAdminCredentials}
        timestamp=$(echo "${settings.timestamp}"| awk '{print tolower($0)}')
        snapshot_state=$(curl -sS -u $ec_admin_credentials \
          "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/${timestamp}_${settings.backtype}" | jq -r ".snapshots[0].state")
        if [[ "$snapshot_state" != "SUCCESS" ]]; then
          echo "The Elasticsearch snapshot you are trying to restore is not complete, aborting" >&2
          exit 1
        fi
        curl -sSo /dev/null \
          -u $ec_admin_credentials \
          -X DELETE "https://$UNOMI_ELASTICSEARCH_ADDRESSES/${UNOMI_ELASTICSEARCH_INDEXPREFIX}-*"
        return_code=$(curl -sS -o $output_file -w '%{http_code}' \
          -H 'Content-Type: application/json' \
          -u $ec_admin_credentials \
          -XPOST "https://$UNOMI_ELASTICSEARCH_ADDRESSES/_snapshot/${env.shortdomain}/${timestamp}_${settings.backtype}/_restore" \
          -d'{
            "indices": "-global-geonameentry*,-context-geonameentry*",
            "rename_pattern": "(.*)(__jc|__as|^context)-(.*)",
            "rename_replacement": "'${UNOMI_ELASTICSEARCH_INDEXPREFIX}'-$3"
          }')
        exit=0
        if [ $return_code -ne 200 ]; then
          cat $output_file | tee -a ${globals.logsPath}
          exit=1
        fi
        rm -f $output_file
        exit $exit
    - if ("${response.out}" != ""):
        - return:
            type: error
            message: "An error occurred during the backup restore process."

  restoreHaproxy:
    - cmd [bl]: |-
        export AWS_ACCESS_KEY_ID="${globals.backrestAwsAccessKeyId}" AWS_SECRET_ACCESS_KEY="${globals.backrestAwsSecretAccessKey}"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f haproxy.tar.gz 2>>${globals.logsPath}
        [ -f haproxy.tar.gz ] && exit 0
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f haproxy-00-global.cfg.gz 2>>${globals.logsPath} || exit 1
        ls haproxy-00-global.cfg.gz || exit 0
    - if ("${response.errOut}" == ""):
        restoreHaproxyIfWorkaround:
    - else:
        log: "No haproxy configuration backup available"

  restoreHaproxyIfWorkaround:
    - cmd [bl]: |-
        OLD_HAPROXY_CONF=haproxy-00-global.cfg
        BACKUP_FILE=haproxy.tar.gz
        AUTH_BASIC=auth_basic
        CUSTOMER_RULES_FILE=customer_rules.cfg
        HAPROXY_CONF_DIR=/etc/haproxy/haproxy.cfg.jahia
        HAPROXY_CONF=$HAPROXY_CONF_DIR/jahia-cloud.cfg
        HAPROXY_CUSTOMER_CONF_DIR_NAME=customer.configuration.d
        HAPROXY_CUSTOMER_CONF_DIR=$HAPROXY_CONF_DIR/$HAPROXY_CUSTOMER_CONF_DIR_NAME
        rm -f $HAPROXY_CUSTOMER_CONF_DIR/* 2>/dev/null
        cd jelastic_backup
        if [ -f $OLD_HAPROXY_CONF.gz ]; then
          gunzip $OLD_HAPROXY_CONF.gz
          # Rewrite rules
          awk '/## START_REWRITES ##/{flag=1; next} /## END_REWRITES ##/{flag=0} flag' $OLD_HAPROXY_CONF > $CUSTOMER_RULES_FILE
          [ -s $CUSTOMER_RULES_FILE ] && mv $CUSTOMER_RULES_FILE $HAPROXY_CUSTOMER_CONF_DIR || rm -f $CUSTOMER_RULES_FILE
          # Basic auth
          old_auth_basic_disabled=$(grep -c  "#acl tools.*#HTTP_AUTH_BASIC" $OLD_HAPROXY_CONF)
          user=$(awk '/user .* password .*/ {print $2}' $OLD_HAPROXY_CONF)
          password=$(awk '/user .* password .*/ {print $4}' $OLD_HAPROXY_CONF)
          rm -f $OLD_HAPROXY_CONF
        else
          tar -xzf $BACKUP_FILE
          # Rules
          mv $HAPROXY_CUSTOMER_CONF_DIR_NAME/* $HAPROXY_CUSTOMER_CONF_DIR 2>/dev/null
          # Basic auth
          old_auth_basic_disabled=1
          user=env-admin
          password=XXXXXXX
          if [ -s $AUTH_BASIC ]; then
            old_auth_basic_disabled=0
            user=$(awk '{print $1}' $AUTH_BASIC)
            password=$(awk '{print $2}' $AUTH_BASIC)
          fi
          rm -rf $HAPROXY_CUSTOMER_CONF_DIR_NAME $BACKUP_FILE $AUTH_BASIC
        fi
        # Basic auth
        sed -i "s;^\(\s*user \).*\( password \).*$;\1$user\2$password;" $HAPROXY_CONF
        auth_basic_disabled=$(grep -c  "#acl tools.*#HTTP_AUTH_BASIC" $HAPROXY_CONF)
        [ $old_auth_basic_disabled -eq $auth_basic_disabled ] && exit 0
        if [ $old_auth_basic_disabled -eq 1 ]; then
          sed -i "s;^\(\s*\)\(.*#HTTP_AUTH_BASIC.*\)$;\1#\2;" $HAPROXY_CONF
        else
          sed -i "s;^\(\s*\)#\+\(.*#HTTP_AUTH_BASIC.*\)$;\1\2;" $HAPROXY_CONF
        fi
        # Replace JSESSIONID and use HAProxy dedicated cookie
        if (grep -qw 'JSESSIONID' $HAPROXY_CONF); then
          cookie_line1="use_backend proc if { cook(SERVERID),lower -m beg s${nodes.proc.first.id} }"
          cookie_line2="cookie SERVERID insert nocache httponly secure"
          sed -i "/use_backend proc.*/c \ \ \ \ $cookie_line1" $HAPROXY_CONF
          sed -i "/cookie JSESSIONID.*/c \ \ \ \ $cookie_line2" $HAPROXY_CONF
        fi
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred while restoring haproxy configuration."
    - cmd [bl]: sudo service haproxy restart

  restoreMariadb:
    - cmd[sqldb]: |-
        ## [${globals.logAction}] - 4/5
        export AWS_ACCESS_KEY_ID="${globals.backrestAwsAccessKeyId}" AWS_SECRET_ACCESS_KEY="${globals.backrestAwsSecretAccessKey}"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f database.tar 2>>${globals.logsPath}
        ls database.tar || exit 0
    - if ("${response.errOut}" == ""):
        dbRestore
    - else:
        - log: "DB backup not found, trying legacy"
        - dbLegacyRestore

  dbRestore:
    - cmd[sqldb]: |-
        sudo service mysql stop
        rm -rf /var/lib/mysql/*
        cd /var/lib/mysql/
        mv /home/jelastic/jelastic_backup/database.tar .
        tar -xf database.tar 2>>${globals.logsPath}
        rm database.tar
        mariabackup --decompress --parallel=2 --remove-original --target-dir . 2>>${globals.logsPath} || exit 1
        mariabackup --prepare --target-dir . 2>>${globals.logsPath} || exit 1

    - if (nodes.sqldb.length > 1):
        cmd[${nodes.sqldb.first.id}]: |-
          echo "# GALERA saved state" > /var/lib/mysql/grastate.dat
          echo "version: 2.1" >> /var/lib/mysql/grastate.dat
          echo "seqno: -1" >> /var/lib/mysql/grastate.dat
          echo "safe_to_bootstrap: 1" >> /var/lib/mysql/grastate.dat

    - forEach(i:nodes.sqldb):
        startGaleraNode: ${@i.id}

    - cmd[${nodes.sqldb.first.id}]: |-
        # reset jahia user and datadog user password
        EXISTING_JAHIA_USER=$(mysql -sNe "select user from mysql.user where user like 'jahia-db-%'")
        mysql -e "DROP USER '${EXISTING_JAHIA_USER}'@'%'; flush privileges"
        mysql -e "CREATE USER '${DB_USER}'@'%' identified by '${DB_PASS}'"
        mysql -e "grant all privileges on jahia.* to '${DB_USER}'@'%'"
        mysql -e "set password for 'datadog'@'localhost' = PASSWORD('${DB_USER_DATADOG}')"
        mysql -e "flush privileges"

        # Now recreate trigger(s)
        # dump trigger(s):
        mysqldump --no-data --no-create-info --skip-routines --triggers --add-drop-trigger --compact jahia > /tmp/triggers_dump.sql
        # change DEFINER(s) if matching /^jahia-db-[0-9]+$/
        sed -i -r "s|(DEFINER=\`)jahia-db-[0-9]+|\1$DB_USER|" /tmp/triggers_dump.sql
        # let's import this
        mysql jahia < /tmp/triggers_dump.sql
        rm /tmp/triggers_dump.sql

  dbLegacyRestore:
    - log: "## TEMPORARY FIX FOR RESTORATION: GRANT ALL ON *.* TO JAHIA'S DB USER"
    # Starting with mariadb 10.4, jahia's db user is no longer root on database
    # resulting in the need to connect as 'mysql' user and temporaly grant
    # jahia's db user in order to successfuly import the dump from processing node.
    # see here for more details: https://mariadb.org/authentication-in-mariadb-10-4/
    # next cmd tests if connexion is ok with 'mysql'@'localhost':
    #   - if ok, so it's >10.4 and we need temporary grants
    - cmd[${nodes.sqldb.master.id}]: |-
          if (mysql -se 'select 1' > /dev/null 2>&1); then
            mysql -e "GRANT ALL PRIVILEGES ON *.* TO '$DB_USER'@'%'"
          fi
      user: mysql
    - cmd [proc]: |-
        export AWS_ACCESS_KEY_ID="${globals.backrestAwsAccessKeyId}" AWS_SECRET_ACCESS_KEY="${globals.backrestAwsSecretAccessKey}"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f jahia.sql.gz 2>>${globals.logsPath} || exit 1
        case ${DB_ENDPOINT} in "mysqldb") mysql_host="mysqldb" ;; "proxy") mysql_host=${PROXYSQL_MASTER_IP} ;; "galera") mysql_host="galera";; *) mysql_host="mysqldb"; esac
        gunzip < jahia.sql.gz | mysql -u$DB_USER -p$DB_PASSWORD -h ${mysql_host} --max_allowed_packet=1024M 2>>${globals.logsPath} || exit 1
        rm -f jahia.sql.gz
        query="delete from JR_J_LOCAL_REVISIONS"
        mysql -h ${mysql_host} -u $DB_USER -p$DB_PASSWORD -s jahia -e "$query" 2>>${globals.logsPath} || exit 1
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred during mysql dump. ${response.errOut}"

    - log: "## TEMPORARY FIX FOR RESTORATION: REVOKE ALL ON *.* TO JAHIA'S DB USER"
    - cmd[${nodes.sqldb.master.id}]: |-
          if (mysql -se 'select 1' > /dev/null 2>&1); then
            mysql -e "REVOKE ALL PRIVILEGES ON *.* FROM '$DB_USER'@'%'"
          fi
      user: mysql

  restoreJcustomer:
    - cmd [cp]: |-
        ## [${globals.logAction}] - 5/5
        export AWS_ACCESS_KEY_ID="${globals.backrestAwsAccessKeyId}" AWS_SECRET_ACCESS_KEY="${globals.backrestAwsSecretAccessKey}"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f jcustomer-env-vars.gz 2>>${globals.logsPath}
        ls jcustomer-env-vars.gz || exit 0
    - if ("${response.errOut}" == ""):
        - cmd [cp]: |-
            cd jelastic_backup
            gunzip jcustomer-env-vars.gz
            echo "Restoring jcustomer env vars" >> ${globals.logsPath}
            # Exclude UNOMI_ELASTICSEARCH env vars if backup pre EC release
            grep -v '^UNOMI_ELASTICSEARCH' jcustomer-env-vars > /tmp/jcustomer-env-vars
            rm -f jcustomer-env-vars
            grep -vP '^UNOMI_(?!ELASTICSEARCH)' /.jelenv  >> /tmp/jcustomer-env-vars
            cat /tmp/jcustomer-env-vars > /.jelenv
            rm -f /tmp/jcustomer-env-vars
          user: root
        - if ("${response.errOut}" != ""):
            - return:
                type: error
                message: "An error occurred while restoring jcustomer environment variables. ${response.errOut}"
        - checkIfReindexationIsNeeded
        - api: env.control.RestartNodes
          envName: ${env.envName}
          nodeGroup: cp
    - else:
        log: "No env var backup available"

  checkIfReindexationIsNeeded:
    - install:
        jps: "${globals.repoRootUrl}/packages/jcustomer/reindexation.yml"

########################################### LEGACY ACTIONS ###########################################
############# TO BE REMOVED WHEN ALL ELASTICSEARCH ARE MIGRATED TO ELASTIC CLOUD #####################
  restorePreECMigrationJCustomerEnv:
    # Get env version
    - script: |-
        const currentVersion = jelastic.env.control.GetNodeGroups("${env.envName}", session).object.filter(function (object) {
            return object.name == "cp";
        }).pop().envVersion;
        return {'result': 0, 'preECMigration': (11 > currentVersion)}

    - if (${response.preECMigration}):
        - LEGACY_restoreElasticsearch
        - LEGACY_restoreJcustomer
        - checkJcustomerHealthWhenStarting: cp
        - return:
            type: success

  LEGACY_restoreElasticsearch:
    - script: |-
        // Extract only bucketname in case of foreign restore
        var account="${globals.bucketname}";
        account=account.split(" ")[0];
        return {'result': 0,'account': account}
    - setGlobals:
        account: ${response.account}
        region: ${globals.regionRealName_source}

    - cmd[${nodes.es.first.id}]: |-
        curl -s "${nodes.es.first.intIP}:9200" | python -c "import sys, json; print(json.load(sys.stdin)['version']['number'])" 2>>${globals.logsPath} || exit 1
    - setGlobals:
        esVersion: ${response.out}

    - if ("${globals.cloudProvider_source}" == "aws" || "${globals.cloudProvider_source}" == "ovh"):
        - LEGACY_setAwsElasticsearchConfig:
            awsAccessKeyId: ${globals.backrestAwsAccessKeyId}
            awsSecretAccessKey: ${globals.backrestAwsSecretAccessKey}
            logsPath: ${globals.logsPath}
        - if ("${globals.cloudProvider_source}" == "aws"):
            - set:
                region: ${globals.region}
        - else:
            - set:
                region: "eu-west-1"
        - LEGACY_setAwsSnapshotRepository:
            backupName: ${settings.backup_name}
            region: ${this.region}
            account: ${globals.account}
            logsPath: ${globals.logsPath}
    - else:
        - LEGACY_setAzureElasticsearchConfig:
            awsAccessKeyId: ${globals.backrestAwsAccessKeyId}
            awsSecretAccessKey: ${globals.backrestAwsSecretAccessKey}
            backupName: ${settings.backup_name}
            operation: "restore"
            account: ${globals.account}
            logsPath: ${globals.logsPath}
        - LEGACY_setAzureSnapshotRepository:
            backupName: ${settings.backup_name}
            logsPath: ${globals.logsPath}

    - cmd[${nodes.es.first.id}]: |-
        timestamp=$(echo "${settings.timestamp}"| awk '{print tolower($0)}')
        index=$(curl -s es:9200/_snapshot/backup_repository/${timestamp}_${settings.backtype} | jq -r .snapshots[0].indices[0])
        # If $index is null, it means that we can't get infos on the backup because it's done with an incompatible version
        if [ "$index" = "null" ]; then
          echo "failed to get snapshot indices" >> ${globals.logsPath}
          echo false
        else
          echo "true"
        fi

    - if ( ! ${response.out}):
        return:
          type: error
          message: "Incompatible backup"

    - cmd[${nodes.es.first.id}]: |-
        ## [${globals.logAction}] - 4/5
        curl -o /dev/null -X DELETE "http://${nodes.es.first.intIP}:9200/*,-context-geonameentry"
        timestamp=$(echo "${settings.timestamp}"| awk '{print tolower($0)}')
        res=$(curl -so /tmp/restore-res  -XPOST "${nodes.es.first.intIP}:9200/_snapshot/backup_repository/${timestamp}_${settings.backtype}/_restore" \
                -H 'Content-Type:application/json' \
                -d'{"indices": "-context-geonameentry*","rename_pattern": "(.*)(__jc|__as)-(.*)","rename_replacement": "context-$3"}')
        return_code=$?
        error=$(grep error /tmp/restore-res)
        res=$(wc -l<<<$error)
        if [ $return_code -ne 00 ] || [ ! -z "$res" ];then echo "$error";fi
        rm -f /tmp/restore-res

    - if ("${response.out}" != ""):
        - return:
            type: error
            message: "An error occurred during the backup restore process."

    - LEGACY_calculateNumberOfReplicas
    - LEGACY_updateReplica:
        replica: ${globals.replica}
        logsPath: ${globals.logsPath}


  LEGACY_restoreJcustomer:
    - cmd [cp]: |-
        ## [${globals.logAction}] - 5/5
        export AWS_ACCESS_KEY_ID="${globals.backrestAwsAccessKeyId}" AWS_SECRET_ACCESS_KEY="${globals.backrestAwsSecretAccessKey}"
        provider=$(awk -F'=' '$1=="JEL_CLOUDPROVIDER" {print $2}' /metadata_from_HOST); if [ "$provider" != "aws" ]; then aws_region='eu-west-1'; else aws_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST); fi; export AWS_DEFAULT_REGION="$aws_region"
        cluster_role=$(awk -F'=' '$1=="JEL_ENV_ROLE" {print $2}' /metadata_from_HOST); export cluster_role
        env_region=$(awk -F'=' '$1=="JEL_REGION" {print $2}' /metadata_from_HOST | sed 's/[[:punct:]]//g'); export env_region
        cd jelastic_backup
        python3 backrest.py -a download --backupname ${settings.backup_name} --bucketname ${globals.bucketname} -m ${settings.backtype} -t ${settings.timestamp} -f jcustomer-env-vars.gz 2>>${globals.logsPath}
        ls jcustomer-env-vars.gz || exit 0
    - if ("${response.errOut}" == ""):
        - cmd [cp]: |-
            cd jelastic_backup
            gunzip jcustomer-env-vars.gz
            echo "Restoring jcustomer env vars" >> ${globals.logsPath}
            grep -v '^UNOMI_ELASTICSEARCH' jcustomer-env-vars > /tmp/jcustomer-env-vars
            rm -f jcustomer-env-vars
            grep -vP '^UNOMI_(?!ELASTICSEARCH)' /.jelenv  >> /tmp/jcustomer-env-vars
            cat /tmp/jcustomer-env-vars > /.jelenv
            rm -f /tmp/jcustomer-env-vars
          user: root
        - if ("${response.errOut}" != ""):
            - return:
                type: error
                message: "An error occurred while restoring jcustomer environment variables. ${response.errOut}"
        - api: env.control.RestartNodes
          envName: ${env.envName}
          nodeGroup: cp
    - else:
        log: "No env var backup available"

settings:
  fields:
    - name: backup_name
      type: string
      caption: Backup Name
      vtype: text
      required: true
    - name: source_env
      type: envlist
      caption: backup from ?
      valueField: appid
      editable: true
    - name: cloud_source
      type: string
      caption: cloud source ?
    - name: region_source
      type: string
      caption: region_source ?
    - name: uid_source
      type: string
      caption: uid_source ?
    - name: envrole_source
      type: list
      caption: envrole_source ?
      values:
        dev: dev
        prod: prod
    - name: timestamp
      caption: timestamp in format %Y-%m-%dT%H:%M:00
      required: true
      type: string
    - name: backtype
      caption: is this a manual or auto backup
      type: string
      default: manual
