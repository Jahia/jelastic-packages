---
# Depends on:
#   - common.yml

actions:
  setupES:
    cmd[${this}]: |-
      /usr/share/elasticsearch/bin/elasticsearch --version
      es_major_version=$(/usr/share/elasticsearch/bin/elasticsearch --version | cut -d"." -f1 | cut -d" " -f2)
      es_conf=/etc/elasticsearch/elasticsearch.yml
      ipadd=$(ip a s scope global | awk '$1=="inet" {split($2, ipad, "/"); print ipad[1]}')
      node_name=$(awk -v ipadd=$ipadd '$1==ipadd && $2~/^es_[0-9]+$/ {print $2; exit}' /etc/hosts)
      # some cleaning in case of an update
      sed -e '/^path\.repo/d' \
          -e '/^node\.name/d' \
          -e '/^discovery\.zen\./d' \
          -e '/^discovery\.seed_/d' \
          -e '/^cluster\.initial_/d' \
          -i $es_conf
      sed 's/\(^discovery.type: single-node\)/#\1/' -i $es_conf
      echo "node.name: $node_name" >> $es_conf
      hosts_list=$(awk '$2~/^es_[0-9]+$/ {nodes[$2]} END{asorti(nodes);printf "["; for(n=1;n<=length(nodes);n++){if(n<length(nodes)){sep=", "}else{sep="]"}; printf "\"%s\"%s",nodes[n],sep}}' /etc/hosts)
      discovery_hosts_parameter_name="discovery.zen.ping.unicast.hosts"
      if [ $es_major_version -ge 7 ]; then
        discovery_hosts_parameter_name="discovery.seed_hosts"
        echo "cluster.initial_master_nodes: $hosts_list" >> $es_conf
      fi
      echo "$discovery_hosts_parameter_name: $hosts_list" >> $es_conf
      echo "discovery.zen.minimum_master_nodes: $(expr ${nodes.es.length} / 2 + 1)" >> $es_conf
      systemctl enable elasticsearch
      systemctl restart elasticsearch

  setupDatadogAgentEs:
    - setGlobalRepoRootUrl
    - cmd[${this}]: |-
        chmod 755 /var/log/elasticsearch/
        chmod 755 /var/log/elasticsearch/*.log -R
        sed -i "s/\(url: http:\/\/\).*\(:.*\)/\1${HOSTNAME}\2/" /etc/datadog-agent/conf.d/elastic.d/conf.yaml
        mkdir /etc/datadog-agent/conf.d/jelastic.d /var/log/jelastic-packages
        chown root:root /var/log/jelastic-packages
        chown dd-agent: /etc/datadog-agent/conf.d/jelastic.d
        wget -qO /etc/datadog-agent/conf.d/jelastic.d/conf.yaml ${globals.repoRootUrl}/assets/common/dd_agent_jelastic_package_conf.yml
        systemctl restart crond datadog-agent

  updateReplica:
    - cmd[${nodes.es.first.id}]: |-
        NODE_HOSTNAME=$(echo "${nodes.es.first.url}" | sed 's/^https:\/\///')
        curl -s http://${NODE_HOSTNAME}:9200/_cat/indices | awk -v repl=${globals.replica} '$6!=repl {print $3}' | while read index; do
            curl -s -XPUT http://${NODE_HOSTNAME}:9200/$index/_settings \
              -H "Content-Type: application/json" \
              -d '{"index":{"number_of_replicas": ${globals.replica} }}'
        done

  setReplica:
    - if(nodes.es.length > 1):
        - if(nodes.es.length > 4):
          - setGlobals:
              - replica: 2
        - else:
          - setGlobals:
              - replica: 1
    - else:
        setGlobals:
          - replica: 0
    - forEach(nodes.cp):
        cmd[${@i.id}]: |-
          setenv=$(find /opt/jcustomer/jcustomer/bin -name setenv)
          # test if not update needed
          actual=$(awk -F'=' '/UNOMI_ELASTICSEARCH_MONTHLYINDEX_REPLICAS/ {print $NF}' $setenv)
          if [ ! -z "$actual" ]; then
            if [ $actual -eq ${globals.replica} ]; then
              echo "$(hostname) already get the good replica parameters (${globals.replica})"
              exit 0
            fi
          fi
          # some cleaning in case of an update
          sed '/^export UNOMI_ELASTICSEARCH/d' -i $setenv
          echo "export UNOMI_ELASTICSEARCH_MONTHLYINDEX_REPLICAS=${globals.replica}" >> $setenv
          echo "export UNOMI_ELASTICSEARCH_DEFAULTINDEX_REPLICAS=${globals.replica}" >> $setenv
          systemctl is-active --quiet karaf && systemctl restart karaf || exit 0
        user: root

  setShardAllocation:
    # Parameters;
    #   - allocation: to be set to strings "primaries" or "null"
    #   - target
    - if("${this.allocation}" == "primaries"):
        - set:
            rule: "\"primaries\""
    - elif("${this.allocation}" == "null"):
        - set:
            rule: "null"
    - log: "Set cluster.routing.allocation.enable to ${this.rule}"
    - cmd[${this.target}]: |-
        # If ${this.rule} is "null" then it's possible that ES daemon is not yet
        # started so we wait a bit for it
        # We check if ES daemon is started with a curl instead of `systemctl is-active`
        # because there is a lag between the time the service is considered active
        # and the node is reachable via curl
        if [ "${this.rule}" == "null" ]; then
          maxi=128
          i=1
          until (curl -sf "${HOSTNAME}:9200/_cluster/health?timeout=1s" >/dev/null); do
            if [ $i -gt $maxi ]; then
              echo "Elasticsearch is still not started, aborting..."
              exit 1
            fi
            echo "Waiting for ES to be started $i/$maxi"
            ((i++))
            sleep 2
          done
        fi

        curl -s -H 'Content-Type: application/json' -XPUT \
          -d '{"persistent":{"cluster.routing.allocation.enable": ${this.rule} }}' \
          http://${HOSTNAME}:9200/_cluster/settings

  forceESFlush:
    # Parameters:
    #   - target
    - cmd[${this.target}]: |-
        [ ! -x /usr/bin/jq ] && yum install -y jq

        es_version=$(curl http://${HOSTNAME}:9200/ -s | jq -r .version.number)
        if printf '%s\n%s' '7.6.0' "$es_version" | sort -CV; then
          endpoint="_flush"         # since 7.6.0
        else
          endpoint="_flush/synced"  # deprecated since 7.6.0
        fi

        flush() {
          failed=$(curl -s -XPOST http://${HOSTNAME}:9200/${endpoint} | jq -r .[].failed | awk '{f=f+$1} END{print f}')
          if [ $failed -eq 0 ]; then
            return 0
          else
            return 1
          fi
        }

        # initiate flush and wait a few
        curl -s -XPOST http://${HOSTNAME}:9200/${endpoint}
        sleep 10

        # now check no flush failed
        maxi=66
        i=1
        until flush; do
          if [ $i -gt $maxi ]; then
            echo "ES node still have failure(s) on flush, aborting..."
            exit 1
          fi
          echo "flush iteration $i/$maxi"
          ((i++))
          sleep 2
        done

  redeployEsNodes:
    - if(nodes.es.length == 1):
        - log: "Stopping Karaf..."
        - manageSystemdService:
            target: cp
            unit: karaf
            action: stop

        - log: "Redeploying Elasticsearch node..."
        - api: environment.control.redeploycontainersbygroup
          nodeGroup: es
          tag: ${nodes.es.first.version}
          useExistingVolumes: true

        - log: "Starting Karaf..."
        - manageSystemdService:
            target: cp
            unit: karaf
            action: start
        - checkEsClusterStatus
    - else:
        - forEach(nodes.es):
            - log: "Redeploying Elasticsearch node ${@i.id}..."
            - api: environment.control.RedeployContainerById
              nodeId: ${@i.id}
              tag: ${nodes.es.first.version}
              useExistingVolumes: true
              skipReinstall: false
            - checkEsClusterStatus

  checkEsClusterStatus:
    # Every second, we check cluster health. If curl fails with a 10s timeout, then exit 1.
    # For 3-nodes clusters, until status is green & actual nodes number equals nodes.es.length,
    # we check that the number of active shards is increasing (meaning the node is not fully
    # initialized yet). If it is still not after 10s, exit 1.
    # For single node ES, we check green or yellow status
    - cmd[${nodes.es.first.id}]: |-
        [ -x /usr/bin/jq ] || yum install -y jq
        health_file="/tmp/es_cluster_health.json"
        i=0
        prev_active_shards=0
        total_nodes_count=${nodes.es.length}
        # Single ES node: we check green or yellow status
        healthcheck_condition='[[ "$status" == "green" ]] || [[ "$status" == "yellow" ]]'

        # 3-nodes cluster: we check green status only
        if [ $total_nodes_count -gt 1 ]; then
          healthcheck_condition='[[ "$status" == "green" ]] && [ $real_nodes_count -eq $total_nodes_count ]'
        fi

        while [ $i -lt 20 ]; do
          curl -Ssf 'es:9200/_cluster/health?timeout=10s' > $health_file || (rm -f $health_file; exit 1)
          status=$(cat $health_file | jq -r '.status')
          real_nodes_count=$(cat $health_file | jq '.number_of_nodes')
          if eval $healthcheck_condition; then
            exit 0
          fi
          active_shards=$(cat $health_file | jq ".active_shards")
          [ $active_shards -eq $prev_active_shards ] && ((i=i+1)) || i=0
          prev_active_shards=$active_shards
          sleep 1
        done
        echo "[ERROR] There is an issue with the cluster, please check" 1>&2
        exit 1
      user: root
