---
# Depends on:
#   - common.yml

actions:
  setupES:
    cmd[${this}]: |-
      /usr/share/elasticsearch/bin/elasticsearch --version
      es_major_version=$(/usr/share/elasticsearch/bin/elasticsearch --version | cut -d"." -f1 | cut -d" " -f2)
      es_conf=/etc/elasticsearch/elasticsearch.yml
      ipadd=$(ip a s scope global | awk '$1=="inet" {split($2, ipad, "/"); print ipad[1]}')
      node_name=$(awk -v ipadd=$ipadd '$1==ipadd && $2~/^es_[0-9]+$/ {print $2; exit}' /etc/hosts)
      # some cleaning in case of an update
      sed -e '/^path\.repo/d' \
          -e '/^node\.name/d' \
          -e '/^discovery\.zen\./d' \
          -e '/^discovery\.seed_/d' \
          -e '/^cluster\.initial_/d' \
          -i $es_conf
      sed 's/\(^discovery.type: single-node\)/#\1/' -i $es_conf
      echo "node.name: $node_name" >> $es_conf
      hosts_list=$(awk '$2~/^es_[0-9]+$/ {nodes[$2]} END{asorti(nodes);printf "["; for(n=1;n<=length(nodes);n++){if(n<length(nodes)){sep=", "}else{sep="]"}; printf "\"%s\"%s",nodes[n],sep}}' /etc/hosts)
      discovery_hosts_parameter_name="discovery.zen.ping.unicast.hosts"
      if [ $es_major_version -ge 7 ]; then
        discovery_hosts_parameter_name="discovery.seed_hosts"
        echo "cluster.initial_master_nodes: $hosts_list" >> $es_conf
      fi
      echo "$discovery_hosts_parameter_name: $hosts_list" >> $es_conf
      echo "discovery.zen.minimum_master_nodes: $(expr ${nodes.es.length} / 2 + 1)" >> $es_conf
      systemctl enable elasticsearch
      systemctl restart elasticsearch

  setupDatadogAgentEs:
    - setGlobalRepoRootUrl
    - cmd[${this}]: |-
        chmod 755 /var/log/elasticsearch/
        chmod 755 /var/log/elasticsearch/*.log -R
        sed -i "s/\(url: http:\/\/\).*\(:.*\)/\1${HOSTNAME}\2/" /etc/datadog-agent/conf.d/elastic.d/conf.yaml
        mkdir /etc/datadog-agent/conf.d/jelastic.d /var/log/jelastic-packages
        chown root:root /var/log/jelastic-packages
        chown dd-agent: /etc/datadog-agent/conf.d/jelastic.d
        wget -qO /etc/datadog-agent/conf.d/jelastic.d/conf.yaml ${globals.repoRootUrl}/assets/common/dd_agent_jelastic_package_conf.yml
        systemctl restart crond datadog-agent

  updateReplica:
    - cmd[${nodes.es.first.id}]: |-
        NODE_HOSTNAME=$(echo "${nodes.es.first.url}" | sed 's/^https:\/\///')
        curl -s http://${NODE_HOSTNAME}:9200/_cat/indices | awk -v repl=${this.replica} '$6!=repl {print $3}' | while read index; do
            curl -s -XPUT http://${NODE_HOSTNAME}:9200/$index/_settings \
              -H "Content-Type: application/json" \
              -d '{"index":{"number_of_replicas": ${this.replica} }}' 2>>${this.logsPath}
        done

  calculateNumberOfReplicas:
    - if(nodes.es.length > 1):
        - if(nodes.es.length > 4):
          - setGlobals:
              - replica: 2
        - else:
          - setGlobals:
              - replica: 1
    - else:
        setGlobals:
          - replica: 0

  setReplica:
    - calculateNumberOfReplicas
    - forEach(nodes.cp):
        cmd[${@i.id}]: |-
          setenv=$(find /opt/jcustomer/jcustomer/bin -name setenv)
          # test if not update needed
          actual=$(awk -F'=' '/UNOMI_ELASTICSEARCH_MONTHLYINDEX_REPLICAS/ {print $NF}' $setenv)
          if [ ! -z "$actual" ]; then
            if [ $actual -eq ${globals.replica} ]; then
              echo "$(hostname) already get the good replica parameters (${globals.replica})"
              exit 0
            fi
          fi
          # some cleaning in case of an update
          sed '/^export UNOMI_ELASTICSEARCH/d' -i $setenv
          echo "export UNOMI_ELASTICSEARCH_MONTHLYINDEX_REPLICAS=${globals.replica}" >> $setenv
          echo "export UNOMI_ELASTICSEARCH_DEFAULTINDEX_REPLICAS=${globals.replica}" >> $setenv
          systemctl is-active --quiet karaf && systemctl restart karaf || exit 0
        user: root

  setShardAllocation:
    # Parameters;
    #   - allocation: to be set to strings "primaries" or "null"
    #   - target
    - if("${this.allocation}" == "primaries"):
        - set:
            rule: "\"primaries\""
    - elif("${this.allocation}" == "null"):
        - set:
            rule: "null"
    - log: "Set cluster.routing.allocation.enable to ${this.rule}"
    - cmd[${this.target}]: |-
        # If ${this.rule} is "null" then it's possible that ES daemon is not yet
        # started so we wait a bit for it
        # We check if ES daemon is started with a curl instead of `systemctl is-active`
        # because there is a lag between the time the service is considered active
        # and the node is reachable via curl
        if [ "${this.rule}" == "null" ]; then
          maxi=128
          i=1
          until (curl -sf "${HOSTNAME}:9200/_cluster/health?timeout=1s" >/dev/null); do
            if [ $i -gt $maxi ]; then
              echo "Elasticsearch is still not started, aborting..."
              exit 1
            fi
            echo "Waiting for ES to be started $i/$maxi"
            ((i++))
            sleep 2
          done
        fi

        curl -s -H 'Content-Type: application/json' -XPUT \
          -d '{"persistent":{"cluster.routing.allocation.enable": ${this.rule} }}' \
          http://${HOSTNAME}:9200/_cluster/settings

  forceESFlush:
    # Parameters:
    #   - target
    - cmd[${this.target}]: |-
        [ ! -x /usr/bin/jq ] && yum install -y jq

        es_version=$(curl http://${HOSTNAME}:9200/ -s | jq -r .version.number)
        if printf '%s\n%s' '7.6.0' "$es_version" | sort -CV; then
          endpoint="_flush"         # since 7.6.0
        else
          endpoint="_flush/synced"  # deprecated since 7.6.0
        fi

        flush() {
          failed=$(curl -s -XPOST http://${HOSTNAME}:9200/${endpoint} | jq -r .[].failed | awk '{f=f+$1} END{print f}')
          if [ $failed -eq 0 ]; then
            return 0
          else
            return 1
          fi
        }

        # initiate flush and wait a few
        curl -s -XPOST http://${HOSTNAME}:9200/${endpoint}
        sleep 10

        # now check no flush failed
        maxi=66
        i=1
        until flush; do
          if [ $i -gt $maxi ]; then
            echo "ES node still have failure(s) on flush, aborting..."
            exit 1
          fi
          echo "flush iteration $i/$maxi"
          ((i++))
          sleep 2
        done

  redeployEsNodes:
    - if(nodes.es.length == 1):
        - log: "Stopping Karaf..."
        - manageSystemdService:
            target: cp
            unit: karaf
            action: stop

        - log: "Redeploying Elasticsearch node..."
        - api: environment.control.redeploycontainersbygroup
          nodeGroup: es
          tag: ${nodes.es.first.version}
          useExistingVolumes: true

        - log: "Starting Karaf..."
        - manageSystemdService:
            target: cp
            unit: karaf
            action: start
        - checkEsClusterStatus
    - else:
        - forEach(nodes.es):
            - log: "Redeploying Elasticsearch node ${@i.id}..."
            - api: environment.control.RedeployContainerById
              nodeId: ${@i.id}
              tag: ${nodes.es.first.version}
              useExistingVolumes: true
              skipReinstall: false
            - checkEsClusterStatus

  checkEsClusterStatus:
    # Every second, we check cluster health. If curl fails with a 10s timeout, then exit 1.
    # For 3-nodes clusters, until status is green & actual nodes number equals nodes.es.length,
    # we check that the number of active shards is increasing (meaning the node is not fully
    # initialized yet). If it is still not after 10s, exit 1.
    # For single node ES, we check green or yellow status
    - cmd[${nodes.es.first.id}]: |-
        [ -x /usr/bin/jq ] || yum install -y jq
        health_file="/tmp/es_cluster_health.json"
        i=0
        prev_active_shards=0
        total_nodes_count=${nodes.es.length}
        # Single ES node: we check green or yellow status
        healthcheck_condition='[[ "$status" == "green" ]] || [[ "$status" == "yellow" ]]'

        # 3-nodes cluster: we check green status only
        if [ $total_nodes_count -gt 1 ]; then
          healthcheck_condition='[[ "$status" == "green" ]] && [ $real_nodes_count -eq $total_nodes_count ]'
        fi

        while [ $i -lt 20 ]; do
          curl -Ssf 'es:9200/_cluster/health?timeout=10s' > $health_file || (rm -f $health_file; exit 1)
          status=$(cat $health_file | jq -r '.status')
          real_nodes_count=$(cat $health_file | jq '.number_of_nodes')
          if eval $healthcheck_condition; then
            exit 0
          fi
          active_shards=$(cat $health_file | jq ".active_shards")
          [ $active_shards -eq $prev_active_shards ] && ((i=i+1)) || i=0
          prev_active_shards=$active_shards
          sleep 1
        done
        echo "[ERROR] There is an issue with the cluster, please check" 1>&2
        exit 1
      user: root

  setAwsSnapshotRepository:
    - cmd[${nodes.es.first.id}]: |-
        curl -H 'Content-Type: application/json' -XPUT "${nodes.es.first.intIP}:9200/_snapshot/backup_repository?verify=false&pretty" -d"
          {
            \"type\": \"s3\",
            \"settings\": {
                \"bucket\": \"${this.account}\",
                \"region\": \"${this.region}\",
                ${this.awsAccessKeyJson}
                ${this.awsSecretKeyJson}
                \"base_path\" : \"${settings.backup_name}/elasticsearch\"
            }
          }" 2>>${this.logsPath} || exit 1

  setAzureSnapshotRepository:
    - cmd[${nodes.es.first.id}]: |-
        curl -H 'Content-Type: application/json' -XPUT "${nodes.es.first.intIP}:9200/_snapshot/backup_repository?verify=false&pretty" -d"
          {
            \"type\": \"azure\",
            \"settings\": {
                \"container\": \"${settings.backup_name}\",
                \"base_path\" : \"elasticsearch\"
            }
          }" 2>>${this.logsPath} || exit 1

  # Using an Elasticsearch keystore to add AWS keys
  setAwsElasticsearchConfig:
    - if ('${this.esVersion}' >= '6.4.0'):
        - cmd[es]: |-
            if ! /usr/share/elasticsearch/bin/elasticsearch-keystore list; then
                /usr/share/elasticsearch/bin/elasticsearch-keystore create
            fi
            printf "${settings.aws_access_key}" | /usr/share/elasticsearch/bin/elasticsearch-keystore add -f s3.client.default.access_key
            printf "${settings.aws_secret_key}" | /usr/share/elasticsearch/bin/elasticsearch-keystore add -f s3.client.default.secret_key
            curl -X POST "${nodes.es.first.intIP}:9200/_nodes/reload_secure_settings" 2>>${this.logsPath} || exit 1
        - setGlobals:
            awsAccessKeyJson: ""
            awsSecretKeyJson: ""
    - else:
        - setGlobals:
            awsAccessKeyJson: "\\\"access_key\\\": \\\"${settings.aws_access_key}\\\","
            awsSecretKeyJson: "\\\"secret_key\\\": \\\"${settings.aws_secret_key}\\\","

  setAzureElasticsearchConfig:
    - if ('${this.esVersion}' >= '6.4.0'):
        - getAzureSecret:
            operation: "${this.operation}"
            account: ${this.account}
            logsPath: ${this.logsPath}

        - cmd[es]: |-
            if ! /usr/share/elasticsearch/bin/elasticsearch-keystore list; then
                /usr/share/elasticsearch/bin/elasticsearch-keystore create
                echo "true"
            fi
            printf "${this.account}" | /usr/share/elasticsearch/bin/elasticsearch-keystore add -f azure.client.default.account
            printf "${globals.azure_secret}" | /usr/share/elasticsearch/bin/elasticsearch-keystore add -f azure.client.default.key
            curl -X POST "${nodes.es.first.intIP}:9200/_nodes/reload_secure_settings" 2>>${this.logsPath} || exit 1
    - else:
        - cmd[${nodes.es.first.id}]: |-
            grep azure_storage_user /etc/elasticsearch/elasticsearch.yml || true
        - if ("${response.out}" == ""):  # If azure config is not set
            forEach(nodes.es):
              cmd[${@i.id}]: |-
                echo -e "cloud:\n  azure:\n    storage:\n      azure_storage_user:" >> /etc/elasticsearch/elasticsearch.yml
                echo -e "        default: true\n        account: placeholder\n        key: placeholder" >> /etc/elasticsearch/elasticsearch.yml
        - cmd[${nodes.es.first.id}]: |-
            grep "${this.account}" /etc/elasticsearch/elasticsearch.yml || true
        - if ("${response.out}" == ""):  # If we need to use a different account
            # Done in a function because of the if... it's the way it works !
            - getAzureSecret:
                operation: "${this.operation}"
                account: ${this.account}
                logsPath: ${this.logsPath}
            # Set azure credentials and restart ES
            - forEach(nodes.es):
                cmd[${@i.id}]: |-
                  sed -i 's#\(^.*account\:\).*#\1 ${this.account}#g' /etc/elasticsearch/elasticsearch.yml
                  sed -i 's#\(^.*key\:\).*#\1 ${globals.azure_secret}#g' /etc/elasticsearch/elasticsearch.yml
                  service elasticsearch restart
                  sleep 30

  getAzureSecret:
    - cmd[${nodes.es.first.id}]: |-
        export AWS_ACCESS_KEY_ID="${settings.aws_access_key}" AWS_SECRET_ACCESS_KEY="${settings.aws_secret_key}"
        cd jelastic_backup
        python3 elasticsearch.py --bucketname ${this.account} --backupname ${settings.backup_name} --cloudprovider=azure --operation=${this.operation} 2>>${this.logsPath} || exit 1
    - cmd[${nodes.es.first.id}]: |-
        cat /tmp/azurecred
        rm -f /tmp/azurecred
    - setGlobals:
        azure_secret: "${response.out}"
    - if ("${globals.azure_secret}" == ""):
        - return:
            type: error
            message: "An error occurred during backup repository configuration."
