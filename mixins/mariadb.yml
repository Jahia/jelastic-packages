---
# Depends on:
#   - common.yml
#   - jahia.yml

actions:
  ####################
  # database related #
  ####################
  updateMysqlUser:
    - cmd [${this}]: sed 's;\(^mysql.*\)/home/jelastic/.*;\1/var/lib/mysql/:/sbin/nologin;' /etc/passwd

  disablePhpMyAdmin:
  # Disable apache2 and php-fpm daemons since we don't use phpMyAdmin
    cmd [${this}]: |-
      curl --retry 6 -fLSso /etc/systemd/system/mariadb.service.d/override.conf ${globals.repoRootUrl}/assets/database/override.conf || exit 1
      rm -rf /etc/systemd/system/httpd.service.d /usr/lib/systemd/system/httpd.service.d
      systemctl daemon-reload
      systemctl disable --now apache2 php-fpm httpd 2>/dev/null
      chkconfig --del httpd
      chkconfig --del simplehttpd
      d=/etc/init.d
      rm -f $d/httpd $d/simplehttpd

  installDatabase:
    # Parameters:
      #   - user: DB username
      #   - __secret__password: DB user's password
    - setJelasticUserAsRoot: sqldb
    - updateMysqlUser: sqldb
    - log: "## Setup MariaDB"
    - setGlobalRepoRootUrl
    - cmd[sqldb]: |-
        systemctl stop mariadb
        sudo -u mysql mysqld_safe --skip-grant-tables --skip-networking &
        sleep 5  # Make sure that the daemon is running
        # allow root user to authenticate
        for u in root mysql; do
          mysql -e "flush privileges; alter user ${u}@localhost identified via unix_socket; flush privileges;"
        done
        pkill 'mysqld_safe|mariadbd'
        # Make sure that the daemon is stopped
        sleep 5
    - disablePhpMyAdmin: sqldb
    - cmd[sqldb]: |-
        # Securing possible blocking issues with jelastic/mariadb:10.4.13 with next 3 lines
        [ -f /etc/mysql/conf.d/master.cnf ] && rm -f /etc/mysql/conf.d/master.cnf
        [ -f /etc/mysql/conf.d/slave.cnf ] && rm -f /etc/mysql/conf.d/slave.cnf
        curl --retry 6 -fLSso /etc/mysql/conf.d/mysql.cnf ${globals.repoRootUrl}/assets/database/mysql.cnf || exit 1
        curl --retry 6 -fLSso /etc/logrotate.d/mysql ${globals.repoRootUrl}/assets/database/logrotate_mysql || exit 1
        chmod 644 /etc/logrotate.d/mysql
        curl --retry 6 --create-dirs -fLSso /var/lib/jelastic/customizations/jahia_override.lib ${globals.repoRootUrl}/assets/database/jahia_override.lib || exit 1
        chown -R mysql:mysql /var/lib/jelastic/customizations
        logrotate -f /etc/logrotate.d/mysql
        systemctl start mariadb
        mysql -e "set global key_buffer_size = 128*1024*1024; set global myisam_sort_buffer_size = 128*1024*1024; set global wsrep_sst_auth = 'mysql:'; set global wsrep_sst_method = 'mariabackup';"
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred during mariadb setup."
    - if (nodes.sqldb.length > 1):
        installGaleraCluster
    - log: "## Create DX's Jahia database and user"
    - cmd[${nodes.sqldb.master.id}]: |-
        __secret__password="${this.__secret__password}"
        mysql -e "CREATE DATABASE IF NOT EXISTS jahia CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;"
        mysql -e "grant all privileges on jahia.* to '${this.user}'@'%' identified by '$__secret__password';"
        mysql -e "flush privileges;"
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred when creating jahia database."

  onAfterRedeploySqldbContainer:
    - setJelasticUserAsRoot: ${this}
    - updateMysqlUser: ${this}
    - installRequiredPackages:
        target: ${this}
        packages: "jq rclone python3-pip python3-boto3 qpress socat MariaDB-backup"
    - disablePhpMyAdmin: ${this}
    - cmd [${this}]: |-
        curl --retry 6 -fLSso /etc/logrotate.d/mysql ${globals.repoRootUrl}/assets/database/logrotate_mysql || exit 1
        chmod 644 /etc/logrotate.d/mysql
        chown -R mysql:mysql /var/lib/jelastic/customizations
        logrotate -f /etc/logrotate.d/mysql
        systemctl enable --now mariadb
        if [[ ${nodes.sqldb.length} > 1 ]]; then
          curl --retry 6 -fLSso /etc/systemd/system/mariadb.service.d/bootstrap.conf ${globals.repoRootUrl}/assets/database/bootstrap.conf &>> /var/log/run.log || exit 1
          systemctl daemon-reload
        fi
    - setupDatadogAgentSql: ${this}
    - mysqlService:
        target: ${this}
        action: enable
    - cmd [${this}]: dnf config-manager --disable epel

  createDataDogUser:
    - log: "## Create Datadog database user"
    - cmd[${nodes.sqldb.master.id}]: |-
        exists=$(mysql -sNe "select count(*) from  mysql.user where User='datadog';")
        if [ $exists -lt 2 ]; then
          mysql -e "CREATE OR REPLACE USER 'datadog'@'localhost' IDENTIFIED BY '${DB_USER_DATADOG}';"
          mysql -e "CREATE OR REPLACE USER 'datadog'@'%' IDENTIFIED BY '${DB_USER_DATADOG}';"
          mysql -e "GRANT REPLICATION CLIENT ON *.* TO 'datadog'@'localhost' WITH MAX_USER_CONNECTIONS 5;"
          mysql -e "GRANT PROCESS ON *.* TO 'datadog'@'localhost';"
          mysql -e "GRANT SELECT ON performance_schema.* TO 'datadog'@'localhost';"
          mysql -e "GRANT SELECT ON jahia.JR_J_LOCAL_REVISIONS TO 'datadog'@'%';"
        fi
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred when creating datadog database user."

  setupDatadogAgentSql:
    - createDataDogUser
    - log: "## Finalize Datadog agent setup on ${this}"
    - setGlobalRepoRootUrl
    - set:
        cluster: "false"
    - if (nodes.sqldb.length > 1):
        set:
          cluster: "true"
    - installLatestDatadogAgent: ${this}
    - cmd [${this}]: |-
        NODE_NAME=${HOSTNAME/-*}
        echo "hostname: ${_ROLE}.${NODE_NAME#node}" >> /etc/datadog-agent/datadog.yaml
        sed -i 's/# logs_enabled: false/logs_enabled: true/' /etc/datadog-agent/datadog.yaml
        echo "tags:" >> /etc/datadog-agent/datadog.yaml
        echo " - product:jahia" >> /etc/datadog-agent/datadog.yaml
        echo " - version:${DX_VERSION}" >> /etc/datadog-agent/datadog.yaml
        echo " - envname:${env.envName}" >> /etc/datadog-agent/datadog.yaml
        echo " - provide:${_PROVIDE}" >> /etc/datadog-agent/datadog.yaml
        echo " - role:${_ROLE}" >> /etc/datadog-agent/datadog.yaml
        echo "---" > /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "logs:" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "  - type: file" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    path: /var/log/mysql/mysqld.log" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    source: mysql" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    service: mysql" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    log_processing_rules:" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "      - type: multi_line " >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "        name: new_log_start_with_date" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "        pattern: \d{4}\-(0?[1-9]|1[012])\-(0?[1-9]|[12][0-9]|3[01])" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "  - type: file" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    path: /var/log/mysql/slow-queries.log" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    source: mysql" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    service: mysql" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "init_config:" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "instances:" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "  - server: 127.0.0.1" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    username: datadog" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    password: ${DB_USER_DATADOG}" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    sock: /var/lib/mysql/mysql.sock" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    tags:" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "      - 'env:${env.envName}'" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "      - 'role:database'" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "    options:" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "       replication: false" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "       galera_cluster: ${this.cluster}" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "       extra_status_metrics: true" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "       extra_innodb_metrics: true" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "       extra_performance_metrics: true" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "       schema_size_metrics: true" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        echo "       disable_innodb_metrics: false" >> /etc/datadog-agent/conf.d/mysql.d/conf.yaml
        curl --retry 6 -fLSso /etc/datadog-agent/checks.d/check_galera_wsrep_ready_status.py ${globals.repoRootUrl}/assets/database/check_galera_wsrep_ready_status.py || exit 1
        chown dd-agent: /etc/datadog-agent/checks.d/check_galera_wsrep_ready_status.py
        ln -s /etc/datadog-agent/conf.d/mysql.d/conf.yaml /etc/datadog-agent/conf.d/check_galera_wsrep_ready_status.yaml
        chown -h dd-agent: /etc/datadog-agent/conf.d/check_galera_wsrep_ready_status.yaml
        mkdir /etc/datadog-agent/conf.d/jelastic.d /var/log/jelastic-packages
        chown mysql:root /var/log/jelastic-packages
        chown dd-agent: /etc/datadog-agent/conf.d/jelastic.d
        curl --retry 6 -fLSso /etc/datadog-agent/conf.d/jelastic.d/conf.yaml ${globals.repoRootUrl}/assets/common/dd_agent_jelastic_package_conf.yml || exit 1
        curl --retry 6 -fLSso /usr/local/bin/set_dd_tags.sh ${globals.repoRootUrl}/assets/common/set_dd_tags.sh || exit 1
        curl --retry 6 -fLSso /etc/cron.d/set_dd_tags_cron ${globals.repoRootUrl}/assets/common/set_dd_tags_cron || exit 1
        chmod u+x /usr/local/bin/set_dd_tags.sh
        chmod 644 /var/log/mysql/mysqld.log
        chmod 644 /var/log/mysql/slow-queries.log  > /dev/null 2>&1
        systemctl restart crond datadog-agent
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred when installing datadog agent on a mariadb node."

  checkMariadbHealth:
    # Parameters:
    #   - target
    #   - maxDuration
    - cmd [${this.target}]: |-
        timeout ${this.maxDuration:1}s bash -c '
          until mysqladmin -s processlist 1>/dev/null || [[ $i == 0 ]]; do
            sleep 1
          done
        '
        rc=$?
        [[ $rc == 0 ]] && exit 0
        if ! systemctl status mysql > /dev/null; then
          echo "[ERROR] Mariadb service is not running" 1>&2
        else
          echo "[ERROR] Can't connect to mysql instance" 1>&2
        fi
        exit 1

  getGaleraNodeNameIndex:
    - cmd [${this}]: |-
        my_ip=$(grep $(hostname) /etc/hosts | awk '{print $1}')
        awk -v my_ip="$my_ip galera" '$0 ~my_ip {print $2}' /etc/hosts
    - setGlobals:
        galeraNodeNameIndex: "${response.out}"

  mysqlService:
    # disable mysql autostart because JEM will do it
    - if ("${this.action}" == "enable"):
        - cmd [${this.target}]: |-
            sed -i '/^\/var\/lib\/jelastic\/overrides\/envinfo.lib/d' /etc/jelastic/redeploy.conf
            rm /var/lib/jelastic/overrides/envinfo.lib || exit 0
    - if ("${this.action}" == "disable"):
        - cmd [${this.target}]: |-
            echo "SERVICE=''" > /var/lib/jelastic/overrides/envinfo.lib
            echo "/var/lib/jelastic/overrides/envinfo.lib" >> /etc/jelastic/redeploy.conf

  ##################
  # galera related #
  ##################
  installGaleraCluster:
    - log: "## Setup Galera cluster"
    - setGlobalRepoRootUrl
    - resetCluster
    - initialSetup
    - setupSeeds
    - forEach(i:nodes.sqldb):
        setupInstance:
          id: "${@i.id}"
    - cmd[${nodes.sqldb.master.id}]: "systemctl stop mariadb; sleep 5; galera_new_cluster;"
    - forEach(n:nodes.sqldb):
        if (!${@n.ismaster}):
          cmd[${@n.id}]: "systemctl start mariadb"
    - cmd[sqldb]: |-
        curl --retry 6 -fLSso /etc/systemd/system/mariadb.service.d/bootstrap.conf ${globals.repoRootUrl}/assets/database/bootstrap.conf &>> /var/log/run.log || exit 1
        systemctl daemon-reload

  setupInstance:
    - cmd[${this.id}]: |-
        sed -i "s/server_id.*/server_id = $(echo '${env.region}' | md5sum | grep -Eo "[[:digit:]]{3}" | head -n1)/" /etc/mysql/conf.d/galera.cnf
        sed -i "s/bind-address.*/bind-address = node${this.id}/" /etc/mysql/conf.d/galera.cnf
        sed -i "s/report_host.*/report_host = node${this.id}/" /etc/mysql/conf.d/galera.cnf
        sed -i "s/wsrep_cluster_name.*/wsrep_cluster_name = galera-cluster/" /etc/mysql/conf.d/galera.cnf
        sed -i "s/wsrep_node_name.*/wsrep_node_name = node${this.id}/" /etc/mysql/conf.d/galera.cnf
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred during galera cluster setup."

  initialSetup:
    - setNodeDisplayName[sqldb]: Galera
    - setGlobalRepoRootUrl
    - cmd[sqldb]: |-
        curl --retry 6 -fLSso /etc/mysql/conf.d/galera.cnf ${globals.repoRootUrl}/assets/database/galera.cnf &>> /var/log/run.log || exit 1
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred during galera cluster setup."

  setupSeeds:
    script: |
      var resp = jelastic.env.control.GetEnvInfo('${env.envName}', session);
      if (resp.result != 0) return resp;
      var intIP = [];
      for (var i = 0, n = resp.nodes; i < n.length; i++)
        n[i].nodeGroup == "sqldb" ? intIP.push(n[i].intIP) : 0
      resp = {result:0, onAfterReturn: {}};
      resp.onAfterReturn['cmd[sqldb]'] = 'sed -i "s|wsrep_cluster_address.*|wsrep_cluster_address = gcomm://'+ intIP.join(',') +'|g " /etc/mysql/conf.d/galera.cnf';
      resp.onAfterReturn['user'] = 'root';
      return resp;

  resetCluster:
    - cmd[sqldb]: |-
        [ -f /etc/mysql/conf.d/galera.cnf ] && rm -f /etc/mysql/conf.d/galera.cnf
        [ -f /var/lib/mysql/grastate.dat ] && rm -f /var/lib/mysql/grastate.dat
        [ -f /var/lib/mysql/gvwstate.dat ] && rm -f /var/lib/mysql/gvwstate.dat
        [ -f /var/lib/mysql/galera.cache ] && rm -f /var/lib/mysql/galera.cache
        (systemctl stop mariadb || pkill -9 mysql) 2> /dev/null
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred during galera cluster init."

  checkGaleraClusterHealth:
    - cmd [${this}]: |-
        if [ -f /var/lib/mysql/grastate.dat ]; then
          if [ "$(mysql -Ns -e "show global status like 'wsrep_local_state_comment'" | awk '{print $NF}')" != "Synced"  ]; then
            echo "[ERROR] Galera cluster not synced, exiting" 1>&2 && exit 1
          fi
        fi

  checkMariadbDatadogCustomChecks:
    - checkDatadogAgentCheck:
        target: ${this}
        checkName: mysql

  stopGaleraNode:
    - log: "Stopping MariaDB on node ${@i.id}..."
    - cmd[${this}]: |-
        systemctl stop mariadb || exit 1
        if [ -f /var/lib/mysql/grastate.dat ]; then
          cat /var/lib/mysql/grastate.dat
        else
          echo "$HOSTNAME is not in a cluster"
        fi

  startGaleraNode:
    - cmd [${this}]: |-
        if [ -f /var/lib/mysql/grastate.dat ]; then
          # for Galera clusters, the startup of MariaDB can take a long time if the node is not safe to bootstrap and there is a lot
          # of data to be transferred with SSTs, so the `systemctl start` command might fail with error messages but MariaDB is actually
          # still starting. Hence the redirect of stderr to `/dev/null` to avoid printing false errors.
          if ! (echo 0391d67edf41ad87eee6b4bd6c445495 /etc/init.d/mysql | md5sum -c --status); then  # check if release is < v3.36.0
            service mysql start 2> /dev/null
          else
            systemctl start mariadb 2>/dev/null
          fi
          i=1
          it=66
          until [ "$(mysql -Ns -e "show global status like 'wsrep_local_state_comment'" 2>/dev/null | awk '{print $NF}')" == "Synced"  ]; do
            if [ $i -ge $it ]; then
              echo "Too long to start, something is wrong here... EXITING"
              exit 1
            fi
            # As long as there is a mariabackup command running, we don't increment the timeout count
            if pgrep wsrep_sst_maria; then
              echo "$(date) SSTs sync still in progress..."
              sleep 5
            else
              echo "$(date) not ready yet (iteration $i/$it)"
              ((i++))
              sleep 1
            fi
          done
          echo "Node $HOSTNAME is now Synced !"
        else
          if ! (echo 0391d67edf41ad87eee6b4bd6c445495 /etc/init.d/mysql | md5sum -c --status); then  # check if release is < v3.36.0
            service mysql start || exit 1
          else
            systemctl start mariadb || exit 1
          fi
        fi
        # In case a backup from a previous minor MariaDB has just been restored, we run mariadb-upgrade
        mariadb-upgrade --check-if-upgrade-is-needed && mariadb-upgrade || exit 0

  rollingRestartGaleraNodes:
    - if (nodes.sqldb.length > 1):
      - muteMonitorsByTags:
          # restarts should take few seconds per nodes.
          # proxysqlSwitchMaster timeout is set to 1200.
          # So it should not exceed 1300 unless there is an issue
          duration: 1300
          tag: database
      - getGaleraMaster
      - forEach(nodes.sqldb): # Only restart no master nodes. It allows to change master node only once
          - getGaleraNodeNameIndex: ${@i.id}
          - if ("${globals.galeraNodeNameIndex}" != "${globals.galeraMasterIndex}"):
            - log: "Restarting MariaDB on node ${@i.id}..."
            - setDonorNode:
                target : ${@i.id}
            - api: environment.control.RestartNodeById
              nodeId: ${@i.id}
            - checkGaleraNodeStartup: ${@i.id}
            - sleep:
              - milliseconds: 15000
          - else:
            - setGlobals:
                origMasterId: ${@i.id}

      - script: |-
          masterIndex = "${globals.galeraMasterIndex}".slice(-1);
          newMasterIndex = ((masterIndex + 1) % 3) + 1
          return {'result':0, 'newMasterIndex': newMasterIndex}

      - proxysqlSwitchMaster:
          target: ${response.newMasterIndex}

      - setDonorNode:
          target: ${globals.origMasterId}

      - api: environment.control.RestartNodeById
        nodeId: ${globals.origMasterId}
      - checkGaleraNodeStartup: ${globals.origMasterId}

      - unmuteMonitorsByTags:
          tag: database
    - else:
        - api: environment.control.RestartNodeById
          nodeId: sqldb


  setDonorNode:
    # Configure the wsrep donor node as a non master node.
    # !!!!
    # It does not restart mysql, just update the configuration file
    # Don't use this action on environments having a single db node (obviously...)
    # !!!!
    # Parameters
    # - target: The node id to update
    - getGaleraMaster
    - forEach(nodes.sqldb):
        - if ("${@i.id}" != "${this.target}"):
          - getGaleraNodeNameIndex: ${@i.id}
          - if ("${globals.galeraNodeNameIndex}" != "${globals.galeraMasterIndex}"):
              - cmd[${@i.id}]: cat /etc/mysql/conf.d/galera.cnf  |awk '$1 == "wsrep_node_name" {print $3}'
              - set:
                  donorNode: ${response.out}
    - cmd[${this.target}]: |-
        sed -i "/wsrep_sst_donor/d" /etc/mysql/conf.d/galera.cnf
        echo "wsrep_sst_donor = ${this.donorNode}," >> /etc/mysql/conf.d/galera.cnf

  checkGaleraClusterSize:
    # Ensure the cluster size match the node count
    - cmd[${nodes.sqldb.first.id}]: |-
        mysql -Nse "show global status like 'wsrep_cluster_size';" | awk '{print $NF}'
    - if ("${response.out}" == "${nodes.sqldb.length}"):
        setGlobals:
          galeraClusterSizeOk: True
    - else:
        setGlobals:
          galeraClusterSizeOk: False

  redeployGaleraClusterNodes:
    - if (nodes.sqldb.length > 1):
        - checkGaleraClusterHealth: sqldb
        - checkGaleraClusterSize
        - if (!${globals.galeraClusterSizeOk}):
            return:
              type: error
              message: "Galera cluster size is wrong. Aborting"
        - getGaleraMaster
        - forEach(nodes.sqldb): # Only redeploy no master nodes. It allows to change master node only once
            - log: "Redeploying MariaDB on node ${@i.id}..."
            - getGaleraNodeNameIndex: ${@i.id}
            - if ("${globals.galeraNodeNameIndex}" != "${globals.galeraMasterIndex}"):
                - setDonorNode:
                    target: ${@i.id}
                - redeployGaleraNode: ${@i.id}
                - sleep:
                    - milliseconds: 15000
            - else:
                - setGlobals:
                    origMasterId: ${@i.id}
        - script: |-
            masterIndex = "${globals.galeraMasterIndex}".slice(-1);
            newMasterIndex = ((masterIndex + 1) % 3) + 1
            return {'result':0, 'newMasterIndex': newMasterIndex}
        - proxysqlSwitchMaster:
            target: ${response.newMasterIndex}
        - setDonorNode:
            target: ${globals.origMasterId}
        - redeployGaleraNode: ${globals.origMasterId}
    - else:
        - getGaleraNodeNameIndex: ${nodes.sqldb.first.id}
        - redeployGaleraNode: ${nodes.sqldb.first.id}



  redeployGaleraNode:
    # Parameters: target node id
    - proxysqlSetMariadbBackendStatus:
        targetHost: ${globals.galeraNodeNameIndex}
        newStatus: OFFLINE_SOFT
    - stopGaleraNode: ${this}
    - api: environment.control.RedeployContainerById
      nodeId: ${this}
      tag: ${globals.targetDockerTag}
      useExistingVolumes: true
      skipReinstall: false
      envName: ${env.envName}
    # We are going to a new if dimension, let's save some information
    - setGlobals:
        currentNode: ${this}
    - checkMariadbHealth:
        target: ${this}
    - if (nodes.sqldb.length > 1):
        - checkGaleraClusterHealth: sqldb
        - checkGaleraClusterSize
        - if (!${globals.galeraClusterSizeOk}):
            return:
              type: error
              message: "Galera node ${globals.currentNode} was not able to join the cluster after being redeployed"
    - proxysqlSetMariadbBackendStatus:
        targetHost: ${globals.galeraNodeNameIndex}
        newStatus: ONLINE

  checkGaleraNodeStartup:
    - cmd[${this}]: |-
        i=1
        it=66
        until [ "$(mysql -Ns -e "show global status like 'wsrep_local_state_comment'" 2>/dev/null | awk '{print $NF}')" == "Synced"  ]; do
          if [ $i -ge $it ]; then
            echo "Too long to start, something is wrong here... EXITING"
            exit 1
          fi
          # As long as there is a mariabackup command running, we don't increment the timeout count
          if pgrep wsrep_sst_maria; then
            echo "$(date) SSTs sync still in progress..."
            sleep 5
          else
            echo "$(date) not ready yet (iteration $i/$it)"
            ((i++))
            sleep 1
          fi
        done
        echo "Node $HOSTNAME is now Synced !"
