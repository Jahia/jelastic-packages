---
# Depends on:
#   - common.yml

globals:
  proxysql_cli: "mysql -h 127.0.0.1 -uadmin -p$PROXYSQL_ADMIN_PASSWORD -P6032"
  org_jahia_ehcachemanager_maxBytesLocalHeap_dev: 700M
  org_jahia_ehcachemanager_big_maxBytesLocalHeap_dev: 700M
  org_jahia_ehcachemanager_maxBytesLocalHeap_prod: 800M
  org_jahia_ehcachemanager_big_maxBytesLocalHeap_prod_cp: 2500M
  org_jahia_ehcachemanager_big_maxBytesLocalHeap_prod_proc: 700M
  expandImportedFilesOnDisk: "true"
  jahiaFileUploadMaxSize: 268435456
  imageService: ImageMagickImageService
  jahiaImageMagickPath: /usr/bin
  java_opts:
    -DDB_USER=${DB_USER}
    -DDB_PASSWORD=${DB_PASSWORD}
    -DREDIS_PASSWORD=${REDIS_PASSWORD}
    -DMANAGER_USER=${MANAGER_USER}
    -DHOST_NAME=$(hostname)
    -Dcom.sun.management.jmxremote
    -Dcom.sun.management.jmxremote.port=7199
    -Dcom.sun.management.jmxremote.ssl=false
    -Dcom.sun.management.jmxremote.authenticate=false
    -XX:MaxPermSize=512m
    -XX:+HeapDumpOnOutOfMemoryError
    -XX:HeapDumpPath=${DUMPS_PATH}/heap_dumps
    -XX:+PrintConcurrentLocks
    -XX:+UseParallelGC
    -XX:SurvivorRatio=8
    -Xmn1G

actions:
  #################
  # jahia related #
  #################
  onAfterBrowsingScaleOut:
    - copyApp: ${this.newNode}
    - setupDatadogAgentJahia: ${this.newNode}
    - cmd[${this.newNode}]: |-
        if (service tomcat status); then
          echo "Now Restarting Tomcat"
          service tomcat restart
        else
          echo "Now Launching Tomcat"
          service tomcat start
        fi

  onAfterRedeployJahiaContainer:
    - cmd[${this}]:
        - service tomcat stop
    - getMavenPath
    - env.control.AddContainerEnvVars [cp, proc]:
      vars:
        jahia_cfg_mvnPath: ${globals.jahia_cfg_mvnPath}
    - getLogEventScript: ${this}
    - if (nodes.sqldb.length == 1):
        - disableDatadogCustomChecks
    - copyApp: ${this}
    - if ("${this}" == "cp"):
        cmd[${this}]:
          - sed -i "s#\(processingServer\s*=\).*#\1 false#g" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
    - setProxysqlCredsInDatadogConfig:
        target: "proc, cp"
    - setupDatadogAgentJahia: ${this}
    - cmd[${this}]: |-
        sudo -u tomcat touch "/data/digital-factory-data/[persisted-configurations].dorestore"
        source /etc/locale.conf
        echo "JAHIA_UPGRADE=$JAHIA_UPGRADE"
        if [ "$JAHIA_UPGRADE" == "true" ]; then
          echo "This is an upgrade, processing's tomcat will not be restarted now"
        else
          echo "This is a regular redeploy, restart tomcat now"
          sudo service tomcat start
        fi

    - script: |-
        ipsec_enabled = jelastic.env.control.GetNodeGroups("${env.envName}", session).object.filter(function (object) {
                          return object.name == "cp";
                        }).pop().ipsec
        return {"result": 0, "out": ipsec_enabled}

    - setGlobals:
        strongswanServiceStatus: ${response.out}

    - if ("${globals.strongswanServiceStatus}" == "enable"):
        - cmd[${this}]: |-
            systemctl enable strongswan.service
            systemctl start strongswan.service

  stopJahia:
    cmd[${this}]: "service tomcat stop"

  initJahiaDatabase:
    - log: "## Import DX schema in database"
    - cmd[${nodes.proc.first.id}]: cat $DATA_PATH/digital-factory-data/db/sql/schema/mysql/*.sql | mysql -h $DB_ENDPOINT -u$DB_USER -p$DB_PASSWORD -f jahia

  getMavenPath:
    - cmd [${nodes.proc.first.id}]: echo $(ls -d /opt/*maven*/bin/mvn)
    - setGlobals:
        jahia_cfg_mvnPath: ${response.out}

  installJahia:
    # Parameters:
    #   - jahiaVersion
    #   - __secret__rootpwd: Jahia root user password
    - getLogEventScript: proc, cp

    - environment.nodegroup.ApplyData [proc, cp]:
        data:
          productName: dx
          productVersion: ${this.jahiaVersion}
    - initJahiaDatabase
    - log: "## Determine JDK version for good _JAVA_OPTIONS envvar"
    - cmd[proc, cp]: |-
        case "$($JAVA_HOME/bin/java -version 2>&1 | awk 'NR==1 {print gensub("(\"|_.*)", "", "g", $3)}')" in
          1.8*)
              j_opts='${globals.java_opts}'
              ;;
          *)
              j_opts='${globals.java_opts} -Xlog:gc:file=/opt/tomcat/logs/gc.log:time,uptime,level,pid,tid,tags'
              ;;
        esac
        sed -e '2isource /etc/profile' -e "s#\(^JAVA_OPTS=.*\)\(\"$\)#\1 ${j_opts}\2#" -i /opt/tomcat/conf/tomcat-env.sh
    - setJahiaPropertiesEnvvars
    - copyApp: proc, cp
    - setJahiaUserFeedbacksConfig
    - cmd[proc]: |-
        __secret__rootpwd="${this.__secret__rootpwd.toBase64()}"
        base64 -d <<< $__secret__rootpwd > $DATA_PATH/digital-factory-data/root.pwd
        chown tomcat:tomcat $DATA_PATH/digital-factory-data/root.pwd
    - cmd [proc]: |-
        file=/opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-purge-jobs.xml
        mv ${file}-disabled $file
    - api: env.control.ExecDockerRunCmd
      nodeId: ${nodes.proc.first.id}
    - checkPatGroovyScriptExecution
    - startupJahiaHealthCheck: proc
    - env.control.ExecDockerRunCmd [${nodes.cp.join(id,)}]

  setJahiaPropertiesEnvvars:
    - log: "## Setting jahia.properties envvars"
    - getMavenPath
    - env.control.AddContainerEnvVars [cp, proc]:
      vars:
        jahia_cfg_expandImportedFilesOnDisk: ${globals.expandImportedFilesOnDisk}
        jahia_cfg_jahiaFileUploadMaxSize: ${globals.jahiaFileUploadMaxSize}
        jahia_cfg_imageService: ${globals.imageService}
        jahia_cfg_imageMagickPath: ${globals.jahiaImageMagickPath}
        jahia_cfg_mvnPath: ${globals.jahia_cfg_mvnPath}

  copyApp:
    - log: "## Copying Jahia app and setting its properties"
    - cmd[${this}]: |-
        [ "$_ROLE" == "Browsing" ] && sed -i "s#\(processingServer\s*=\).*#\1 false#g" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
        if [ -d $STACK_PATH/webapps-tmp ]; then
          rm -rf $STACK_PATH/webapps
          mv $STACK_PATH/webapps-tmp $STACK_PATH/webapps
        fi
        chown -R tomcat:tomcat $STACK_PATH/webapps

        tomcat_env=/opt/tomcat/conf/tomcat-env.sh
        short_name=$(echo ${_ROLE}.$HOSTNAME | sed -r 's/^([a-Z]+)\.[a-Z]+([0-9]+)-.+$/\1.\2/' | tr 'A-Z' 'a-z')
        sed -i "s|^cluster.node.serverId.*|cluster.node.serverId = $short_name|" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
        # the follwing will update or add if not present the jvmRoute property
        sed -i -n -e '/^\s*jahia.session.jvmRoute\s*=/!p' -e "\$ajahia.session.jvmRoute = $short_name" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
        grep -q '^JMX_OPTS=\-XX:+UseParallelGC$' ${tomcat_env} || sed -i "2i JMX_OPTS=\-XX:+UseParallelGC" ${tomcat_env}
        sed -i '/<!-- Access log processes all example./i \\t<!-- Remote IP Valve -->\n \t<Valve className="org.apache.catalina.valves.RemoteIpValve" protocolHeader="X-Forwarded-Proto" />\n' /opt/tomcat/conf/server.xml
        #Secure cookies from cross scripting
        indent="      " && printf "$indent<cookie-config>\n$indent$indent<secure>true</secure>\n$indent$indent<http-only>true</http-only>\n$indent</cookie-config>\n" > /tmp/cookies-config
        sed -i '/<session-config>/r /tmp/cookies-config' /opt/tomcat/conf/web.xml && rm /tmp/cookies-config
        # the following will update or add if not present these attributes: jvmRoute, maxPostSize, maxHttpHeaderSize
        xmlstarlet ed -P -L -S \
          -u "Server/Service/Engine/@jvmRoute" -v "$short_name" \
          -i "Server/Service/Engine[not(@jvmRoute)]" -t attr -n "jvmRoute" -v "$short_name" \
          -u "Server/Service/Connector[@port='80']/@maxPostSize" -v '${maxPostSize}' \
          -i "Server/Service/Connector[@port='80' and not(@maxPostSize)]" -t attr -n "maxPostSize" -v '${maxPostSize}' \
          -u "Server/Service/Connector[@port='80']/@maxHttpHeaderSize" -v '65536' \
          -i "Server/Service/Connector[@port='80' and not(@maxHttpHeaderSize)]" -t attr -n "maxHttpHeaderSize" -v '65536' \
          /opt/tomcat/conf/server.xml
        # jahia_cfg_cluster_tcp_bindAddress has to be exported in order for jahia's java process to see it since PAAS-2119 (jelastic/tomcat:9.0.75-openjdk-11.0.19 moved init scripts to systemd)
        grep -q '^export jahia_cfg_cluster_tcp_bindAddress=' ${tomcat_env} || sed -i '/TOMCAT_USER=/ a export jahia_cfg_cluster_tcp_bindAddress=$(hostname -i)' ${tomcat_env}
        sed -i '/^jahia_cfg_cluster_tcp_bindAddress=/d' ${tomcat_env}  # remove old fashion line if needed
        grep -q '^JAVA_OPTS.*-DmaxPostSize=' ${tomcat_env} || sed -i -E '/^JAVA_OPTS/ s/(.)$/ -DmaxPostSize=${tomcat_cfg_maxpostsize}\1/g' ${tomcat_env}
        grep -q '^JAVA_OPTS.*-XX:NativeMemoryTracking=' ${tomcat_env} || sed -i -E '/^JAVA_OPTS/ s/(.)$/ -XX:NativeMemoryTracking=summary\1/g' ${tomcat_env}
        grep -q '^JAVA_OPTS.*-Djava.security.egd=' ${tomcat_env} || sed -i -E '/^JAVA_OPTS/ s|(.)$| -Djava.security.egd=file:/dev/urandom\1|g' ${tomcat_env}
        sed -i 's/SHUTDOWN_WAIT.*=.*/SHUTDOWN_WAIT=30/' ${tomcat_env}
        # Datadog APM
        grep -q '^APM_OPTS=*' ${tomcat_env} || echo 'APM_OPTS="-Ddd.profiling.enabled=true -XX:FlightRecorderOptions=stackdepth=256 -Ddd.logs.injection=true -javaagent:/opt/tomcat/datadog/dd-java-agent.jar -Ddd.service=jahia -Ddd.env=${env.domain} -Ddd.trace.classes.exclude=org.jahia.modules.forms.dsl.*,org.jahia.modules.databaseConnector.dsl.* -Ddd.resolver.use.loadclass=false -Ddd.profiling.disabled.events=jdk.OldObjectSample"' >>  ${tomcat_env}
        grep -q '^$DATADOG_APM_ENABLED*' ${tomcat_env} || echo '$DATADOG_APM_ENABLED && JAVA_OPTS+=" $APM_OPTS"' >> ${tomcat_env}

    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred when installing jahia."

  setupDatadogAgentJahia:
    - log: "## Finalize Datadog agent setup on ${this}"
    - setGlobalRepoRootUrl
    - isAugSearchEnabled
    - generateNodesListFile
    - getPatTokenAndKey
    - cmd[${this}]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        NODE_NAME=${HOSTNAME/-*}
        echo "hostname: ${_ROLE}.${NODE_NAME#node}" >> /etc/datadog-agent/datadog.yaml
        chmod 644 /opt/tomcat/logs/catalina.out
        mkdir /etc/datadog-agent/conf.d/jelastic.d /var/log/jelastic-packages
        chown tomcat:root /var/log/jelastic-packages
        chown dd-agent: /etc/datadog-agent/conf.d/jelastic.d
        curl -fLSso /etc/datadog-agent/conf.d/jelastic.d/conf.yaml ${globals.repoRootUrl}/assets/common/dd_agent_jelastic_package_conf.yml || exit 1
        # Configure AS check and Healthcheck metric
        sed -i "s|jahia_root_token_placeholder|$__secret__pat_token|g" /etc/datadog-agent/conf.d/augmented_search.yaml-disabled \
          /etc/datadog-agent/conf.d/healthcheck_metric.d/healthcheck_metric.yaml
        if [ "${globals.isAugSearchEnabled}" == "true" ]; then
          mv /etc/datadog-agent/conf.d/augmented_search.yaml-disabled /etc/datadog-agent/conf.d/augmented_search.yaml
        fi
        if [ "${_ROLE}" == "Processing" ]; then
          mv /etc/datadog-agent/conf.d/jahia_local_revisions_discrepancies.yaml{-disabled,}
          sed "s/__PASSWORD__/${DB_USER_DATADOG}/" -i /etc/datadog-agent/conf.d/jahia_local_revisions_discrepancies.yaml
        fi
        /usr/local/bin/set_dd_tags.sh
    - if (nodes.sqldb.length > 1):
        cmd[${this}]: |-
          setfacl -m u:dd-agent:rx /var/log/glusterfs/*.log
          sed -i '/killall.*glusterd/a \  /usr\/bin\/setfacl -m u:dd-agent:rx \/var\/log\/glusterfs\/*.log' /etc/logrotate.d/glusterfs
          cat > /etc/datadog-agent/conf.d/glusterfs.d/conf.yaml << EOF
          logs:
            - type: file
              path: /var/log/glusterfs/share.log
              source: glusterfs
              service: glusterfs
          EOF
          systemctl restart rsyslog crond datadog-agent
    - else:
        cmd[${this}]: |-
          setfacl -m u:dd-agent:rx /var/log/messages
          sed -i '/\/bin\/kill/a \        setfacl -m u:dd-agent:rx /var/log/messages' /etc/logrotate.d/syslog
          cat > /etc/datadog-agent/conf.d/nfsstat.d/conf.yaml << EOF
          init_config:
          instances:
            -
              min_collection_interval: 60
          logs:
            - type: file
              path: /var/log/messages
              source: nfsstat
              service: nfs
              log_processing_rules:
              - type: include_at_match
                name: include_nfsstat_only
                pattern: "nfsstat"
          EOF
          systemctl restart rsyslog crond datadog-agent

  startupJahiaHealthCheck:
    # Two arguments:
    #   - target: Mandatory, the target nodeId or nodeGroup. If the duration is not specified, the target
    #     can be passed as a parameter directly after the action name, e.g.: startupJahiaHealthCheck: <target>
    #   - duration: Optional, duration in seconds. Default value of 24 hours if not specified
    # The .print() call surrounded by simple quotes is the only working way I found to test if the variable exists
    - if ('${this.print()}' != ''):
        set:
          target: ${this}
    - else:
        set:
    - getPatTokenAndKey
    - cmd [${this.target}]: |-
        __secret__jahia_cfg_healthcheck_token=${globals.__secret__pat_token}
        if ! tomcat_pid=$(pgrep -u tomcat -f java); then
          echo "[ERROR] Tomcat process not found, please check." >&2
          exit 1
        fi

        timeout 60 bash -c '
          until [ -f /var/log/tomcat/jahia.log ]; do
            sleep 0.5
          done
        '
        if (($?)); then
          echo "[ERROR] Jahia log file not found, it seems there is a problem with tomcat instance, please check." >&2
          exit 2
        fi

        startup_line=$(grep -n "s t a r t i n g" /opt/tomcat/logs/catalina.out | tail -n1 | cut -d":" -f1)
        timeout=$(date --date="+$HEALTHCHECK_DURATION minutes" +%s)
        hc_url="http://127.0.0.1:8080/modules/healthcheck?severity=critical"

        # Number of minutes allowed for healthcheck to be completed once tomcat startup is finished
        jahia_running_timeout=5

        while [ $(date +%s) -lt $timeout ]; do
          # First we test if Jahia is up with a curl request.
          if curl_resp=$(curl -f -s -m 1 "$hc_url" -H "authorization: APIToken $__secret__jahia_cfg_healthcheck_token"); then
            status=$(echo $curl_resp | jq -r ".status.health")
            if [ "$status" = "GREEN" ] || [ "$status" = "YELLOW" ]; then
              exit 0
            fi
          fi

          # If it isn't, we first check tomcat process status
          if ! ps --pid $tomcat_pid > /dev/null; then
            echo "[ERROR] Tomcat process no more running, please check." >&2
            exit 3
          fi

          # Then we check Jahia startup status, all
          tail -n +${startup_line} /opt/tomcat/logs/catalina.out | grep -q "Server startup in"
          if [ $? -eq 0 ]; then
            if [ $jahia_running_timeout -eq 1 ]; then
              for module in graphql-dxm-provider server-availability-manager
              do
                curl -SsH "Authorization: APIToken $__secret__jahia_cfg_healthcheck_token" -XPOST -d "" \
                  "localhost/modules/api/bundles/$module/_refresh"
                curl -SsH "Authorization: APIToken $__secret__jahia_cfg_healthcheck_token" -XPOST -d "" \
                  "localhost/modules/api/bundles/$module/_stop"
                curl -SsH "Authorization: APIToken $__secret__jahia_cfg_healthcheck_token" -XPOST -d "" \
                  "localhost/modules/api/bundles/$module/_start"
              done
            fi
            if [ $jahia_running_timeout -eq 0 ]; then
              echo "[ERROR] Tomcat startup is finished but healthcheck failed, please check." >&2
              exit 4
            fi
            ((jahia_running_timeout-=1))
          fi

          sleep 60
        done

        echo "[ERROR] Timeout, the Tomcat process is still running but Jahia is not started yet" >&2
        exit 5

  checkJahiaHealth:
    - getPatTokenAndKey
    - cmd [${this}]: |-
        __secret__jahia_cfg_healthcheck_token=${globals.__secret__pat_token}
        if ! tomcat_pid=$(pgrep -u tomcat -f java); then
          echo "[ERROR] Tomcat process not found, please check" >&2
          exit 1
        fi

        hc_url="http://127.0.0.1:8080/modules/healthcheck?severity=critical"

        if curl_resp=$(curl -f -s -m 1 "$hc_url" -H "authorization: APIToken $__secret__jahia_cfg_healthcheck_token"); then
          status=$(echo $curl_resp | jq -r ".status.health")
          if [ "$status" = "GREEN" ] || [ "$status" = "YELLOW" ]; then
            exit 0
          fi
        fi
        echo "[ERROR] Healthcheck result different from GREEN or YELLOW, exiting" 1>&2 && exit 1

  checkJahiaDatadogCustomChecks:
    - if (nodes.sqldb.length == 3):
        - checkDatadogAgentCheck:
            target: ${this}
            checkName: proxysql_backend_missing
        - checkDatadogAgentCheck:
            target: ${this}
            checkName: proxysql_connections_discrepancies
    - else:
        - checkDatadogAgentCheck:
            target: ${this}
            checkName: proxysql
    - checkDatadogAgentCheck:
        target: ${this}
        checkName: jahia_node_not_in_haproxy_pool
    - checkDatadogAgentCheck:
        target: ${this}
        checkName: strongswan_connections_status
    - cmd[${this}]: |-
        if [ -f /etc/datadog-agent/conf.d/augmented_search.yaml-disabled ]; then
          echo "disabled"
        fi
    - if ("${response.out}" != "disabled"):
      - checkDatadogAgentCheck:
          target: ${this}
          checkName: augmented_search

  saveApplicationcontextFilesBeforeRedeploy:
    cmd[${this}]: |-
      mkdir /opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-files.tmp
      mv /opt/tomcat/conf/digital-factory-config/jahia/applicationcontext*.xml /opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-files.tmp/

  restoreApplicationcontextFilesAfterRedeploy:
    cmd[${this}]: |-
      mv /opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-files.tmp/* /opt/tomcat/conf/digital-factory-config/jahia/
      rmdir /opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-files.tmp

  deleteEnvLinkJahia:
    # Parameters:
    #   - jCustomerEnv: Jcustomer env name in the env link of Jahia
    # Returns:
    #   - ${response.link_removed}: true or false (true if Jahia env is removed from envLink of Jcustomer)
    - script: |-
        const envName = "${env.envName}";
        const jCustomerEnv = "${this.jCustomerEnv}";
        envsLinked = jelastic.env.control.GetNodeGroups(jCustomerEnv, session).object.filter(function (object) {
                                return object.name == "cp";
                              }).pop().envLink;
        if (envsLinked.indexOf(envName) == -1) {
          return {"result": 0, "link_removed": false, "out": envName + " not in envLink of " + jCustomerEnv};
        }

        // envLink can contain multiple Jahia envs on jCustomer side separated by a comma
        if (envsLinked.indexOf(",") != -1) {
          envsLinkedArr = envsLinked.split(",");
          envsLinkedArr.splice(envsLinkedArr.indexOf(envName), 1);
          newEnvLink = String(envsLinkedArr);
        } else {
          newEnvLink = null;
        }
        resp = jelastic.env.nodegroup.ApplyData(jCustomerEnv, session, nodeGroup='cp', data={'envLink': newEnvLink});
        return {"result": 0, "link_removed": true, "out": envName + " removed from envLink of " + jCustomerEnv};

  removeKibanaDashboardAccountsAndSpace:
    # Parameters:
    #   - jCustomerEnv: Jcustomer env name in the env link of Jahia
    - setGlobalRepoRootUrl
    - set:
        dashboardRoleAndAccountName: ${env.envName}-kibana-dashboard
        customerUserRoleAndAccountName: ${env.envName}-kibana-user
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/delete-kibana-role.yml",
              envName: "${this.jCustomerEnv}",
              settings: {
                'roleName': '${this.dashboardRoleAndAccountName}',
              }
            }
          );
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/delete-elasticsearch-account.yml",
              envName: "${this.jCustomerEnv:}",
              settings: {
                'accountName': '${this.dashboardRoleAndAccountName}',
              }
            }
          );
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/delete-elasticsearch-account.yml",
              envName: "${this.jCustomerEnv:}",
              settings: {
                'accountName': '${this.customerUserRoleAndAccountName}',
              }
            }
          );
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/delete-kibana-space.yml",
              envName: "${this.jCustomerEnv}",
              settings: {
                'spaceName': '${this.dashboardRoleAndAccountName}',
              }
            }
          );

  switchFullReadonly:
    - log: "switch full read mode to ${this.fullreadmode} on nodegroup ${this.group}"
    - cmd[${this.group}]: |-
          max_try=15
          try=0
          until (ssh abricot@localhost -p 8101 -i /tmp/abricot -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR full-read-only ${this.fullreadmode}); do
            [[ $try == $max_try ]] && exit 1
            ((try++))
          done
          # it's not elegant, but it seems that the "until" loop can have an "exit status" of 1 if the second try is successful
          # so we force an "0" exit status if the loop ended before max_try is reached
          true

  switchReadonly:
    - log: "switch read-only mode to ${this.readmode} on nodegroup ${this.group}"
    - cmd[${this.group}]: |-
        # Don't know why but running this command in one line fails half of the time so the output is captured and displayed so we have two lines.
        res=$(ssh abricot@localhost -p 8101 -i /tmp/abricot -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR read-only ${this.readmode})
        echo $res

  enableKarafLogin:
    - isKarafLoginEnabled: ${globals.isKarafLoginEnabled}
      script: |-
          try {
            var resp = JSON.parse(isKarafLoginEnabled)["${this}"]
          } catch(error) {
            var resp = false
          }
          return {"result": 0, "resp": resp}
    - if(${response.resp}):
        - log: "karaf login is already enabled, nothing to do"
    - else:
        - log: "Activate karaf's ssh login on ${this}"
        - cmd[${this}]: |-
            # Clear everything and enable karaf login
            [ -f /tmp/abricot ] && rm /tmp/abricot
            [ -f /tmp/abricot.pub  ] && rm /tmp/abricot.pub
            ssh-keygen -t rsa -f /tmp/abricot -P ""
            awk '{printf "abricot:%s,_g_:admingroup\n",$2}' /tmp/abricot.pub >> /data/digital-factory-data/karaf/etc/keys.properties
            sed 's,\(sshRealm\s*=\s*\)jahia,\1karaf,' -i /data/digital-factory-data/karaf/etc/org.apache.karaf.shell.cfg
            i=1
            it=66
            until (ssh abricot@localhost -p 8101 -i /tmp/abricot -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR full-read-only); do
              echo "karaf ssh login not updated yet (iteration $i/$it)"
              if [ $i -ge $it ]; then
                echo "Too long to start, something is wrong here... EXITING"
                exit 1
              fi
              ((i++))
              sleep 1
            done
        - isKarafLoginEnabled: ${globals.isKarafLoginEnabled}
          script: |-
            try {
              var isEnable = JSON.parse(isKarafLoginEnabled)
            } catch(error) {
              var isEnable = {}
            } finally {
              isEnable["${this}"] = true
            }
            return {"result": 0, "isEnable": isEnable}
        - setGlobals:
            isKarafLoginEnabled: ${response.isEnable}
            karafConsole: "ssh abricot@localhost -p 8101 -i /tmp/abricot -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR"

  disableKarafLogin:
    - isKarafLoginEnabled: ${globals.isKarafLoginEnabled}
      script: |-
          try {
            var resp = JSON.parse(isKarafLoginEnabled)["${this}"]
          } catch(error) {
            var resp = false
          }
          return {"result": 0, "resp": resp}
    - if(! ${response.resp}):
        - log: "karaf login is already disabled, nothing to do"
    - else:
        - log: "Disable karaf's ssh login on ${this}"
        - cmd[${this}]: |-
            [ -f /tmp/abricot  ] && rm /tmp/abricot
            [ -f /tmp/abricot.pub  ] && rm /tmp/abricot.pub
            sed '/^abricot:/d' -i /data/digital-factory-data/karaf/etc/keys.properties
            sed 's,\(sshRealm\s*=\s*\)karaf,\1jahia,' -i /data/digital-factory-data/karaf/etc/org.apache.karaf.shell.cfg
        - isKarafLoginEnabled: ${globals.isKarafLoginEnabled}
          script: |-
            try {
              var isEnable = JSON.parse(isKarafLoginEnabled)
            } catch(error) {
              var isEnable = {}
            } finally {
              isEnable["${this}"] = false
            }
            return {"result": 0, "isEnable": isEnable}
        - setGlobals:
            isKarafLoginEnabled: ${response.isEnable}

  enableReadOnlyOnCluster:
    - enableKarafLogin: "proc, cp"
    - switchReadonly:
        group: "proc, cp"
        readmode: "ON"
    - disableKarafLogin: "proc, cp"

  disableReadOnlyOnCluster:
    - enableKarafLogin: "proc, cp"
    - switchReadonly:
        group: "proc, cp"
        readmode: "OFF"
    - disableKarafLogin: "proc, cp"

  enableFullReadOnlyOnCluster:
    - enableKarafLogin: proc
    - switchFullReadonly:
        group: "proc"
        fullreadmode: "ON"
    - disableKarafLogin: proc

  disableFullReadOnlyOnCluster:
    - enableKarafLogin: proc
    - switchFullReadonly:
        group: "proc"
        fullreadmode: "OFF"
    - disableKarafLogin: proc

  getJahiaVersion:
  # The placeholder ${nodes.proc.first.customitem.nodeVersion} doesn't return the correct value during an upgrade
  # and thus jelastic API is more reliable in this case.
    - log: "Get jahia version"
    - script: |-
        const envName = "${env.envName}";
        resp = jelastic.env.control.GetEnvInfo(envName, session)
        for (i = 0, g = resp.nodes; i < g.length; i++) {
          if (g[i].nodeGroup == "proc") {
            nodeVersion = g[i].customitem.nodeVersion
            return {
              "result": 0,
              "onAfterReturn": {
                "setGlobals": {
                  "jahiaVersion": nodeVersion
                }
              }
            }
          }
        }
        return {"result": 1, errOut: "Can't get Jahia version"}
    - log: "Jahia is v${globals.jahiaVersion}"

  installOrUpgradeModule:
    # Installs and starts a module with the specified version. If the module is already installed, several use cases:
    # - if it is not running, then the specified version is installed and started
    # - if it is running with an older version, then the specified version is installed and started
    # - if it is running with the same version or a more recent one, then nothing is done, to prevent downgrade
    # Last step is to clean the module by uninstalling stopped versions to only keep the running (and desired) one
    #
    # Parameters:
    # - moduleSymname: the symbolic name of the module to upgrade (for instance "distributed-sessions")
    # - moduleVersion: the version of the module to install
    # - moduleGroupId: group id of the artifact
    # - moduleRepository: repository that the artifact is contained in Nexus
    - set:
        moduleInstalled: true
    - checkModule:
        moduleSymname: ${this.moduleSymname}
    - if ("${globals.moduleState}" == "uninstalled"):
        - installModule:
            moduleSymname: ${this.moduleSymname}
            moduleVersion: ${this.moduleVersion}
            moduleGroupId: ${this.moduleGroupId}
            moduleRepository: ${this.moduleRepository}
            startModule: true
    - else:
        - if ("${globals.moduleState}" == "installed"):
            - installModule:
                moduleSymname: ${this.moduleSymname}
                moduleVersion: ${this.moduleVersion}
                moduleGroupId: ${this.moduleGroupId}
                moduleRepository: ${this.moduleRepository}
                startModule: true
        - if ("${globals.moduleState}" == "started"):
            - isVersionStrictlyHigher:
                a: "${this.moduleVersion}"
                b: "${globals.runningVersion}"
                res: needsInstall
            - if (${globals.needsInstall}):
                - installModule:
                    moduleSymname: ${this.moduleSymname}
                    moduleVersion: ${this.moduleVersion}
                    moduleGroupId: ${this.moduleGroupId}
                    moduleRepository: ${this.moduleRepository}
                    startModule: true
            # If the module is already running with the desired version or a more recent one, we don't need to check the status right after
            - else:
                # Prints a warning if the module is running with a more recent version only. If same version, nothing is wrong so no warning
                - if ("${globals.runningVersion}" != "${this.moduleVersion}"):
                    - log: "WARNING: ${this.moduleSymname} is already running with a more recent version (${globals.runningVersion}), version ${this.moduleVersion} won't be installed"
                - set:
                    moduleInstalled: false
    - if (${this.moduleInstalled}):
        - checkModule:
            moduleSymname: ${this.moduleSymname}
        - if ("${globals.moduleState}" != "started") || ("${globals.runningVersion}" != "${this.moduleVersion}"):
            - log: ${this.moduleSymname} is either not running, or running with a different version than the desired one
            - return:
                type: error
                message: "An error occurred during module upgrade/installation."
    - uninstallStoppedVersionsOfModule:
        moduleSymname: ${this.moduleSymname}
        moduleVersion: ${this.moduleVersion}

  checkModule:
    # Checks the state of a module via the endpoint modules/api/bundles (or fallback to the digital-factory-data/bundles-deployed folder if the previous one failed).
    # First we check the state of the module, uninstalled, installed or started. Then, if installed or started,
    # we check the version: most recent version installed if module is not running, or running version otherwise.
    #
    # Parameters:
    # - moduleSymname: the symbolic name of the module to upgrade (for instance "distributed-sessions")
    # Returns:
    # - globals.moduleState:
    #   - "started" if the module is running
    #   - "installed" if the module is installed but not running
    #   - "uninstalled" if the module is not installed
    # - globals.mostRecentInstalledVersion: the most recent version installed (or an empty string if the module is uninstalled)
    # - globals.installedVersionsCount: the number of installed versions (or an empty string if the module is uninstalled)
    # - globals.runningVersion: the version running (or an empty string if the module is not running)
    - setGlobals:  # re-initialise globals to prevent successive calls to this action from overlapping
        moduleState: ""
        runningVersion: ""
        mostRecentInstalledVersion: ""
        installedVersionsCount: 0
    - getPatTokenAndKey
    - cmd [proc]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        resp=$(curl -fSsH "Authorization: APIToken $__secret__pat_token" "localhost/modules/api/bundles/${this.moduleSymname}/*/_localInfo")
        if [ $? -ne 0 ]; then  # if curl failed, we fallback to the FS method
          modules_found=$(grep "/${this.moduleSymname}/" -l /data/digital-factory-data/bundles-deployed/*/bundle.info)
          read -r -d '' awkScript <<- 'EOS'
        	BEGIN {
        	  FS=":"
        	  printf "{"
        	  file_count=ARGC-1
        	}
        	BEGINFILE {
        	  file_number++
        	  NR=0
        	}
        	{
        	  switch(NR) {
        	    case 2:
        	      module_ver=$NF
        	      break
        	    case 3:
        	      switch($1) {
        	        case 1:
        	          status="UNINSTALLED"
        	          break
        	        case 2:
        	          status="INSTALLED"
        	          break
        	        case 4:
        	          status="RESOLVED"
        	          break
        	        case 8:
        	          status="STARTING"
        	          break
        	        case 16:
        	          status="STOPPING"
        	          break
        	        case 32:
        	          status="STARTED"
        	          break
        	        default:
        	          status="go fuck yourself"
        	          break
        	      }
        	      break
        	  }
        	}
        	ENDFILE {
        	  if(file_number<file_count) {
        	    sep=","
        	  }
        	  else {
        	    sep=""
        	  }
        	  printf "\"%s\": {\"moduleState\":\"%s\"}%s",module_ver,status,sep
        	}
        	END {
        	  print "}"
        	}
        	EOS
          if [ $(echo $modules_found | wc -w) -ne 0 ]; then
            resp=$(awk "$awkScript" $modules_found)
          else
            resp="{}"
          fi
        fi

        case "$(jq -r ". | length" <<< $resp)" in
          "0") # Means the module is not installed
            module_state=uninstalled
            ;;
          "") # if response is not JSON
            echo $resp >&2
            exit 1
            ;;
          *)
            module_started=$(jq -r 'to_entries[] | "\(.key);\(.value | .moduleState)"' <<< $resp | grep STARTED)
            if [ $? -eq 0 ]; then
              module_state=started
              running_version=$(echo $module_started | cut -d';' -f1 | awk -F'/' '{print $NF}')
            else
              module_state=installed
            fi
            most_recent_installed_version=$(jq -r "keys[]" <<< $resp | awk -F'/' '{print $NF}' | sort -nr | head -1)
            installed_versions_count=$(jq -r "keys | length" <<< $resp)
            ;;
        esac
        echo "{ \
            'moduleState': '$module_state', \
            'runningVersion': '$running_version', \
            'mostRecentInstalledVersion': '$most_recent_installed_version', \
            'installedVersionsCount': '$installed_versions_count' \
          }"
    - script: |-
        const resp = ${response.out.toJSON()}
        return {
          "result": 0,
          "onAfterReturn": {
            setGlobals: resp
          }
        };

  checkVersionOfModule:
    # Checks the state of a module for a specific version: uninstalled, installed or started.
    #
    # Parameters:
    # - moduleSymname: the symbolic name of the module to upgrade (for instance "distributed-sessions")
    # - moduleVersion: the version of the module to check
    # Returns:
    # - globals.moduleVersionState:
    #   - "started" if the specified version of the module is running
    #   - "installed" if the specified version of the module is installed but not running
    #   - "uninstalled" if the specified version of the module is not installed
    - getPatTokenAndKey
    - cmd [proc]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        resp=$(curl -SsH "Authorization: APIToken $__secret__pat_token" "localhost/modules/api/bundles/${this.moduleSymname}/${this.moduleVersion}/_localInfo")
        case "$(jq -r .moduleState <<< $resp)" in
          "") # if response is not JSON, for instance 401 unauthorized
            echo "error"
            echo $resp >&2
            ;;
          STARTED) # Means the module is running
            echo "started"
            ;;
          null|UNINSTALLED) # null is in case a 404 (but still JSON) is returned (normal behavior)
            echo "uninstalled"
            ;;
          *)
            echo "installed"
            ;;
        esac
    - if ("${response.out}" == "error"):
        - return:
            type: error
            message: "An error occurred during module verification."
    - else:
        - log: ${this.moduleSymname}/${this.moduleVersion} is ${response.out}
        - setGlobals:
                moduleVersionState: ${response.out}

  installModule:
    # Downloads a module jar file and installs it. The module can also be started at the same time.
    # If the module is already installed, the installation is skipped, but if the "startModule" parameter is set to true,
    # then the specified version of the module is started.
    #
    # Parameters:
    #   - moduleSymname: the symbolic name of the module to upgrade (for instance "distributed-sessions")
    #   - moduleVersion: the version of the module to install
    #   - moduleGroupId: group id of the artifact in Nexus
    #   - moduleRepository: repository that the artifact is contained in Nexus
    #   - startModule (optional): if the module should be started or not, true or false. Default: false
    - checkVersionOfModule:
        moduleSymname: ${this.moduleSymname}
        moduleVersion: ${this.moduleVersion}
    - if ("${globals.moduleVersionState}" == "uninstalled"):
      - set:
          modulesUrl: https://devtools.jahia.com/nexus/service/local/artifact/maven/content
      - if ('${this.startModule.print()}' == ''):
          set:
            startModule: "false"
      - getNexusCredentials
      - getPatTokenAndKey
      - log: Installing ${this.moduleSymname}/${this.moduleVersion}
      - cmd [proc]: |-
          __secret__nexus_creds=$(echo -n "${globals.__secret__nexusLogin}:${globals.__secret__nexusPassword}" | base64)
          __secret__pat_token="${globals.__secret__pat_token}"

          tmp_dir=$(mktemp -d)
          local_jar_file=$tmp_dir/${this.moduleSymname}-${this.moduleVersion}.jar
          curl -fLSso $local_jar_file "${this.modulesUrl}" -G \
            -H "Authorization: Basic $__secret__nexus_creds" \
            -d "g=${this.moduleGroupId}" \
            -d "r=${this.moduleRepository}" \
            -d "a=${this.moduleSymname}" \
            -d "v=${this.moduleVersion}" >&2
          if [ $? -ne 0 ]; then
            echo "error"
            rm -rf $tmp_dir
            exit 0
          fi
          resp=$(curl -SsH "Authorization: APIToken $__secret__pat_token" -XPOST \
                  --form bundle=@$local_jar_file \
                  --form start=${this.startModule} \
                  --form ignoreChecks=true \
                  localhost/modules/api/bundles)
          case "$(jq -r .bundleInfos <<< $resp)" in
            "null") # If install failed, there is no bundleInfos attribute in the JSON response
              echo "error"
              jq -r .message <<< $resp >&2
              ;;
            "") # if response is not JSON, for instance 401 unauthorized
              echo "error"
              echo $resp >&2
              ;;
            *) # Otherwise it means the module was installed, nothing to do
              ;;
          esac
          rm -rf $tmp_dir
      - if ("${response.out}" == "error"):
          - return:
              type: error
              message: "An error occurred during module installation."
    - elif ("${globals.moduleVersionState}" == "installed"):
        - log: ${this.moduleSymname}/${this.moduleVersion} is already installed, skipping the installation
        - if (${this.startModule}):
            - startModule:
                moduleSymname: ${this.moduleSymname}
                moduleVersion: ${this.moduleVersion}
    - else:
        - log: ${this.moduleSymname}/${this.moduleVersion} is already running, nothing to do
    - if ("${globals.moduleVersionState}" != "started") && (${this.startModule}):
      - checkVersionOfModule:
          moduleSymname: ${this.moduleSymname}
          moduleVersion: ${this.moduleVersion}
      - if ("${globals.moduleVersionState}" != "started"):
        - script: |-
            return {"result": 0, "tryCount": [1,2,3,4,5,6,7,8,9,10]}
        - forEach(response.tryCount):
          - if ("${globals.moduleVersionState}" != "started"):
            - log: "Try #${@i}..."
            - checkVersionOfModule:
                moduleSymname: ${this.moduleSymname}
                moduleVersion: ${this.moduleVersion}
            - if ("${globals.moduleVersionState}" != "started"):
              - sleep: 5000
        - if ("${globals.moduleVersionState}" != "started"):
          - return:
              type: error
              message: "An error occurred during module startup."

  stopModule:
    # Stops the running version of a module.
    #
    # Parameters:
    #   - moduleSymname: the symbolic name of the module to stop (for instance "distributed-sessions")
    - getPatTokenAndKey
    - log: Stopping ${this.moduleSymname}
    - cmd [proc]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        resp=$(curl -SsH "Authorization: APIToken $__secret__pat_token" -XPOST -d "" \
                 "localhost/modules/api/bundles/${this.moduleSymname}/_stop")
        case "$(jq -r .bundleInfos <<< $resp)" in
          "null") # If stop failed, there is no bundleInfos attribute in the JSON response
            echo "error"
            jq -r .message <<< $resp >&2
            ;;
          "") # if response is not JSON, for instance 401 unauthorized
            echo "error"
            echo $resp >&2
            ;;
          *) # Otherwise it means the module was stopped, nothing to do
            ;;
        esac
    - if ("${response.out}" == "error"):
        - return:
            type: error
            message: "An error occurred during module stop."
    - log: ${this.moduleSymname}/${this.moduleVersion} has been stopped


  startModule:
    # Starts the specified version of a module.
    #
    # Parameters:
    #   - moduleSymname: the symbolic name of the module to start (for instance "distributed-sessions")
    #   - moduleVersion: the version of the module to install
    - getPatTokenAndKey
    - log: Starting ${this.moduleSymname}/${this.moduleVersion}
    - cmd [proc]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        resp=$(curl -SsH "Authorization: APIToken $__secret__pat_token" -XPOST -d "" \
                 "localhost/modules/api/bundles/${this.moduleSymname}/${this.moduleVersion}/_start")
        case "$(jq -r .bundleInfos <<< $resp)" in
          "null") # If install failed, there is no bundleInfos attribute in the JSON response
            echo "error"
            jq -r .message <<< $resp >&2
            ;;
          "") # if response is not JSON, for instance 401 unauthorized
            echo "error"
            echo $resp >&2
            ;;
          *) # Otherwise it means the module was started, nothing to do
            ;;
        esac
    - if ("${response.out}" == "error"):
        - return:
            type: error
            message: "An error occurred during module startup."
    - log: ${this.moduleSymname}/${this.moduleVersion} has been started

  uninstallModuleVersion:
    # Uninstalls the specified version of a module.
    #
    # Parameters:
    #   - moduleSymname: the symbolic name of the module to upgrade (for instance "distributed-sessions")
    #   - moduleVersion: the version of the module to install
    - getPatTokenAndKey
    - log: Uninstalling ${this.moduleSymname}/${this.moduleVersion}
    - cmd [proc]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        resp=$(curl -SsH "Authorization: APIToken $__secret__pat_token" -XPOST -d "" \
              "localhost/modules/api/bundles/${this.moduleSymname}/${this.moduleVersion}/_uninstall")
        case "$(jq -r .bundleInfos <<< $resp)" in
          "null") # If uninstall failed, there is no bundleInfos attribute in the JSON response
            echo "error"
            jq -r .message <<< $resp >&2
            ;;
          "") # if response is not JSON, for instance 401 unauthorized
            echo "error"
            echo $resp >&2
            ;;
          *) # Otherwise it means the module was uninstalled, nothing to do
            ;;
        esac
    - if ("${response.out}" == "error"):
        - return:
            type: error
            message: "An error occurred during module uninstall."
    - log: Version ${this.moduleVersion} of ${this.moduleSymname} module uninstalled

  uninstallStoppedVersionsOfModule:
    # Uninstalls all the stopped versions of a module. If no version of the module is running, the module
    # will be completely uninstalled. If the module is not installed, nothing is done.
    #
    # Parameters:
    #   - moduleSymname: the module to clean
    - getPatTokenAndKey
    - log: Uninstalling stopped versions of ${this.moduleSymname} module
    - cmd [proc]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        resp=$(curl -SsH "Authorization: APIToken $__secret__pat_token" "localhost/modules/api/bundles/${this.moduleSymname}/*/_localInfo")
        case "$(jq -r ". | length" <<< $resp)" in
          "0") # Means the module is not installed, nothing to do
            ;;
          "") # if response is not JSON, for instance 401 unauthorized
            echo "error"
            echo $resp >&2
            ;;
          *)
            jq -r 'to_entries[] | "\(.key);\(.value | .moduleState)"' <<<$resp | grep -v STARTED | cut -d';' -f1 | awk -F'/' '{print $NF}' | tr "\n" ";" | sed 's/.$//'
            ;;
        esac
    - if ("${response.out}" == ""):
        log: ${this.moduleSymname} module doesn't have any stopped version, nothing to do
    - elif ("${response.out}" == "error"):
        - return:
            type: error
            message: "An error occurred during module installation."
    - else:
        - log: "Versions to delete for ${this.moduleSymname} module: ${response.out}"
        - script: |-
            return {"result": 0, "versions": "${response.out}".split(";")}
        - forEach(response.versions):
            - uninstallModuleVersion:
                moduleSymname: ${this.moduleSymname}
                moduleVersion: ${@i}
        - log: Stopped versions of ${this.moduleSymname} module uninstalled

  uninstallModule:
    # Uninstalls all the versions of a module If the module is not installed, nothing is done.
    #
    # Parameters:
    #   - moduleSymname: the module to clean
    - getPatTokenAndKey
    - log: Uninstalling all versions of ${this.moduleSymname} module
    - cmd[proc]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        resp=$(curl -SsH "Authorization: APIToken $__secret__pat_token" "localhost/modules/api/bundles/${this.moduleSymname}/*/_localInfo")
        case "$(jq -r ". | length" <<< $resp)" in
          "0") # Means the module is not installed
            echo "not installed"
            ;;
          "") # if response is not JSON, for instance 401 unauthorized
            echo "error"
            echo $resp >&2
            ;;
          *)
            jq -r "keys[]" <<< $resp | awk -F'/' '{print $NF}' | sort -nr | tr "\n" ";" | sed 's/.$//'
            ;;
        esac
    - if ("${response.out}" == "not installed"):
        log: ${this.moduleSymname} module is not installed, nothing to do
    - elif ("${response.out}" == "error"):
        - return:
            type: error
            message: "An error occurred during module uninstall."
    - else:
        - log: "Versions to delete for ${this.moduleSymname} module: ${response.out}"
        - script: |-
            return {"result": 0, "versions": "${response.out}".split(";")}
        - forEach(response.versions):
            - uninstallModuleVersion:
                moduleSymname: ${this.moduleSymname}
                moduleVersion: ${@i}
        - log: ${this.moduleSymname} module completely uninstalled

  triggerAugSearchFullReindex:
    # Parameters:
    #   asynchronous: true to not wait for the end of the fullReindex, false otherwise. Default true
    cmd[proc]: |-
        __secret__API_TOKEN="${globals.__secret__pat_token}"
        curl -XPOST \
          http://localhost:8080/modules/graphql \
          -H "Authorization: APIToken $__secret__API_TOKEN" \
          -H "Content-Type: application/json" \
          -H "Origin: http://localhost:8080" \
          -d '{"query":"mutation {admin {search {startIndex {jobs {id status project {siteKey} } } } } } "}'

        if [ "${this.asynchronous:true}" == "true" ]; then
          exit 0
        fi

        check_indexation_state() {
          res=$(curl -s -XPOST \
          http://localhost:8080/modules/graphql \
          -H "Authorization: APIToken $__secret__API_TOKEN" \
          -H 'Content-Type: application/json' \
          -H 'Origin: http://localhost:8080' \
          -d '{"query":"{admin {search {listSites {sites {indexationStatus siteKey} } } } } "}')
          echo $res | jq -r '.data.admin.search.listSites.sites | .[] | select(.indexationStatus!="COMPLETED")'
        }

        check=$(check_indexation_state)
        while [ ! -z "$check" ]; do
          sleep 30
          check=$(check_indexation_state)

          # As the operation can take several hours, it's impossible to set a timeout value
          # The manual and "clean" way to abort the package is to create the /tmp/stopWaitingForReindexEnd file.
          if [ -f "/tmp/stopWaitingForReindexEnd" ]; then
            echo "Force quit"
            exit 1
          fi
        done

  setJahiaUserFeedbacksConfig:
    - getJahiaVersion
    - isVersionHigherOrEqual:
        a: ${globals.jahiaVersion}
        b: 8.1.0.0
        res: jahia81plus
    - if (${globals.jahia81plus}):
        - script: |-
            const envName = "${env.envName}";
            orgName = jelastic.env.control.GetNodeGroups(envName, session).object.filter(function (object) {
                                        return object.name == "cp";}).pop().ORGANIZATION_NAME;
            return {
              "result": 0,
              "onAfterReturn": {
                "set": {
                  "organizationName": orgName
                }
              }
            }
        - cmd[proc]: |-
            config_file="/data/digital-factory-data/karaf/etc/org.jahia.services.env.cfg"
            organization_name="${this.organizationName}"
            env_name="${env.envName}"
            if [ ! -f $config_file ]; then
              touch $config_file
              echo "org.jahia.services.env.organization=$organization_name" >> $config_file
              echo "org.jahia.services.env.env_name=$env_name" >> $config_file
            else
              if ( ! grep -q "organization=$organization_name" $config_file ) || ( grep -qc "organization=$organization_name" $config_file | awk -F: '$NF+0 > 1' ); then
                sed -i '/.*organization=.*/d' $config_file
                echo "org.jahia.services.env.organization=$organization_name" >> $config_file
              fi
              if ( ! grep -q "env_name=$env_name" $config_file ) || ( grep -qc "env_name=$env_name" $config_file | awk -F: '$NF+0 > 1' ); then
                sed -i '/.*env_name=.*/d' $config_file
                echo "org.jahia.services.env.env_name=$env_name" >> $config_file
              fi
            fi
            chown tomcat:tomcat $config_file

  generateNodesListFile:
    - set:
        nodesList: "processing.${nodes.proc.first.id}"
    - forEach(nodes.cp):
        - set:
            nodesList: browsing.${@i.id} ${this.nodesList}
    - cmd[proc]: |-
        echo "${this.nodesList}" > /nodesList

  ####################
  # proxysql related #
  ####################
  setupProxysqlCluster:
    - cmd[cp, proc]: |-
          ${globals.proxysql_cli} -e "DELETE FROM proxysql_servers"
    - foreach (nodes.cp):
        - cmd[cp, proc]: |-
            ${globals.proxysql_cli} -e "INSERT INTO proxysql_servers (hostname,port,weight,comment) VALUES ('node${@i.id}-${env.domain}',6032,0,'browsing_$((${@}+1))')"
    - cmd [cp, proc]: |-
        ${globals.proxysql_cli} -e "INSERT INTO proxysql_servers (hostname,port,weight,comment) VALUES ('node${nodes.proc.first.id}-${env.domain}',6032,0,'processing')"
        ${globals.proxysql_cli} -e "LOAD PROXYSQL SERVERS TO RUNTIME"
        ${globals.proxysql_cli} -e "SAVE PROXYSQL SERVERS TO DISK"
    - cmd [cp, proc]: |-
        i=60
        sql="select count(*) from stats_proxysql_servers_metrics where Uptime_s = 0"
        while ! sleep 1 && ${globals.proxysql_cli} -e "$sql" | grep -s 0; do
          ((i--))
          if [ $i -eq 0 ]; then
            echo "[ERROR] ProxySQL cluster is not healthy" 1>&2
            exit 1
          fi
        done
    - if (nodes.sqldb.length == 1):
        configureProxysqlForSingleDBNode

  configureProxysqlForSingleDBNode:
    cmd[cp, proc]: |-
      ${globals.proxysql_cli} -e "DELETE FROM mysql_servers WHERE hostgroup_id!=2;"
      ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections) VALUES (2,'galera_1',3306, 100)"
      ${globals.proxysql_cli} -e "UPDATE mysql_galera_hostgroups SET active=0;"
      ${globals.proxysql_cli} -e "LOAD MYSQL SERVERS TO RUNTIME;"
      ${globals.proxysql_cli} -e "SAVE MYSQL SERVERS TO DISK;"

  setupMysqlServers:
    - cmd[proc]: |-
        ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections) VALUES (2,'galera_1',3306, 100)"
    - if (nodes.sqldb.length > 1):
        - cmd[proc]: |-
            ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections) VALUES (2,'galera_2',3306, 100)"
            ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections, weight) VALUES (2,'galera_3',3306, 100, 1000)"
    - cmd[proc]: |-
        ${globals.proxysql_cli} -e "LOAD MYSQL SERVERS TO RUNTIME"
        ${globals.proxysql_cli} -e "SAVE MYSQL SERVERS TO DISK"

  setupMonitorUser:
    # Parameters:
    # __secret__password: proxysql monitor user password
    - cmd[sqldb]: |-
        __secret__password="${this.__secret__password}"
        mysql -e "CREATE USER IF NOT EXISTS 'proxysql'@'%' IDENTIFIED BY '$__secret__password'"
        mysql -e "GRANT SELECT on sys.* TO 'proxysql'@'%'"
        mysql -e "GRANT SELECT on performance_schema.* TO 'proxysql'@'%'"
        mysql -e "GRANT  PROCESS, REPLICATION CLIENT ON *.* TO 'proxysql'@'%'"

  enableBackendMonitor:
    - cmd[cp, proc]: |-
        ${globals.proxysql_cli} -e "UPDATE global_variables SET variable_value='true' WHERE variable_name='mysql-monitor_enabled'"
        ${globals.proxysql_cli} -e "LOAD MYSQL VARIABLES TO RUNTIME"
        ${globals.proxysql_cli} -e "SAVE MYSQL VARIABLES TO DISK"

  finishProxysqlInstall:
    # Parameters:
    # __secret__monitor_password: proxysql monitor user password
    - updateProxySQLAdminPassword:
        target: "proc, cp"
    - setProxysqlCredsInDatadogConfig:
        target: "proc, cp"

    - setupMonitorUser:
        __secret__password: ${this.__secret__monitor_password}

    - enableBackendMonitor
    - if (nodes.sqldb.length > 1):
        - setupMysqlServers
    - else:
        - disableDatadogCustomChecks
    - setupProxysqlCluster

  updateProxySQLAdminPassword:
    # Parameters
    # target: nodes/nodesGroup to run the script on
    cmd[${this.target}]: |-
      admin_creds_query="set admin-admin_credentials='admin:$PROXYSQL_ADMIN_PASSWORD;cluster1:$PROXYSQL_CLUSTER_PASSWORD';"
      cluster_creds_query="set admin-cluster_password='$PROXYSQL_CLUSTER_PASSWORD';"
      monitor_creds_query="set mysql-monitor_password='$PROXYSQL_MONITORING_PASSWORD';"
      persist_config_query="LOAD ADMIN VARIABLES TO RUNTIME; LOAD MYSQL VARIABLES TO RUNTIME;"
      mysql -h 127.0.0.1 -uadmin -padmin -P6032 -e "$admin_creds_query $cluster_creds_query $monitor_creds_query $persist_config_query" || exit 1
      ${globals.proxysql_cli} -e "SAVE ADMIN VARIABLES TO DISK" || exit 1
      ${globals.proxysql_cli} -e "SAVE MYSQL VARIABLES TO DISK" || exit 1

  setProxysqlCredsInDatadogConfig:
    # Parameters
    # target: nodes/nodesGroup to run the script on
    cmd[${this.target}]: |-
      sed -i "s/password: admin/password: $PROXYSQL_ADMIN_PASSWORD/" /etc/datadog-agent/conf.d/proxysql.d/custom_checks
      sed -i "s/password: admin/password: $PROXYSQL_ADMIN_PASSWORD/" /etc/datadog-agent/conf.d/proxysql.d/conf.yaml

  refreshProxysqlInstancesList:
    - setupProxysqlCluster

  proxysqlSetMariadbBackendStatus:
    - cmd[cp, proc]: |-
        ${globals.proxysql_cli} -e "UPDATE mysql_servers SET status='${this.newStatus}' WHERE hostname='${this.targetHost}';"

  getGaleraMaster:
    # Return:
    #   - galeraMasterIndex
    - cmd[proc]: |-
        ${globals.proxysql_cli} -BNe "select DISTINCT hostname from runtime_mysql_servers where weight = 1000"
    - setGlobals:
        - galeraMasterIndex: ${response.out}

  proxysqlSwitchMaster:
    # Parameter:
    #   - target: galera number (1,2,3)
    - cmd[proc, cp]: |-
        # Check that the future master node is online
        res=$(${globals.proxysql_cli} -BNe "select * from runtime_mysql_servers where hostname='galera_${this.target}' and hostgroup_id=2 and status='ONLINE';")
        if [ -z "$res" ];then
          echo "The new masterNode is not online in proxySQL configuration" >> /var/log/jelastic-packages/proxySqlSwitchMaster.log
          exit 0
        fi

    - cmd[proc, cp]: |-
        ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections, weight) VALUES (2,'galera_1',3306, 100, 1);"
        ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections, weight) VALUES (2,'galera_2',3306, 100, 1);"
        ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections, weight) VALUES (2,'galera_3',3306, 100, 1);"
        ${globals.proxysql_cli} -e "update mysql_servers set weight=1000 where hostname='galera_${this.target}';"
        ${globals.proxysql_cli} -e "LOAD MYSQL SERVERS TO RUNTIME;"
        ${globals.proxysql_cli} -e "SAVE MYSQL SERVERS TO DISK"
        sleep 15

    - cmd[proc, cp]: |-
        # Check that there is only a single node with a weight of 1000
        res=$(${globals.proxysql_cli} -BNe "select * from runtime_mysql_servers where weight=1000 group by hostname;" | wc -l)
        if [ $res -lt 1 ];then
          echo "There is no master node with a weight of 1000 in proxysql runtime configuration" >> /var/log/jelastic-packages/proxySqlSwitchMaster.log
          exit 1
        elif [ $res -gt 1 ]; then
          echo "There is more than one master node with a weight of 1000 in proxysql runtime configuration" >> /var/log/jelastic-packages/proxySqlSwitchMaster.log
          exit 1
        fi

    - cmd[sqldb]: |-
        my_ip=$(grep $(hostname) /etc/hosts | awk '{print $1}')
        my_node_name_index=$(awk -v my_ip="$my_ip galera" '$0 ~my_ip {print $2}' /etc/hosts)
        if [[ "$my_node_name_index" == "galera_${this.target}" ]]; then
          exit 0
        fi
        timeout=1200 # wait for 1200s
        tries=0
        request="select COUNT(*) from INFORMATION_SCHEMA.PROCESSLIST where db='jahia' and user like 'jahia-db-%'"
        while [[ $(mysql -BNe "$request") -gt 0 ]]; do
          if [[ $tries -ge $timeout ]]; then
            echo "There are still open connections on $my_node_name_index"
            exit 1
          fi
          tries=`expr $tries + 10`
          sleep 10
        done

  disableDatadogCustomChecks:
    - cmd[cp, proc]: |-
        p="/etc/datadog-agent/conf.d"
        for check in proxysql_backend_missing proxysql_connections_discrepancies; do
          [ -h $p/${check}.yaml ] && mv $p/${check}.yaml $p/${check}.yaml_disabled
        done
        if systemctl -q is-active datadog-agent; then
          systemctl restart datadog-agent
        fi
        # In case the agent is not started yet (after a redeploy for example)
        # we don't want the script to fail since the agent will be configured and
        # started later on anyway
        exit 0

  procRedeploy:
    # Parameters:
    #   - upgradeJahia: true or false
    #   - targetDockerTag: jahia version string (eg: 8.0.2.0)
    - set:
        useExistingVolumes: true
    # Do not keep volumes in the case of an upgrade
    - if (${this.upgradeJahia}):
        - set:
            useExistingVolumes: false
    - cmd [proc]:
        - echo 'export JAHIA_UPGRADE="${this.upgradeJahia}"' >> /etc/locale.conf
    - api: environment.control.RedeployContainersByGroup
      nodeGroup: proc
      tag: ${this.targetDockerTag}
      useExistingVolumes: ${this.useExistingVolumes}
      skipReinstall: false
      envName: ${env.envName}
    # restore-module-state is not compatible with rolling redeploy
    - if (${this.upgradeJahia}):
        - cmd [proc]: |-
            rm -fr /data/digital-factory-data/bundles-deployed/*
            sudo -u tomcat touch "/data/digital-factory-data/[persisted-bundles].dorestore"
            echo "restore-module-state have been asked"
            ls -l /data/digital-factory-data/*.dorestore
            touch /data/digital-factory-data/modules/*
            service tomcat start
        - setJahiaUserFeedbacksConfig
    - cmd [proc]:
        - sed '/JAHIA_UPGRADE/d' -i /etc/locale.conf
    - startupJahiaHealthCheck: ${nodes.proc.first.id}

  browsingNodesBulkRedeploy:
    # Parameters:
    #   - targetDockerTag: jahia version string (eg: 8.0.2.0)
    - api: environment.control.RedeployContainersByGroup
      nodeGroup: cp
      tag: ${this.targetDockerTag}
      useExistingVolumes: false
      skipReinstall: false
      envName: ${env.envName}

  browsingNodesRollingRedeploy:
    # Parameters:
    #   - upgradeJahia: true or false
    #   - targetDockerTag: jahia version string (eg: 8.0.2.0)
    - set:
        useExistingVolumes: true
    # Do not keep volumes in the case of an upgrade
    - if (${this.upgradeJahia}):
        - set:
            useExistingVolumes: false
    - forEach (nodes.cp):
        - api: environment.control.RedeployContainerById
          nodeId: ${@i.id}
          tag: ${this.targetDockerTag}
          useExistingVolumes: ${this.useExistingVolumes}
          skipReinstall: false
          envName: ${env.envName}

  cleanJRLocalRevisionsTable:
    - cmd [${nodes.sqldb.first.id}]: |-
        mysql jahia -Nse "SELECT revision_id FROM JR_J_LOCAL_REVISIONS WHERE journal_id LIKE 'processing.%'"
        mysql jahia -e "TRUNCATE JR_J_LOCAL_REVISIONS"
    - set:
        currentRevision: ${response.out}
    - setEnvNodesAndRevision:
        target: proc
        currentRevision: ${this.currentRevision}
    - forEach(nodes.cp):
        setEnvNodesAndRevision:
          target: ${@i.id}
          currentRevision: ${this.currentRevision}
    - cleanJRJJournalTable:
        batchSize: 100000


  cleanJRJJournalTable:
    # Parameters:
    #   - batchSize: the number of lines to delete per batch
    - cmd [${nodes.sqldb.first.id}]: |-
        batch_size=${this.batchSize}
        query="SELECT MIN(REVISION_ID) FROM JR_J_LOCAL_REVISIONS;"
        min_revision_number=$(mysql jahia -sN -e "$query")
        if [ $? -ne 0 ]; then
          echo "Can't get the minimum revision number, aborting"
          exit 1
        fi
        query="DELETE FROM JR_J_JOURNAL WHERE REVISION_ID < $min_revision_number ORDER BY REVISION_ID LIMIT $batch_size"
        echo "Deleting rows with revision_id < $min_revision_number in table JR_J_JOURNAL. Batch size: $batch_size"
        n=1
        while true; do
          result=$(mysql jahia -vv -e "$query")
          if [ $? -ne 0 ]; then
            echo "Error when trying to delete the rows, aborting"
            exit 1
          fi
          if echo $result | grep -q " 0 rows affected"; then
            exit 0
          fi
          echo "Batch #$n executed"
          ((n+=1))
          sleep 2
        done

  setEnvNodesAndRevision:
    # Parameters:
    #   - target: target node group or id
    #   - currentRevision: Current Revision number
    - cmd [${this.target}]: awk '$1=="cluster.node.serverId" {print $NF; exit}' /opt/tomcat/conf/digital-factory-config/jahia/jahia.node.properties
    - cmd [${nodes.sqldb.first.id}]: |-
        node_name=${response.out}
        current_revision=${this.currentRevision}
        mysql jahia -e "INSERT INTO JR_J_LOCAL_REVISIONS values ('$node_name',$current_revision)"

  setupPat:
    # Parameters:
    # jahia_version: the jahia version
    - generatePatAndKey
    - savePatInVault:
        __secret__pat_token: ${globals.__secret__pat_token}
        __secret__pat_key: ${globals.__secret__pat_key}
    - setPatInJahia:
        __secret__pat_token: ${globals.__secret__pat_token}
        jahia_version: ${this.jahiaVersion}
    - setPatInHaproxy:
        __secret__pat_token: ${globals.__secret__pat_token}

  generatePatAndKey:
    - cmd[proc]: |-
        graph_token=$(cat /dev/urandom | tr -dc '[[:graph:]]' | fold -w 32 | head -n 1)
        token=$(echo -n "$graph_token" | base64)
        key=$(echo -n "$graph_token" | awk '{printf substr($1,0,16)}' | od -A n -t x1 | awk '{printf "%s%s%s%s-%s%s-%s%s-%s%s-%s%s%s%s%s%s",$1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16}')
        printf '{"HideThisOutput": true, "token": "%s", "key": "%s"}' "$token" "$key"

    - script: |-
        __secret__response = ${response.out};
        token = __secret__response["token"];
        key = __secret__response["key"];

        return {
          "result": 0,
          "onAfterReturn": {
            setGlobals: {
              "__secret__pat_token": token,
              "__secret__pat_key": key,
            }
          }
        };

  savePatInVault:
    # Parameters:
    #  - __secret__pat_token: PAT token
    #  - __secret__pat_key: PAT key
    - getVaultData
    - vaultSecretSet:
        secretPath: "paas/customers/${globals.organizationName}/paas_${env.shortdomain}/PAT/root"
        __secret__secretData: '{"token":"${this.__secret__pat_token}","key":"${this.__secret__pat_key}"}'

  setPatInJahia:
    # Parameters:
    # __secret__pat_token: the personal access token
    # jahia_version: the jahia version
    - cmd[proc]: |-
        __secret__pat_token=${this.__secret__pat_token}
        groovy_file_path=/data/digital-factory-data/patches/groovy/pat.groovy

        # Clean up any possible remainder of previous script execution
        rm -f ${groovy_file_path}*

        echo """
        org.jahia.services.content.JCRTemplate.getInstance().doExecuteWithSystemSession({ session ->
          org.jahia.osgi.BundleUtils.getOsgiService(\"org.jahia.modules.apitokens.TokenService\")
              .tokenBuilder(\"/users/root\", \"jahia-cloud-token_admin_${env.envName}\", session)
              .setToken(\"$__secret__pat_token\")
              .setActive(true)
              .create()
          session.save();
        })
        """ >> $groovy_file_path
        chown tomcat:tomcat $groovy_file_path

  setPatInHaproxy:
    # Parameters:
    # __secret__pat_token: the personal access token
    - script: |-
        __secret__patToken = "${this.__secret__pat_token}";
        return jelastic.env.control.AddContainerEnvVars(
          '${env.envName}',
          session,
          nodeGroup='bl',
          vars={"jahia_cfg_healthcheck_token": __secret__patToken}
        );

  getPatTokenAndKey:
    # set token value in __secret__pat_token and key value in __secret__pat_key if not already set
    - if ("HideThisLine" && "${globals.__secret__pat_token.print()}" == "" || "${globals.__secret__pat_key.print()}" == ""):
        - getVaultData
        - vaultSecretReadAllKeysB64:
            secretPath: "paas/customers/${globals.organizationName}/paas_${env.shortdomain}/PAT/root"
        - script: |-
            __secret__pat_creds_base64 = "${globals.__secret__vaultSecretData}";
            pat_creds = JSON.parse(java.lang.String(java.util.Base64.getDecoder().decode(__secret__pat_creds_base64)))
            return {
              "result": 0,
              "onAfterReturn": {
                setGlobals: {
                  "__secret__pat_token": pat_creds["token"],
                  "__secret__pat_key": pat_creds["key"]
                }
              }
            };

  checkPatGroovyScriptExecution:
    - cmd[proc]: |-
        groovy_file_path=/data/digital-factory-data/patches/groovy/pat.groovy
        jahia_running_timeout=360 # 6 minutes
        sleep_interval=5

        while [ -f $groovy_file_path ]; do
          sleep $sleep_interval;
          ((jahia_running_timeout-=sleep_interval))
          if [ $jahia_running_timeout -eq 0 ]; then
            echo "[ERROR] $groovy_file_path is still not executed after 360 seconds"
            exit 1
          fi
        done

        rm -f $groovy_file_path.installed

        if [ -f $groovy_file_path.failed ]; then
            echo "[ERROR] pat.groovy execution failed"
            exit 1
        fi

  isAugSearchEnabled:
    # Returns:
    #   ${globals.isAugSearchEnabled}: true if augmented search is enabled, false otherwise
    - script: |-
        const augsearch = jelastic.env.control.GetNodeGroups("${env.envName}", session).object.filter(function (object) {
            return object.name == "cp";
        }).pop().augsearch;

        resp = {"result": 0}
        resp.onAfterReturn = {
          setGlobals: {
            isAugSearchEnabled: (augsearch != null)
          }
        }

        return resp

  getAugSearchConnectionName:
    # Returns:
    #   - globals.augSearchConnectionName : contains the augmented search connection name configured, empty if there is nothing configured
    - getPatTokenAndKey
    - cmd[proc]: |-
        __secret__API_TOKEN="${globals.__secret__pat_token}"
        get_current_as_connection() {
          curl -fLSs http://localhost:8080/modules/graphql \
            -H "Authorization: APIToken $__secret__API_TOKEN" \
            -H 'Origin: http://localhost:8080' \
            -H 'Content-Type: application/json' \
            -d '{"query":"query { admin { search { currentConnection } } }"}' \
            | jq -r '.data.admin.search.currentConnection'
        }
        current_connection=$(get_current_as_connection)
        if [ "$current_connection" != "null" ]; then
          echo $current_connection
        fi

    - setGlobals:
        augSearchConnectionName: ${response.out}

  removeDefaultESConnection:
    # Delete the jahia-cloud_augmented-search connexion from elasticsearch-connector module
    - cmd[proc]: |-
        __secret__API_TOKEN="${globals.__secret__pat_token}"

        es_connection_name="jahia-cloud_augmented-search"
        result=$(curl -fLSs -XDELETE \
          -H "Authorization: APIToken $__secret__API_TOKEN" \
          -H 'Origin: http://localhost:8080' \
          -H 'Content-Type: application/json' \
          http://localhost:8080/modules/dbconn/elasticsearch/remove/$es_connection_name | jq -r ".success" )
        if [ "$result" != "Successfully removed ElasticSearch connection" ]; then
          echo "Failed to remove the old connection but may be normal, so we can continue"
        fi


  setDefaultESConnection:
    # Set the jahia-cloud_augmented-search connexion from elasticsearch-connector module
    - getPapiInfoAll
    - cmd[proc]: |-
        __secret__API_TOKEN="${globals.__secret__pat_token}"

        __secret__PAPI_TOKEN="${globals.__secret__papiToken}"
        export PAPI_TOKEN="$__secret__PAPI_TOKEN"
        export PAPI_HOSTNAME="${globals.papiHostname}"
        export PAPI_ENV_ID="${globals.papiEnvId}"
        export PAPI_API_VERSION="${globals.papiApiVersion}"

        environment=$(papi.py -X GET "paas-environment/$PAPI_ENV_ID")
        ec_deployment_id=$(echo $environment | jq -r .ec_deployment_id)
        ec_deployment=$(papi.py -X GET "ec-deployment/$ec_deployment_id")

        es_endpoint=$(echo $ec_deployment | jq -r .es_endpoint | sed 's/https:\/\/\(.*\):.*/\1/g')

        es_connection_name="jahia-cloud_augmented-search"

        __secret__elasticsearch_password="${globals.__secret__elasticsearch_password}"

        result=$(curl -fLSs -XPOST \
          -H 'Authorization: APIToken $__secret__API_TOKEN' \
          -H 'Origin: http://localhost:8080' \
          -H 'Content-Type: application/json' \
          -d "{\"id\": \"$es_connection_name\", \"isConnected\": true, \"host\": \"$es_endpoint\", \"port\": 443, \"user\": \"${env.envName}\", \"password\": \"$__secret__elasticsearch_password\", \"options\": {\"useXPackSecurity\": true, \"useEncryption\": true} }" \
          http://localhost:8080/modules/dbconn/elasticsearch/add
        )
        success=$(echo $result | jq -r .success)
        verified=$(echo $result | jq -r .connectionVerified)
        if [ "$success" != "Connection successfully added" ]; then
          echo "Failed to add the connection"
          exit 1
        elif [ "$verified" != "true" ]; then
          echo "The new connection could not be verified"
          exit 1
        fi

  setAugSearchESConnection:
    - getAugSearchConnectionName
    - if ("${globals.augSearchConnectionName}" != "jahia-cloud_augmented-search"):
        cmd[proc]: |-
          __secret__API_TOKEN="${globals.__secret__pat_token}"

          es_connection_name="jahia-cloud_augmented-search"

          response=$(curl -fLSs -XPOST \
            -H "Authorization: APIToken $__secret__API_TOKEN" \
            -H 'Origin: http://localhost:8080' \
            -H 'Content-Type: application/json' \
            -d '{"query":"mutation { admin { search { setDbConnection(connectionId:\"jahia-cloud_augmented-search\") } } }"}' \
            http://localhost:8080/modules/graphql)
            result=$(echo $response | jq -r '.data.admin.search.setDbConnection')
          if [ "$result" != "Successful" ]; then
            echo "Failed to use the $es_connection_name connection for AS: $response"
            exit 1
          fi

  #######################
  # jexperience related #
  #######################
  getJexperienceVersion:
    # Get jExperience version to install according to jCustomer env version:
    # - jCustomer 1.x ==> jExperience 2.8.0 & jExperienceDashboards 0.3.0
    # - jCustomer 2.x ==> jExperience 3.3.0 & jExperienceDashboards 1.0.0
    # Parameters:
    #   unomi_env_name: name of jCustomer environment
    # Returns:
    #   ${globals.jexperienceVersion}: jExperience version to install according to current Jahia / jCustomer version
    #   ${globals.jexperienceDashboardsVersion}: jExperience-Dashboards version to install according to current Jahia / jCustomer version
    - api: env.control.GetContainerEnvVarsByGroup
      envName: ${this.unomi_env_name}
      nodeGroup: cp
    - set:
        unomiVersion: ${response.object.UNOMI_VERSION}
    - isVersionHigherOrEqual:
        a: ${this.unomiVersion}
        b: 2.0.0
        res: isHigherOrEqual200
    - if( ${globals.isHigherOrEqual200} ):
        - setGlobals:
            jexperienceVersion: 3.3.0
            jexperienceDashboardsVersion: 1.0.0
    - else:
        - setGlobals:
            jexperienceVersion: 2.8.0
            jexperienceDashboardsVersion: 0.3.0

  getEnvLinkedJcustomer:
    # Parameters:
    #   - envName: jahia env name to fetch info from
    - script: |
        const envName = "${this.envName}";

        unomi_linked = jelastic.env.control.GetNodeGroups(envName, session).object.filter(function (object) {
                                        return object.name == "cp";}).pop().envLink;

        return unomi_linked?
        {"result": 0, value: unomi_linked, "is_linked": true, "out": "Found a linked env"} :
        {"result": 0, value: "none", "is_linked": false, "out": "No unomi env linked"};
    - setGlobals:
        unomi_env_name: ${response.value}
        unomi_env_linked: ${response.is_linked}

  checkJexperienceCfg:
    # Parameters:
    #   - jcustomerDns: jahia env name to fetch info from
    #   - __secret__jcustomerPwdB64: Jcustomer password (base64 encoded)
    - cmd[proc]: |-
        CONFIG_FILE="/data/digital-factory-data/karaf/etc/org.jahia.modules.jexperience.settings-global.cfg"
        URL=$(grep jexperience.jCustomerURL $CONFIG_FILE | cut -d'=' -f2 | sed 's/ //g' |sed 's/https\?...//g')
        if [ "$URL" != "${this.jcustomerDns}" ]; then
          >&2 echo "error"
        fi
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "jCustomer url is wrong."
    - cmd[proc]: |-
        CONFIG_FILE="/data/digital-factory-data/karaf/etc/org.jahia.modules.jexperience.settings-global.cfg"
        PASSWD=$(sed -n 's/\( *jexperience.jCustomerPassword *= *\)\(.*\) *$/\2/p' $CONFIG_FILE | tr -d '\n' | base64)
        __secret__jcustomerPwdB64="${this.__secret__jcustomerPwdB64}"
        if [ "$PASSWD" != "$(echo -n $__secret__jcustomerPwdB64)" ]; then
          >&2 echo "error"
        fi
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "jCustomer password is wrong."

  removeAndCleanJexperience:
    - cmd[proc, cp]: |-
        JCUSTOMER_CONFIG_FILE="/data/digital-factory-data/karaf/etc/org.jahia.modules.jexperience.settings-global.cfg"
        if [ -f $JCUSTOMER_CONFIG_FILE ]; then
         sed -i 's/\(jexperience.jCustomer.*=\).*/\1/g' $JCUSTOMER_CONFIG_FILE
        fi
    - uninstallModule:
        moduleSymname: jexperience
    - environment.nodegroup.ApplyData [proc, cp]:
        data:
          envLink: null
    - removeKibanaAndJexperienceDashboards


  getKibanaEndpointOfJcustomer:
    # Parameters:
    # - jcustomerEnv
    # Returns:
    # - globals.kibanaEndpoint
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/jcustomer/get-kibana-endpoint.yml",
              envName: "${this.jcustomerEnv}"
            }
          );
    - setGlobals:
        kibanaEndpoint: "${response.successText.fromBase64()}"

  setupKibanaDashboard:
    # Parameters
    # roleAndAccountName: kibana role and es account name
    # __secret__password: es account password
    # jcustomerEnv: target jcustomer environment
    - setGlobalRepoRootUrl
    - set:
        es_permissions: '{"indices": [{"names": ["${this.jcustomerEnv}_*"], "privileges": ["read"]}]}'
        kibana_permissions: '[{"base": ["all"], "feature": {}, "spaces": ["${this.roleAndAccountName}"]}]'
    - installOrUpgradeModule:
        moduleSymname: kibana-dashboards-provider
        moduleVersion: 1.1.0
        moduleGroupId: org.jahia.modules
        moduleRepository: marketing-factory-releases
    - getJexperienceVersion:
        unomi_env_name: ${this.jcustomerEnv}
    - installOrUpgradeModule:
        moduleSymname: jexperience-dashboards
        moduleVersion: ${globals.jexperienceDashboardsVersion}
        moduleGroupId: org.jahia.modules
        moduleRepository: marketing-factory-releases
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/create-kibana-space.yml",
              envName: "${this.jcustomerEnv}",
              settings: {
                'spaceName': '${this.roleAndAccountName}',
              }
            }
          );
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/create-kibana-role.yml",
              envName: "${this.jcustomerEnv}",
              settings: {
                'roleName': '${this.roleAndAccountName}',
                'esPermissions': '${this.es_permissions}',
                'kibanaPermissions': '${this.kibana_permissions}'
              }
            }
          );
    - script: |-
        var __secret__password = "${this.__secret__password}";
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/create-elasticsearch-account.yml",
              envName: "${this.jcustomerEnv}",
              settings: {
                'accountName': '${this.roleAndAccountName}',
                'password': __secret__password,
                'rolesList': '${this.roleAndAccountName}',
              }
            }
          );
    - getKibanaEndpointOfJcustomer:
        jcustomerEnv: ${this.jcustomerEnv}
    - cmd[proc]: |-
        __secret__password="${this.__secret__password}"
        cfg_file=/data/digital-factory-data/karaf/etc/org.jahia.modules.kibana_dashboards_provider.cfg
        sed -i "s,.*\(kibana_dashboards_provider.kibanaURL\).*,\1=${globals.kibanaEndpoint},g" $cfg_file
        sed -i 's/.*\(kibana_dashboards_provider.kibanaUser\).*/\1=${this.roleAndAccountName}/g' $cfg_file
        sed -i "s/.*\(kibana_dashboards_provider.kibanaPassword\).*/\1=$__secret__password/g" $cfg_file
        sed -i 's/.*\(kibana_dashboards_provider.kibanaSpace\).*/\1=${this.roleAndAccountName}/g' $cfg_file
        sed -i 's/.*\(kibana_dashboards_provider.kibanaProxy.enable\).*/\1=true/g' $cfg_file
        sed -i 's/.*\(kibana_dashboards_provider.kibanaProxy.cloud\).*/\1=true/g' $cfg_file

  createKibanaCustomerUserAccount:
    # Parameters:
    # - accountName: name of the customer's Kibana user
    # - rolesList: comma-separated list of Kibana roles
    # - __secret__password: password of the customer's Kibana user
    # - jcustomerEnv: target jcustomer environment
  - setGlobalRepoRootUrl
  - script: |-
      var __secret__password = "${this.__secret__password}";
      return api.marketplace.jps.Install(
          {
            jps: "${globals.repoRootUrl}/packages/common/create-elasticsearch-account.yml",
            envName: "${this.jcustomerEnv}",
            settings: {
              'accountName': '${this.accountName}',
              'password': __secret__password,
              'rolesList': '${this.rolesList}',
            }
          }
        );

  removeKibanaAndJexperienceDashboards:
    - cmd [proc]: |-
        cfg_file=/data/digital-factory-data/karaf/etc/org.jahia.modules.kibana_dashboards_provider.cfg
        if [ -f $cfg_file ]; then
          sed -i "s,.*\(kibana_dashboards_provider.\)\(kibanaURL\|kibanaUser\|kibanaPassword\|kibanaSpace\).*,\1\2=,g" $cfg_file
          sed -E 's:(^\s*kibana_dashboards_provider\.)(kibana(URL|User|Password|Space)).*:\1\2=:' -i $cfg_file
        fi
    - uninstallModule:
        moduleSymname: kibana-dashboards-provider
    - uninstallModule:
        moduleSymname: jexperience-dashboards

### Vault related actions ###
  vaultGetIPsecConfB64:
    # Returns:
    #   ${globals.__secret__IPsecConfB64}: the IPsec conf, base64 encoded
    - getVaultData
    - vaultSecretReadKeyB64:
        secretPath: paas/customers/${globals.organizationName}/paas_${env.shortdomain}/ipsec/conn-1
        secretKey: conf
    - setGlobals:
        __secret__IPsecConfB64: ${globals.__secret__vaultSecretData}

  vaultGetIPsecPSKB64:
    # Returns:
    #   ${globals.__secret__IPsecPSKB64}: the PSK, base64 encoded
    - getVaultData
    - vaultSecretReadKeyB64:
        secretPath: paas/customers/${globals.organizationName}/paas_${env.shortdomain}/ipsec/conn-1
        secretKey: secret
    - setGlobals:
        __secret__IPsecPSKB64: ${globals.__secret__vaultSecretData}

  getNexusCredentials:
    # Returns:
    #   ${globals.__secret__nexusLogin}: Nexus login
    #   ${globals.__secret__nexusPassword}: Nexus password
    - getVaultData
    - vaultSecretReadAllKeysB64:
        secretPath: "paas/envs-common/nexus"
    - script: |-
        __secret__vaultResponseBase64 = "${globals.__secret__vaultSecretData}";
        vaultResponse = JSON.parse(java.lang.String(java.util.Base64.getDecoder().decode(__secret__vaultResponseBase64)))
        return {
          "result": 0,
          "onAfterReturn": {
            setGlobals: {
              "__secret__nexusLogin": vaultResponse.login,
              "__secret__nexusPassword": vaultResponse.password
            }
          }
        };

  queryShutdownEndpoint:
    # Parameters:
    # - target: target node id
    - getPatTokenAndKey
    - cmd[${this.target}]: |-
        result=$(curl -SsH "Authorization: APIToken ${globals.__secret__pat_token}" \
            -X POST \
            "http://localhost:8080/modules/graphql" \
            -H 'Content-Type: application/json' \
            -H "Origin: http://localhost:8080" \
            -d '{"query":"mutation { admin { jahia { shutdown( dryRun: true timeout: 120 force: false ) } } }"}' \
            | jq -r .data.admin.jahia.shutdown)
        echo $result
    - if ("${response.errOut}" != ""):
        - log: "Unable to query SAM shutdown endpoint before stopping the node."
    - elif ("${response.out}" != "true"):
        - cmd[${this.target}]: |-
            tasklist=$(curl -SsH "Authorization: APIToken ${globals.__secret__pat_token}" \
                  -X POST \
                  "http://localhost:8080/modules/graphql" \
                  -H 'Content-Type: application/json' \
                  -H "Origin: http://localhost:8080" \
                  -d '{"query":"query { admin { jahia { tasks { name, service, started } } } }"}' \
                  | jq -r '.data.admin.jahia.tasks | map(.name) | join(", ")' )
            echo $tasklist
        - if ("${response.errOut}" != ""):
            - log: "Unable to list running tasks before stopping."
        - else:
            - log: "The node was stopped while these tasks were running: ${response.out}"

  setupDatadogAgentStorage:
    - log: "## Finalize Datadog agent setup on ${this}"
    - setGlobalRepoRootUrl
    - installLatestDatadogAgent: ${this}
    - cmd[${this}]: |-
        NODE_NAME=${HOSTNAME/-*}
        echo "hostname: $(echo $_ROLE| sed 's/_//g').${NODE_NAME#node}" >> /etc/datadog-agent/datadog.yaml
        sed -i 's/# logs_enabled: false/logs_enabled: true/' /etc/datadog-agent/datadog.yaml
        echo "tags:" >> /etc/datadog-agent/datadog.yaml
        echo " - product:jahia" >> /etc/datadog-agent/datadog.yaml
        echo " - version:${DX_VERSION}" >> /etc/datadog-agent/datadog.yaml
        echo " - envname:${env.envName}" >> /etc/datadog-agent/datadog.yaml
        echo " - provide:${_PROVIDE}" >> /etc/datadog-agent/datadog.yaml
        echo " - role:${_ROLE}" >> /etc/datadog-agent/datadog.yaml
        curl -fLSso /usr/local/bin/set_dd_tags.sh ${globals.repoRootUrl}/assets/common/set_dd_tags.sh || exit 1
        curl -fLSso /etc/cron.d/set_dd_tags_cron ${globals.repoRootUrl}/assets/common/set_dd_tags_cron || exit 1
        chmod u+x /usr/local/bin/set_dd_tags.sh
    - if (nodes.sqldb.length > 1):
        - cmd[${this}]: |-
            echo "dd-agent ALL=(ALL) NOPASSWD:/opt/datadog-agent/embedded/sbin/gstatus" > /etc/sudoers.d/dd-agent
            cat > /etc/datadog-agent/conf.d/glusterfs.d/conf.yaml << EOF
            init_config:
            instances:
              -
                min_collection_interval: 60
            logs:
              - type: file
                path: /var/log/glusterfs/glusterd.log
                source: glusterfs
                service: glusterfs
              - type: file
                path: /var/log/glusterfs/glustershd.log
                source: glusterfs
                service: glusterfs
              - type: file
                path: /var/log/glusterfs/data.log
                source: glusterfs
                service: glusterfs
            EOF
            log_dir=/var/log/glusterfs
            [ -d $log_dir ] || mkdir $log_dir
            setfacl -m u:dd-agent:rx /var/log/glusterfs
            setfacl -m u:dd-agent:rx /var/log/glusterfs/*.log
            sed -i '/killall.*glusterd/a \  touch \/var\/log\/glusterfs\/glusterd.log && /usr\/bin\/setfacl -m u:dd-agent:rx \/var\/log\/glusterfs\/*.log' /etc/logrotate.d/glusterfs
            systemctl restart rsyslog crond datadog-agent

  dumpModules:
    # Parameters:
    # - operation: type of operation performed
    # - checkVersion: if true, add versions to validation
    - enableKarafLogin: "proc, cp"
    - cmd [proc]: date +"%Y-%m-%dT%H:%M:%S"
    - set:
        nowDate: ${response.out}
    - if (${this.checkVersion}):
      - cmd[cp, proc]: |-
          ${globals.karafConsole} cluster:bundle-list -s default | awk -F ' ' 'NR > 3 { print $2, $6, $7 }' > /opt/tomcat/temp/modulesdump-cluster.${this.operation}.${this.nowDate}
          ${globals.karafConsole} bundle:list -s | awk -F ' ' 'NR > 3 { print $2, $4, $5 }' > /opt/tomcat/temp/modulesdump-local.${this.operation}.${this.nowDate}
    - else:
      - cmd[cp, proc]: |-
          ${globals.karafConsole} cluster:bundle-list -s default | awk -F ' ' 'NR > 3 { print $2, $7 }' > /opt/tomcat/temp/modulesdump-cluster.${this.operation}.${this.nowDate}
          ${globals.karafConsole} bundle:list -s | awk -F ' ' 'NR > 3 { print $2, $5 }' > /opt/tomcat/temp/modulesdump-local.${this.operation}.${this.nowDate}
    - cmd[cp, proc]: |-
        sed -i -E "s/Resolved|Installed/Stopped/" /opt/tomcat/temp/modulesdump-*.${this.operation}.${this.nowDate}
    - disableKarafLogin: "proc, cp"

  checkModulesAfterOperation:
    # Parameters:
    # - operation: type of operation performed
    # - checkVersion: if true, add versions to validation
    # - (optional) ignoredModules: list of ignored modules (ex: "module1,module2,module3")
    # Dump modules after operation
    - dumpModules:
        operation: ${this.operation}
        checkVersion: ${this.checkVersion}

    # Clean the dumps of ignored modules if any
    - if ("${this.ignoredModules:}" != ""):
        - cmd[cp, proc]: |-
            # Clean the dumps
            clusterdump_before=$(ls -t /opt/tomcat/temp/modulesdump-cluster* | sed -n '2 p')
            clusterdump_after=$(ls -t /opt/tomcat/temp/modulesdump-cluster* | sed -n '1 p')
            localdump_before=$(ls -t /opt/tomcat/temp/modulesdump-local* | sed -n '2 p')
            localdump_after=$(ls -t /opt/tomcat/temp/modulesdump-local* | sed -n '1 p')
            dumps=($clusterdump_before $clusterdump_after $localdump_before $localdump_after)
            for dump in ${dumps[@]}; do
              sed -i -E "/\s+($(echo "${this.ignoredModules}" | tr ',' '|'))$/d" "$dump"
            done

    # Validate before vs after and log any errors
    - cmd[cp, proc]: |-
        clusterdump_before=$(ls -t /opt/tomcat/temp/modulesdump-cluster.${this.operation}* | sed -n '2 p')
        clusterdump_after=$(ls -t /opt/tomcat/temp/modulesdump-cluster.${this.operation}* | sed -n '1 p')
        localdump_before=$(ls -t /opt/tomcat/temp/modulesdump-local.${this.operation}* | sed -n '2 p')
        localdump_after=$(ls -t /opt/tomcat/temp/modulesdump-local.${this.operation}* | sed -n '1 p')

        # If this is an upgrade, only keep active modules
        dumps_before=($clusterdump_before $localdump_before)
        if [[ ${this.operation} == "upgrade" ]]; then
            for dump in ${dumps_before[@]}; do
              sed -i '/Active/!d' "$dump"
            done
        fi

        # Make sure there's no space issues
        dumps=($clusterdump_before $clusterdump_after $localdump_before $localdump_after)
        for dump in ${dumps[@]}; do
          sed -i 's/  */ /g' "$dump"
        done

        result_file=$(mktemp)
        comm -23 <(sort $clusterdump_before) <(sort $clusterdump_after) | awk '{print $NF}' >> $result_file
        comm -23 <(sort $localdump_before) <(sort $localdump_after) | awk '{print $NF}' >> $result_file

        # Log error if any
        if [ -s "$result_file" ]; then
          logfile=/var/log/jelastic-packages/modulescheck.log
          timestamp=$(date +"%Y-%m-%dT%H:%M:%S")
          errormsg="$timestamp - Errors detected in module states after ${this.operation}. Please check the state of the following module(s): $(sort -u $result_file | paste -s -d,)"
          echo '{"level": "ERROR", "message": "'"$errormsg"'" }' >> $logfile
        fi

  callProvisioningAPI:
    # Parameters:
    #  - target: the node group/id that will perform the call
    #  - payload: the request payload. Can have many formats:
    #     - file path of a non-script (like .zip). form_type parameter must be set to "file"
    #     - file path of a script ex: '@install_jexperience.yaml'. script_format must be set unless it's yaml
    #     - yaml/json string  ex: '- karafCommand: "cluster:bundle-list -s default"' . script_format parameter must be set unless it's yaml
    #     - raw yaml. Example of parameter:
    #         payload:
    #           - installBundle:
    #               - "mvn:org.jahia.modules/addstuff/2.1.0"
    #             autostart: false
    #             uninstallPreviousVersion: false
    #  - script_format (optional): ex: yaml, json. If form_type=script, default value is set to yaml
    #  - form_type (optional): possible values: script, file. Default value is script
    #  - curl_extra_args (optional)
    # Returns
    #   provisioning_api_call_success - true if succeed, false otherwise
    - getPatTokenAndKey
    - if ( "${this.form_type:script}" == "script" ):
        - set:
            format: ";type=text/${this.script_format:yaml}"

    - cmd[${this.target}]: |-
        __secret__API_TOKEN="${globals.__secret__pat_token}"
        current_line_number=$(wc -l /opt/tomcat/logs/catalina.out |awk '{print $1}')
        http_code=$(curl -sS -o /tmp/output -w '%{http_code}' \
                    -H "authorization: APIToken $__secret__API_TOKEN" \
                    http://localhost/modules/api/provisioning ${this.curl_extra_args} \
                    --form ${this.form_type:script}='${this.payload}${this.format:}')
        if [ $http_code -lt 200 ] && [ $http_code -ge 299 ]; then
          >&2 echo "Curl command returned wrong http code $http_code"
          >&2 cat /tmp/output
          rm /tmp/output
        fi

        errors=$(tail -n +$current_line_number /opt/tomcat/logs/catalina.out | grep -e ProvisioningScriptUtil -e KarafCommand -e ProvisioningResource | grep ERROR)
        if [ -n "$errors" ]; then
          >&2 echo "The command returned an error: \n$errors"
          >&2 echo "Check catalina.out file from line $current_line_number"
        fi

    - if ("${response.errOut}" == ""):
        setGlobals:
          provisioning_api_call_success: true
    - else:
        setGlobals:
          provisioning_api_call_success: false
