---
# Depends on:
#   - common.yml

globals:
  proxysql_cli: "mysql -h 127.0.0.1 -uadmin -p$PROXYSQL_ADMIN_PASSWORD -P6032"
  org_jahia_ehcachemanager_maxBytesLocalHeap_dev: 700M
  org_jahia_ehcachemanager_big_maxBytesLocalHeap_dev: 700M
  org_jahia_ehcachemanager_maxBytesLocalHeap_prod: 800M
  org_jahia_ehcachemanager_big_maxBytesLocalHeap_prod_cp: 2500M
  org_jahia_ehcachemanager_big_maxBytesLocalHeap_prod_proc: 700M
  expandImportedFilesOnDisk: "true"
  jahiaFileUploadMaxSize: 268435456
  imageService: ImageMagickImageService
  jahiaImageMagickPath: /usr/bin
  java_opts:
    -DDB_USER=${DB_USER}
    -DDB_PASSWORD=${DB_PASSWORD}
    -DREDIS_PASSWORD=${REDIS_PASSWORD}
    -DMANAGER_USER=${MANAGER_USER}
    -DHOST_NAME=$(hostname)
    -Dcom.sun.management.jmxremote
    -Dcom.sun.management.jmxremote.port=7199
    -Dcom.sun.management.jmxremote.ssl=false
    -Dcom.sun.management.jmxremote.authenticate=false
    -XX:+UseG1GC
    -XX:+DisableExplicitGC
    -XX:-UseBiasedLocking
    -XX:+SafepointTimeout
    -XX:SafepointTimeoutDelay=1000
    -XX:+UnlockDiagnosticVMOptions
    -XX:GuaranteedSafepointInterval=0
    -XX:+ParallelRefProcEnabled
    -XX:+UseCountedLoopSafepoints
    -XX:+UseStringDeduplication
    -XX:LoopStripMiningIter=100
    -XX:MaxTenuringThreshold=7
    -XX:MaxMetaspaceSize=512m
    -XX:+HeapDumpOnOutOfMemoryError
    -XX:HeapDumpPath=${DUMPS_PATH}/heap_dumps
    -XX:+PrintConcurrentLocks
    -XX:NativeMemoryTracking=summary
    -XshowSettings:vm
    -Dkaraf.handle.sigterm=false
    -Xmn1G
    -Duser.timezone=${jahia_cfg_timezone}

actions:
  #################
  # jahia related #
  #################
  onAfterBrowsingScaleOut:
    - copyApp: ${this.newNode}
    - setupDatadogAgentJahia: ${this.newNode}
    - cmd[${this.newNode}]: |-
        if (service tomcat status); then
          echo "Now Restarting Tomcat"
          service tomcat restart
        else
          echo "Now Launching Tomcat"
          service tomcat start
        fi

  onAfterRedeployJahiaContainer:
    - cmd[${this}]:
        - service tomcat stop
    - getMavenPath
    - env.control.AddContainerEnvVars [cp, proc]:
      vars:
        jahia_cfg_mvnPath: ${globals.jahia_cfg_mvnPath}
    - getLogEventScript: ${this}
    - if (nodes.sqldb.length == 1):
        - disableDatadogCustomChecks
    - copyApp: ${this}
    - if ("${this}" == "cp"):
        cmd[${this}]:
          - sed -i "s#\(processingServer\s*=\).*#\1 false#g" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
    - setProxysqlCredsInDatadogConfig:
        target: "proc, cp"
    - setupDatadogAgentJahia: ${this}

    - getJahiaVersion
    - isVersionHigherOrEqual:
        a: ${globals.jahiaVersion}
        b: 8.2.0.0
        res: jahia8plus
    - if (${globals.jahia8plus}):
        cmd[${this}]: |-
          if ! grep -qE ACTIVE_JDK /opt/tomcat/bin/setenv.sh; then
            echo 'JDK_JAVA_OPTIONS="--add-opens=java.base/java.net=ALL-UNNAMED"' >> /opt/tomcat/bin/setenv.sh
            echo '[[ $ACTIVE_JDK == "graalvm" ]] && JDK_JAVA_OPTIONS="$JDK_JAVA_OPTIONS --add-opens=java.base/java.net=ALL-UNNAMED --add-exports org.graalvm.truffle/com.oracle.truffle.api.nodes=ALL-UNNAMED --add-exports org.graalvm.truffle/com.oracle.truffle.api.instrumentation=ALL-UNNAMED --add-exports org.graalvm.truffle/com.oracle.truffle.api.dsl=ALL-UNNAMED --add-exports org.graalvm.truffle/com.oracle.truffle.api=ALL-UNNAMED --add-exports org.graalvm.truffle/com.oracle.truffle.api.exception=ALL-UNNAMED --add-exports org.graalvm.truffle/com.oracle.truffle.api.frame=ALL-UNNAMED --add-exports org.graalvm.truffle/com.oracle.truffle.api.object=ALL-UNNAMED --add-exports org.graalvm.truffle/com.oracle.truffle.api.interop=ALL-UNNAMED --add-exports org.graalvm.truffle/com.oracle.truffle.api.strings=ALL-UNNAMED --add-exports org.graalvm.truffle/com.oracle.truffle.api.library=ALL-UNNAMED"' >> /opt/tomcat/bin/setenv.sh
          fi

    - cmd[${this}]: |-
        sudo -u tomcat touch "/data/digital-factory-data/[persisted-configurations].dorestore"
        source /etc/locale.conf
        echo "JAHIA_UPGRADE=$JAHIA_UPGRADE"
        if [ "$JAHIA_UPGRADE" == "true" ]; then
          echo "This is an upgrade, processing's tomcat will not be restarted now"
        else
          echo "This is a regular redeploy, restart tomcat now"
          sudo service tomcat start
        fi

    - script: |-
        ipsec_enabled = jelastic.env.control.GetNodeGroups("${env.envName}", session).object.filter(function (object) {
                          return object.name == "cp";
                        }).pop().ipsec
        return {"result": 0, "out": ipsec_enabled}

    - setGlobals:
        strongswanServiceStatus: ${response.out}

    - if ("${globals.strongswanServiceStatus}" == "enable"):
        - cmd[${this}]: |-
            systemctl enable strongswan.service
            systemctl start strongswan.service

  stopJahia:
    cmd[${this}]: "service tomcat stop"

  initJahiaDatabase:
    - log: "## Import DX schema in database"
    - cmd[${nodes.proc.first.id}]: cat $DATA_PATH/digital-factory-data/db/sql/schema/mysql/*.sql | mysql -h $DB_ENDPOINT -u$DB_USER -p$DB_PASSWORD -f jahia

  getMavenPath:
    - cmd [${nodes.proc.first.id}]: echo $(ls -d /opt/*maven*/bin/mvn)
    - setGlobals:
        jahia_cfg_mvnPath: ${response.out}

  installJahia:
    # Parameters:
    #   - jahiaVersion
    #   - __secret__rootpwd: Jahia root user password
    - getLogEventScript: proc, cp

    - environment.nodegroup.ApplyData [proc, cp]:
        data:
          productName: dx
          productVersion: ${this.jahiaVersion}
    - initJahiaDatabase
    - log: "## Determine JDK version for good _JAVA_OPTIONS envvar"
    - cmd[proc, cp]: |-
        tomcat_env=/opt/tomcat/conf/tomcat-env.sh
        case "$($JAVA_HOME/bin/java -version 2>&1 | awk 'NR==1 {print gensub("(\"|_.*)", "", "g", $3)}')" in
          1.8*)
              j_opts='${globals.java_opts}'
              ;;
          *)
              j_opts='${globals.java_opts} -Xlog:gc:file=/opt/tomcat/logs/gc.log:time,uptime,level,pid,tid,tags'
              ;;
        esac
        sed -e '2isource /etc/profile' -e "s#\(^JAVA_OPTS=.*\)\(\"$\)#\1 ${j_opts}\2#" -i $tomcat_env
        sed -i "s/jahia_cfg_timezone/jahia_cfg_timezone:-UTC/" $tomcat_env
    - setJahiaPropertiesEnvvars
    - copyApp: proc, cp
    - setJahiaUserFeedbacksConfig
    - cmd[proc]: |-
        __secret__rootpwd="${this.__secret__rootpwd.toBase64()}"
        base64 -d <<< $__secret__rootpwd > $DATA_PATH/digital-factory-data/root.pwd
        chown tomcat:tomcat $DATA_PATH/digital-factory-data/root.pwd
    - cmd [proc]: |-
        file=/opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-purge-jobs.xml
        mv ${file}-disabled $file
    - cmd[proc, cp]: |-
        systemctl restart proxysql
    - api: env.control.ExecDockerRunCmd
      nodeId: ${nodes.proc.first.id}
    - checkPatGroovyScriptExecution
    - startupJahiaHealthCheck: proc
    - env.control.ExecDockerRunCmd [${nodes.cp.join(id,)}]

  setJahiaPropertiesEnvvars:
    - log: "## Setting jahia.properties envvars"
    - getMavenPath
    - env.control.AddContainerEnvVars [cp, proc]:
      vars:
        jahia_cfg_expandImportedFilesOnDisk: ${globals.expandImportedFilesOnDisk}
        jahia_cfg_jahiaFileUploadMaxSize: ${globals.jahiaFileUploadMaxSize}
        jahia_cfg_imageService: ${globals.imageService}
        jahia_cfg_imageMagickPath: ${globals.jahiaImageMagickPath}
        jahia_cfg_mvnPath: ${globals.jahia_cfg_mvnPath}

  copyApp:
    - log: "## Copying Jahia app and setting its properties"
    - cmd[${this}]: |-
        [ "$_ROLE" == "Browsing" ] && sed -i "s#\(processingServer\s*=\).*#\1 false#g" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
        if [ -d $STACK_PATH/webapps-tmp ]; then
          rm -rf $STACK_PATH/webapps
          mv $STACK_PATH/webapps-tmp $STACK_PATH/webapps
        fi
        chown -R tomcat:tomcat $STACK_PATH/webapps

        tomcat_env=/opt/tomcat/conf/tomcat-env.sh
        short_name=$(echo ${_ROLE}.$HOSTNAME | sed -r 's/^([a-Z]+)\.[a-Z]+([0-9]+)-.+$/\1.\2/' | tr 'A-Z' 'a-z')
        sed -i "s|^cluster.node.serverId.*|cluster.node.serverId = $short_name|" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
        # the follwing will update or add if not present the jvmRoute property
        sed -i -n -e '/^\s*jahia.session.jvmRoute\s*=/!p' -e "\$ajahia.session.jvmRoute = $short_name" $STACK_PATH/conf/digital-factory-config/jahia/jahia.node.properties
        sed -i '/<!-- Access log processes all example./i \\t<!-- Remote IP Valve -->\n \t<Valve className="org.apache.catalina.valves.RemoteIpValve" protocolHeader="X-Forwarded-Proto" />\n' /opt/tomcat/conf/server.xml
        #Secure cookies from cross scripting
        indent="      " && printf "$indent<cookie-config>\n$indent$indent<secure>true</secure>\n$indent$indent<http-only>true</http-only>\n$indent</cookie-config>\n" > /tmp/cookies-config
        sed -i '/<session-config>/r /tmp/cookies-config' /opt/tomcat/conf/web.xml && rm /tmp/cookies-config
        # the following will update or add if not present these attributes: jvmRoute, maxPostSize, maxHttpHeaderSize
        xmlstarlet ed -P -L -S \
          -u "Server/Service/Engine/@jvmRoute" -v "$short_name" \
          -i "Server/Service/Engine[not(@jvmRoute)]" -t attr -n "jvmRoute" -v "$short_name" \
          -u "Server/Service/Connector[@port='80']/@maxPostSize" -v '${maxPostSize}' \
          -i "Server/Service/Connector[@port='80' and not(@maxPostSize)]" -t attr -n "maxPostSize" -v '${maxPostSize}' \
          -u "Server/Service/Connector[@port='80']/@maxHttpHeaderSize" -v '65536' \
          -i "Server/Service/Connector[@port='80' and not(@maxHttpHeaderSize)]" -t attr -n "maxHttpHeaderSize" -v '65536' \
          /opt/tomcat/conf/server.xml
        # jahia_cfg_cluster_tcp_bindAddress has to be exported in order for jahia's java process to see it since PAAS-2119 (jelastic/tomcat:9.0.75-openjdk-11.0.19 moved init scripts to systemd)
        grep -q '^export jahia_cfg_cluster_tcp_bindAddress=' ${tomcat_env} || sed -i '/TOMCAT_USER=/ a export jahia_cfg_cluster_tcp_bindAddress=$(hostname -i)' ${tomcat_env}
        sed -i '/^jahia_cfg_cluster_tcp_bindAddress=/d' ${tomcat_env}  # remove old fashion line if needed
        grep -q '^JAVA_OPTS.*-DmaxPostSize=' ${tomcat_env} || sed -i -E '/^JAVA_OPTS/ s/(.)$/ -DmaxPostSize=${tomcat_cfg_maxpostsize}\1/g' ${tomcat_env}
        grep -q '^JAVA_OPTS.*-XX:NativeMemoryTracking=' ${tomcat_env} || sed -i -E '/^JAVA_OPTS/ s/(.)$/ -XX:NativeMemoryTracking=summary\1/g' ${tomcat_env}
        grep -q '^JAVA_OPTS.*-Djava.security.egd=' ${tomcat_env} || sed -i -E '/^JAVA_OPTS/ s|(.)$| -Djava.security.egd=file:/dev/urandom\1|g' ${tomcat_env}
        sed -i 's/SHUTDOWN_WAIT.*=.*/SHUTDOWN_WAIT=30/' ${tomcat_env}
        # Datadog APM
        grep -q '^APM_OPTS=*' ${tomcat_env} || echo 'APM_OPTS="-Ddd.profiling.enabled=true -XX:FlightRecorderOptions=stackdepth=256 -Ddd.logs.injection=true -javaagent:/opt/tomcat/datadog/dd-java-agent.jar -Ddd.service=jahia -Ddd.env=${env.domain} -Ddd.trace.classes.exclude=org.jahia.modules.forms.dsl.*,org.jahia.modules.databaseConnector.dsl.* -Ddd.resolver.use.loadclass=false -Ddd.profiling.disabled.events=jdk.OldObjectSample"' >>  ${tomcat_env}
        grep -q '^$DATADOG_APM_ENABLED*' ${tomcat_env} || echo '$DATADOG_APM_ENABLED && JAVA_OPTS+=" $APM_OPTS"' >> ${tomcat_env}

        if [ "$(echo $DX_VERSION | awk '{print substr($NF,0,4) }')" != "8.1." ]; then
          echo 'CATALINA_OPTS="$CATALINA_OPTS -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory"' >> ${tomcat_env}
        fi

    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "An error occurred when installing jahia."

  setupDatadogAgentJahia:
    - log: "## Finalize Datadog agent setup on ${this}"
    - setGlobalRepoRootUrl
    - isAugSearchEnabled
    - generateNodesListFile
    - getPatTokenAndKey
    - configureDatadogSite:
        target: ${this}
    - cmd[${this}]: |-
        __secret__pat_token="${globals.__secret__pat_token}"
        NODE_NAME=${HOSTNAME/-*}
        dd_dir=/etc/datadog-agent
        dd_conf_dir=$dd_dir/conf.d
        dd_jelastic_dir=$dd_conf_dir/jelastic.d
        jahia_root_token_file=$dd_conf_dir/healthcheck_metric.d/jahia_root_token

        echo "hostname: ${_ROLE}.${NODE_NAME#node}" >> $dd_dir/datadog.yaml
        chmod 644 /opt/tomcat/logs/catalina.out
        mkdir $dd_jelastic_dir /var/log/jelastic-packages
        chown tomcat:root /var/log/jelastic-packages
        chown dd-agent: $dd_jelastic_dir
        curl -fLSso $dd_jelastic_dir/conf.yaml ${globals.repoRootUrl}/assets/common/dd_agent_jelastic_package_conf.yml || exit 1
        # Configure AS check and Healthcheck metric
        echo -n $__secret__pat_token > $jahia_root_token_file
        if [ "${globals.isAugSearchEnabled}" == "true" ]; then
          mv $dd_conf_dir/augmented_search.d/augmented_search.yaml-disabled $dd_conf_dir/augmented_search.d/augmented_search.yaml
        fi
        if [ "${_ROLE}" == "Processing" ]; then
          mv $dd_conf_dir/jahia_local_revisions_discrepancies.yaml{-disabled,}
          sed "s/__PASSWORD__/${DB_USER_DATADOG}/" -i $dd_conf_dir/jahia_local_revisions_discrepancies.yaml
          mv $dd_conf_dir/jahia_custom_metrics.d/jahia_custom_metrics.yaml{-disabled,}
        fi
        /usr/local/bin/set_dd_tags.sh
    - if (nodes.sqldb.length > 1):
        cmd[${this}]: |-
          setfacl -m u:dd-agent:rx /var/log/glusterfs/*.log
          sed -i '/killall.*glusterd/a \  /usr\/bin\/setfacl -m u:dd-agent:rx \/var\/log\/glusterfs\/*.log' /etc/logrotate.d/glusterfs
          cat > /etc/datadog-agent/conf.d/glusterfs.d/conf.yaml << EOF
          logs:
            - type: file
              path: /var/log/glusterfs/share.log
              source: glusterfs
              service: glusterfs
          EOF
          systemctl restart rsyslog crond datadog-agent
    - else:
        cmd[${this}]: |-
          setfacl -m u:dd-agent:rx /var/log/messages
          sed -i '/\/bin\/kill/a \        setfacl -m u:dd-agent:rx /var/log/messages' /etc/logrotate.d/syslog
          cat > /etc/datadog-agent/conf.d/nfsstat.d/conf.yaml << EOF
          init_config:
          instances:
            -
              min_collection_interval: 60
          logs:
            - type: file
              path: /var/log/messages
              source: nfsstat
              service: nfs
              log_processing_rules:
              - type: include_at_match
                name: include_nfsstat_only
                pattern: "nfsstat"
          EOF
          systemctl restart rsyslog crond datadog-agent

  startupJahiaHealthCheck:
    # Two arguments:
    #   - target: Mandatory, the target nodeId or nodeGroup. If the duration is not specified, the target
    #     can be passed as a parameter directly after the action name, e.g.: startupJahiaHealthCheck: <target>
    #   - duration: Optional, duration in seconds. Default value of 24 hours if not specified
    # The .print() call surrounded by simple quotes is the only working way I found to test if the variable exists
    - if ('${this.print()}' != ''):
        set:
          target: ${this}
    - else:
        set:
    - getPatTokenAndKey
    - cmd [${this.target}]: |-
        __secret__jahia_cfg_healthcheck_token=${globals.__secret__pat_token}
        if ! tomcat_pid=$(pgrep -u tomcat -f java); then
          echo "[ERROR] Tomcat process not found, please check." >&2
          exit 1
        fi

        timeout 60 bash -c '
          until [ -f /var/log/tomcat/jahia.log ]; do
            sleep 0.5
          done
        '
        if (($?)); then
          echo "[ERROR] Jahia log file not found, it seems there is a problem with tomcat instance, please check." >&2
          exit 2
        fi

        startup_line=$(grep -n "s t a r t i n g" /opt/tomcat/logs/catalina.out | tail -n1 | cut -d":" -f1)
        timeout=$(date --date="+$HEALTHCHECK_DURATION minutes" +%s)
        hc_url="http://127.0.0.1:8080/modules/healthcheck?severity=critical"

        # Number of minutes allowed for healthcheck to be completed once tomcat startup is finished
        jahia_running_timeout=5

        while [ $(date +%s) -lt $timeout ]; do
          # First we test if Jahia is up with a curl request.
          if curl_resp=$(curl -f -s -m 1 "$hc_url" -H "authorization: APIToken $__secret__jahia_cfg_healthcheck_token"); then
            status=$(echo $curl_resp | jq -r ".status.health")
            if [ "$status" = "GREEN" ] || [ "$status" = "YELLOW" ]; then
              exit 0
            fi
          fi

          # If it isn't, we first check tomcat process status
          if ! ps --pid $tomcat_pid > /dev/null; then
            echo "[ERROR] Tomcat process no more running, please check." >&2
            exit 3
          fi

          # Then we check Jahia startup status, all
          tail -n +${startup_line} /opt/tomcat/logs/catalina.out | grep -q "Server startup in"
          if [ $? -eq 0 ]; then
            if [ $jahia_running_timeout -eq 1 ]; then
              for module in graphql-dxm-provider server-availability-manager
              do
                curl -SsH "Authorization: APIToken $__secret__jahia_cfg_healthcheck_token" -XPOST -d "" \
                  "localhost/modules/api/bundles/$module/_refresh"
                curl -SsH "Authorization: APIToken $__secret__jahia_cfg_healthcheck_token" -XPOST -d "" \
                  "localhost/modules/api/bundles/$module/_stop"
                curl -SsH "Authorization: APIToken $__secret__jahia_cfg_healthcheck_token" -XPOST -d "" \
                  "localhost/modules/api/bundles/$module/_start"
              done
            fi
            if [ $jahia_running_timeout -eq 0 ]; then
              echo "[ERROR] Tomcat startup is finished but healthcheck failed, please check." >&2
              exit 4
            fi
            ((jahia_running_timeout-=1))
          fi

          sleep 60
        done

        echo "[ERROR] Timeout, the Tomcat process is still running but Jahia is not started yet" >&2
        exit 5

  checkJahiaHealth:
    - getPatTokenAndKey
    - cmd [${this}]: |-
        __secret__jahia_cfg_healthcheck_token=${globals.__secret__pat_token}
        if ! tomcat_pid=$(pgrep -u tomcat -f java); then
          echo "[ERROR] Tomcat process not found, please check" >&2
          exit 1
        fi

        hc_url="http://127.0.0.1:8080/modules/healthcheck?severity=critical"

        if curl_resp=$(curl -f -s -m 1 "$hc_url" -H "authorization: APIToken $__secret__jahia_cfg_healthcheck_token"); then
          status=$(echo $curl_resp | jq -r ".status.health")
          if [ "$status" = "GREEN" ] || [ "$status" = "YELLOW" ]; then
            exit 0
          fi
        fi
        echo "[ERROR] Healthcheck result different from GREEN or YELLOW, exiting" 1>&2 && exit 1

  checkJahiaDatadogCustomChecks:
    - if (nodes.sqldb.length == 3):
        - checkDatadogAgentCheck:
            target: ${this}
            checkName: proxysql_backend_missing
        - checkDatadogAgentCheck:
            target: ${this}
            checkName: proxysql_connections_discrepancies
    - else:
        - checkDatadogAgentCheck:
            target: ${this}
            checkName: proxysql
    - checkDatadogAgentCheck:
        target: ${this}
        checkName: jahia_node_not_in_haproxy_pool
    - checkDatadogAgentCheck:
        target: ${this}
        checkName: strongswan_connections_status
    - cmd[${this}]: |-
        if [ -f /etc/datadog-agent/conf.d/augmented_search.d/augmented_search.yaml-disabled ]; then
          echo "disabled"
        fi
    - if ("${response.out}" != "disabled"):
      - checkDatadogAgentCheck:
          target: ${this}
          checkName: augmented_search

  saveApplicationcontextFilesBeforeRedeploy:
    cmd[${this}]: |-
      mkdir /opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-files.tmp
      mv /opt/tomcat/conf/digital-factory-config/jahia/applicationcontext*.xml /opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-files.tmp/

  restoreApplicationcontextFilesAfterRedeploy:
    cmd[${this}]: |-
      mv /opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-files.tmp/* /opt/tomcat/conf/digital-factory-config/jahia/
      rmdir /opt/tomcat/conf/digital-factory-config/jahia/applicationcontext-files.tmp

  deleteEnvLinkJahia:
    # Parameters:
    #   - jCustomerEnv: Jcustomer env name in the env link of Jahia
    # Returns:
    #   - ${response.link_removed}: true or false (true if Jahia env is removed from envLink of Jcustomer)
    - script: |-
        const envName = "${env.envName}";
        const jCustomerEnv = "${this.jCustomerEnv}";
        envsLinked = jelastic.env.control.GetNodeGroups(jCustomerEnv, session).object.filter(function (object) {
                                return object.name == "cp";
                              }).pop().envLink;
        if (envsLinked.indexOf(envName) == -1) {
          return {"result": 0, "link_removed": false, "out": envName + " not in envLink of " + jCustomerEnv};
        }

        // envLink can contain multiple Jahia envs on jCustomer side separated by a comma
        if (envsLinked.indexOf(",") != -1) {
          envsLinkedArr = envsLinked.split(",");
          envsLinkedArr.splice(envsLinkedArr.indexOf(envName), 1);
          newEnvLink = String(envsLinkedArr);
        } else {
          newEnvLink = null;
        }
        resp = jelastic.env.nodegroup.ApplyData(jCustomerEnv, session, nodeGroup='cp', data={'envLink': newEnvLink});
        return {"result": 0, "link_removed": true, "out": envName + " removed from envLink of " + jCustomerEnv};

  removeKibanaDashboardAccountsAndSpace:
    # Parameters:
    #   - jCustomerEnv: Jcustomer env name in the env link of Jahia
    - setGlobalRepoRootUrl
    - set:
        dashboardRoleAndAccountName: ${env.envName}-kibana-dashboard
        customerUserRoleAndAccountName: ${env.envName}-kibana-user
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/delete-kibana-role.yml",
              envName: "${this.jCustomerEnv}",
              settings: {
                'roleName': '${this.dashboardRoleAndAccountName}',
              }
            }
          );
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/delete-elasticsearch-account.yml",
              envName: "${this.jCustomerEnv:}",
              settings: {
                'accountName': '${this.dashboardRoleAndAccountName}',
              }
            }
          );
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/delete-elasticsearch-account.yml",
              envName: "${this.jCustomerEnv:}",
              settings: {
                'accountName': '${this.customerUserRoleAndAccountName}',
              }
            }
          );
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/delete-kibana-space.yml",
              envName: "${this.jCustomerEnv}",
              settings: {
                'spaceName': '${this.dashboardRoleAndAccountName}',
              }
            }
          );

  switchFullReadonly:
    # Parameters:
    # maxTry (optional): default value is 15
    # group: nodegroup/host where the commands will be executed
    # fullreadmode: ON/OFF to enable/disable FRO
    - log: "switch full read mode to ${this.fullreadmode} on nodegroup ${this.group}"
    - cmd[proc]: |-
        wc -l /opt/tomcat/logs/catalina.out | awk '{print $1}'
    - set:
        maxTry: ${this.maxTry:15}
        execution: "FAIL"
        last_log_line: ${response.out}

    - callProvisioningAPI:
        target: ${this.group}
        payload:
          - karafCommand: "full-read-only ${this.fullreadmode}"
            timeout: "60000"

    - if (${globals.provisioning_api_call_success}):
        # If OK, let's check logs to make sure FRO is really in the state we asked
        - cmd[${this.group}]: |-
            pattern="Now the read-only mode is"
            last_mode=$(tail -n +${this.last_log_line} /opt/tomcat/logs/catalina.out | grep "$pattern" | tail -n1 | awk '{print $NF}')
            if [ -z "$last_mode" ] || [ "$last_mode" != "${this.fullreadmode}" ]; then
              echo "NOK"
            fi
        - if ("${response.out}" == ""):
            - set:
                execution: "OK"
            - log: "FRO is now ${this.fullreadmode}"

    - if ( "${this.execution}" == "FAIL" ):
        - if (${this.maxTry} > 0):
            - script: |-
                resp = {"result": 0}
                resp.onAfterReturn = {
                  set: {
                    maxTry: ${this.maxTry} -1
                  }
                }
                return resp

            - switchFullReadonly:
                maxTry: ${this.maxTry}
                group: ${this.group}
                fullreadmode: ${this.fullreadmode}
        - else:
            - if ('${this.fullreadmode}' == 'ON'):
                callProvisioningAPI:
                  target: ${this.group}
                  payload: '- karafCommand: "full-read-only OFF"'
                  timeout: "60000"
            - return:
              type: error
              message: "Read only switch failed"

  switchReadonly:
    - log: "switch read-only mode to ${this.readmode} on nodegroup ${this.group}"
    - callProvisioningAPI:
        target: ${this.group}
        payload: '- karafCommand: "read-only ${this.readmode}"'
    - if (! ${globals.provisioning_api_call_success}):
        return:
        type: error
        message: "Read only switch failed"

  enableKarafLogin:
    - isKarafLoginEnabled: ${globals.isKarafLoginEnabled}
      script: |-
          try {
            var resp = JSON.parse(isKarafLoginEnabled)["${this}"]
          } catch(error) {
            var resp = false
          }
          return {"result": 0, "resp": resp}
    - if(${response.resp}):
        - log: "karaf login is already enabled, nothing to do"
    - else:
        - log: "Activate karaf's ssh login on ${this}"
        - cmd[${this}]: |-
            # Clear everything and enable karaf login
            [ -f /tmp/abricot ] && rm /tmp/abricot
            [ -f /tmp/abricot.pub  ] && rm /tmp/abricot.pub
            ssh-keygen -t rsa -f /tmp/abricot -P ""
            awk '{printf "abricot:%s,_g_:admingroup\n",$2}' /tmp/abricot.pub >> /data/digital-factory-data/karaf/etc/keys.properties
            sed 's,\(sshRealm\s*=\s*\)jahia,\1karaf,' -i /data/digital-factory-data/karaf/etc/org.apache.karaf.shell.cfg
            i=1
            it=66
            until (ssh abricot@localhost -p 8101 -i /tmp/abricot -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR full-read-only); do
              echo "karaf ssh login not updated yet (iteration $i/$it)"
              if [ $i -ge $it ]; then
                echo "Too long to start, something is wrong here... EXITING"
                exit 1
              fi
              ((i++))
              sleep 1
            done
        - isKarafLoginEnabled: ${globals.isKarafLoginEnabled}
          script: |-
            try {
              var isEnable = JSON.parse(isKarafLoginEnabled)
            } catch(error) {
              var isEnable = {}
            } finally {
              isEnable["${this}"] = true
            }
            return {"result": 0, "isEnable": isEnable}
        - setGlobals:
            isKarafLoginEnabled: ${response.isEnable}
            karafConsole: "ssh abricot@localhost -p 8101 -i /tmp/abricot -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o LogLevel=ERROR"

  disableKarafLogin:
    - isKarafLoginEnabled: ${globals.isKarafLoginEnabled}
      script: |-
          try {
            var resp = JSON.parse(isKarafLoginEnabled)["${this}"]
          } catch(error) {
            var resp = false
          }
          return {"result": 0, "resp": resp}
    - if(! ${response.resp}):
        - log: "karaf login is already disabled, nothing to do"
    - else:
        - log: "Disable karaf's ssh login on ${this}"
        - cmd[${this}]: |-
            [ -f /tmp/abricot  ] && rm /tmp/abricot
            [ -f /tmp/abricot.pub  ] && rm /tmp/abricot.pub
            sed '/^abricot:/d' -i /data/digital-factory-data/karaf/etc/keys.properties
            sed 's,\(sshRealm\s*=\s*\)karaf,\1jahia,' -i /data/digital-factory-data/karaf/etc/org.apache.karaf.shell.cfg
        - isKarafLoginEnabled: ${globals.isKarafLoginEnabled}
          script: |-
            try {
              var isEnable = JSON.parse(isKarafLoginEnabled)
            } catch(error) {
              var isEnable = {}
            } finally {
              isEnable["${this}"] = false
            }
            return {"result": 0, "isEnable": isEnable}
        - setGlobals:
            isKarafLoginEnabled: ${response.isEnable}

  enableReadOnlyOnCluster:
    - switchReadonly:
        group: "proc, cp"
        readmode: "ON"

  disableReadOnlyOnCluster:
    - switchReadonly:
        group: "proc, cp"
        readmode: "OFF"

  enableFullReadOnlyOnCluster:
    - switchFullReadonly:
        group: "proc"
        fullreadmode: "ON"

  disableFullReadOnlyOnCluster:
    - switchFullReadonly:
        group: "proc"
        fullreadmode: "OFF"

  getJahiaVersion:
  # The placeholder ${nodes.proc.first.customitem.nodeVersion} doesn't return the correct value during an upgrade
  # and thus jelastic API is more reliable in this case.
    - log: "Get jahia version"
    - script: |-
        const envName = "${env.envName}";
        resp = jelastic.env.control.GetEnvInfo(envName, session)
        for (i = 0, g = resp.nodes; i < g.length; i++) {
          if (g[i].nodeGroup == "proc") {
            nodeVersion = g[i].customitem.nodeVersion
            return {
              "result": 0,
              "onAfterReturn": {
                "setGlobals": {
                  "jahiaVersion": nodeVersion
                }
              }
            }
          }
        }
        return {"result": 1, errOut: "Can't get Jahia version"}
    - log: "Jahia is v${globals.jahiaVersion}"

  checkModule:
    # Checks the state of a module using the provisioning API (or fallback to the digital-factory-data/bundles-deployed folder if failed).
    # First we check the state of the module, uninstalled, installed or started. Then, if installed or started,
    # we check the versions: most recent version installed, running version, list of installed versions...
    #
    # Parameters:
    # - moduleSymname: the symbolic name of the module to upgrade (for instance "distributed-sessions")
    # Returns:
    # - globals.moduleState:
    #   - "started" if the module is running
    #   - "installed" if the module is installed but not running
    #   - "uninstalled" if the module is not installed
    # - globals.mostRecentInstalledVersion: the most recent version installed (or an empty string if the module is uninstalled)
    # - globals.installedVersionsCount: the number of installed versions (or an empty string if the module is uninstalled)
    # - globals.installedVersions: a comma seperated list of installed versions (or an empty string if the module is uninstalled)
    # - globals.runningVersion: the version running (or an empty string if the module is not running)
    - setGlobals:  # re-initialise globals to prevent successive calls to this action from overlapping
        moduleState: ""
        runningVersion: ""
        mostRecentInstalledVersion: ""
        installedVersionsCount: 0
    - set:
        karafOutputFile: /tmp/check_modue_karaf_output_${fn.random}
    - cmd [proc]: |-
        rm -f ${this.karafOutputFile}
    - callProvisioningAPI:
        target: proc
        payload: '- karafCommand: "cluster:bundle-list -s default ${this.moduleSymname} | tac -a ${this.karafOutputFile}"'
    - cmd [proc]: |-
        karaf_output=${this.karafOutputFile}
        # if the `cluter:bundle-list` command succeeded, it does contain the header line
        # even if the module is not present...
        if grep -q ID $karaf_output; then
          awkScript='BEGIN {
            FS = "│"
            printf "{"
            comma = ", "
          }
          NR>3 {
            gsub(/ /, "", $0)
            if (NR == lastLine) comma = ""
            printf("\"%s/%s\": {\"moduleState\":\"%s\"}%s", $7, $6, $2, comma)
          }
          END {
            print "}"
          }'
          resp=$(awk -v lastLine="$(awk "END {print NR}" $karaf_output)" "$awkScript" $karaf_output)
        else # ...if not then it means the command failed and we have to use the fallback method
          modules_found=$(grep "/${this.moduleSymname}/" -l /data/digital-factory-data/bundles-deployed/*/bundle.info)
          read -r -d '' awkScript <<- 'EOS'
          BEGIN {
            FS=":"
            printf "{"
            file_count=ARGC-1
          }
          BEGINFILE {
            file_number++
            NR=0
          }
          {
            switch(NR) {
              case 2:
                module_ver=$NF
                break
              case 3:
                switch($1) {
                  case 1:
                    status="uninstalled"
                    break
                  case 2:
                    status="installed"
                    break
                  case 4:
                    status="resolved"
                    break
                  case 8:
                    status="starting"
                    break
                  case 16:
                    status="stopping"
                    break
                  case 32:
                    status="started"
                    break
                  default:
                    status="go fuck yourself"
                    break
                }
                break
            }
          }
          ENDFILE {
            if(file_number<file_count) {
              sep=","
            }
            else {
              sep=""
            }
            printf "\"%s\": {\"moduleState\":\"%s\"}%s",module_ver,status,sep
          }
          END {
            print "}"
          }
        EOS
          if [ $(echo $modules_found | wc -w) -ne 0 ]; then
            resp=$(awk "$awkScript" $modules_found)
          else
            resp="{}"
          fi
        fi

        case "$(jq -r ". | length" <<< $resp)" in
          "0") # Means the module is not installed
            module_state=uninstalled
            ;;
          "") # if response is not JSON
            echo $resp >&2
            exit 1
            ;;
          *)
            module_started=$(jq -r 'to_entries[] | "\(.key);\(.value | .moduleState | ascii_downcase)"' <<< $resp | grep -E "started|active")
            if [ $? -eq 0 ]; then
              module_state=started
              running_version=$(echo $module_started | cut -d';' -f1 | awk -F'/' '{print $NF}')
            else
              module_state=installed
            fi
            most_recent_installed_version=$(jq -r "keys[]" <<< $resp | awk -F'/' '{print $NF}' | sort -nr | head -1)
            installed_versions_count=$(jq -r "keys | length" <<< $resp)
            installed_versions=$(jq -r 'keys[] | gsub("^.+/"; "")' <<< $resp | paste -sd, -)  # comma separated string of installed versions
            ;;
        esac
        echo "{ \
            'moduleState': '$module_state', \
            'runningVersion': '$running_version', \
            'mostRecentInstalledVersion': '$most_recent_installed_version', \
            'installedVersionsCount': '$installed_versions_count', \
            'installed_versions': '$installed_versions' \
          }"
    - script: |-
        const resp = ${response.out.toJSON()}
        return {
          "result": 0,
          "onAfterReturn": {
            setGlobals: resp
          }
        };

  installOrUpgradeModules:
    # Installs or upgrades a list of one or more modules using the "installOrUpgradeBundle" endpoint
    # from the provisioning API.
    # (https://github.com/Jahia/jahia/tree/master/bundles/provisioning#additional-syntax)
    #
    # Parameters:
    #   - modules: a comma separated list of "<module>/<version>" to install. Examples:
    #              - "distributed-session/3.4.0"
    #              - "augmented-search/3.3.2, jexperience/3.3.0"
    - set:
        provisioningFile: /opt/tomcat/temp/install_modules_provisioning_${fn.random}.yaml
        mavenRepository: devtools.jahia.com/nexus/content/groups/enterprise@id=jahia-enterprise
        paxFile: /data/digital-factory-data/karaf/etc/org.ops4j.pax.url.mvn.cfg
    - cmd [proc]: grep -q '${this.mavenRepository}' ${this.paxFile} || echo "need to install maven repository"
    - if ("${response.out}"):
      - log: Installing the Jahia Enterprise Maven repository to install new modules
      - getNexusCredentials
      - callProvisioningAPI:
          target: proc
          payload:
            - addMavenRepository: "https://${this.mavenRepository}"
              username: ${globals.__secret__nexusLogin}
              password: ${globals.__secret__nexusPassword}
      - cmd [proc]: grep -q '${this.mavenRepository}' ${this.paxFile} || echo "maven repository not present"
      - if ("${response.out}"):
        - log: The installation of the Jahia Enterprise Maven repository failed, please check Jahia logs
        - return:
            type: error
            message: "An error occurred at module installation."
    - log: "Installing the following module(s): ${this.modules}"
    - cmd [proc]: |-
        awk 'BEGIN {
          RS = ","
          print("- installOrUpgradeBundle:")
        }
        $1 != "" {
          printf("  - \"mvn:org.jahia.modules/%s\"\n", $1)
        }
        ' <<<'${this.modules}' > ${this.provisioningFile}
    - callProvisioningAPI:
        target: proc
        payload: "@${this.provisioningFile}"
    - if (! ${globals.provisioning_api_call_success}):
        - return:
            type: error
            message: "An error occurred at module installation."
    # We parse the list of modules to be able to check each one of them
    - script: |
        var modulesParsed = []
        "${this.modules}".split(",").forEach(
          function(moduleWithVersion) {
            var moduleAndVersion = moduleWithVersion.split("/")
            modulesParsed.push({"module": moduleAndVersion[0], "version": moduleAndVersion[1]})
          }
        )
        return {"result": 0, "onAfterReturn": {set: {"modulesParsed": modulesParsed}}}
    - forEach (this.modulesParsed):
      - checkModule:
          moduleSymname: ${@i.module}
      # If the module is "uninstalled", it means the installation/upgrade completely failed
      - if ("${globals.moduleState}" == "uninstalled"):
        - return:
            type: error
            message: "An error occurred at module installation."
      # If it is "installed", it can be for multiple reasons:
      # - the module was "installed" with an older version
      # - the module was "installed" with a newer version
      # - the module was "started" with an older version or "uninstalled" and the startup failed
      # In each one of these situations we want to try and start the most recent installed version of the module
      # (if it is greater or equal to the version we tried to install or else it means the installation/upgrade failed)
      #  since the installOrUpgradeBundle action will preserve the module state but we want the module to be running
      # regardless of the previous status
      - if ("${globals.moduleState}" == "installed"):
        - isVersionHigherOrEqual:
            a: ${globals.mostRecentInstalledVersion}
            b: ${@i.version}
            res: moduleIsUpToDate
        - if (! "${globals.moduleIsUpToDate}"):
          - return:
              type: error
              message: "An error occurred at module upgrade."
        - startModules:
            modules: "${@i.module}/${globals.mostRecentInstalledVersion}"

  startModules:
    # Starts a list of one or more modules and make sure the right version of the module
    # is running.
    #
    # Parameters:
    #   - modules: a comma separated list of "<module>/<version>" to start. Examples:
    #              - "distributed-session/3.4.0"
    #              - "augmented-search/3.3.2, jexperience/3.3.0"
    - set:
        provisioningFile: /opt/tomcat/temp/start_modules_provisioning_${fn.random}.yaml
    - log: "Starting the following module(s): ${this.modules}"
    - cmd [proc]: |-
        awk 'BEGIN {
          RS = ","
          print("- startBundle:")
        }
        $1 != "" {
          printf("  - \"%s\"\n", $1)
        }
        ' <<<'${this.modules}' > ${this.provisioningFile}
    - callProvisioningAPI:
        target: proc
        payload: "@${this.provisioningFile}"
    - if (! ${globals.provisioning_api_call_success}):
        - return:
            type: error
            message: "An error occurred at module startup."
    # We parse the list of modules to be able to check each one of them
    - script: |
        var modulesParsed = []
        "${this.modules}".split(",").forEach(
          function(moduleWithVersion) {
            var moduleAndVersion = moduleWithVersion.split("/")
            modulesParsed.push({"module": moduleAndVersion[0], "version": moduleAndVersion[1]})
          }
        )
        return {"result": 0, "onAfterReturn": {set: {"modulesParsed": modulesParsed}}}
    - forEach (this.modulesParsed):
      - checkModule:
          moduleSymname: ${@i.module}
      # If the module is not "started" with the version we tried to start, we wait for 30s
      # and exit with an error if it is still not running
      - if ("${globals.runningVersion}" != "${@i.version}"):
        - set:
            tryCount: [1,2,3,4,5,6,7,8,9,10]
        - forEach (this.tryCount):
          - if ("${globals.runningVersion}" != "${@i.version}"):
            - log: "Try #${@i}..."
            - checkModule:
                moduleSymname: ${this.moduleSymname}
            - if ("${globals.runningVersion}" != "${@i.version}"):
              - sleep: 3000
        - if ("${globals.runningVersion}" != "${@i.version}"):
        - return:
            type: error
            message: "An error occurred at module startup."

  stopModules:
    # Stops a list of one or more modules and make sure the right version of the module
    # is running.
    #
    # Parameters:
    #   - modules: a comma separated list of modules to stop (including the version is not mandatory). Examples:
    #              - "distributed-session/3.4.0"
    #              - "augmented-search, jexperience/3.3.0"
    - set:
        provisioningFile: /opt/tomcat/temp/stop_modules_provisioning_${fn.random}.yaml
    - log: "Stopping the following module(s): ${this.modules}"
    - cmd [proc]: |-
        awk 'BEGIN {
          RS = ","
          print("- stopBundle:")
        }
        $1 != "" {
          printf("  - \"%s\"\n", $1)
        }
        ' <<<'${this.modules}' > ${this.provisioningFile}
    - callProvisioningAPI:
        target: proc
        payload: "@${this.provisioningFile}"
    - if (! ${globals.provisioning_api_call_success}):
        - return:
            type: error
            message: "An error occurred at module stop."
    # We parse the list of modules to be able to check each one of them
    - script: |
        var modulesParsed = []
        "${this.modules}".split(",").forEach(
          function(moduleWithVersion) {
            var moduleAndVersion = moduleWithVersion.split("/")
            modulesParsed.push({"module": moduleAndVersion[0], "version": moduleAndVersion[1]})
          }
        )
        return {"result": 0, "onAfterReturn": {set: {"modulesParsed": modulesParsed}}}
    - forEach (this.modulesParsed):
      - checkModule:
          moduleSymname: ${@i.module}
      # If the module is not "installed" or "uninstalled", we wait for 15s
      # and exit with an error if it is still running
      - if ("${globals.moduleState}" == "started"):
        - set:
            tryCount: [1,2,3,4,5]
        - forEach (this.tryCount):
          - if ("${globals.moduleState}" == "started"):
            - log: "Try #${@i}..."
            - checkModule:
                moduleSymname: ${this.moduleSymname}
            - if ("${globals.moduleState}" == "started"):
              - sleep: 3000
        - if ("${globals.moduleState}" == "started"):
        - return:
            type: error
            message: "An error occurred at module stop."

  uninstallModule:
    # Does a partial or complete uninstallation of a module.
    #
    # Parameters:
    #   - moduleSymname: the symbolic name of the module to uninstall (for instance "distributed-sessions")
    #   - moduleVersion: the version of the module to uninstall
    #   - stoppedVersions: set to "true" if only stopped versions should be uninstalled.
    #                      Default value is "false". This parameter is ignored if "moduleVersion" is set
    - set:
        provisioningFile: /opt/tomcat/temp/uninstall_module_provisioning_${fn.random}.yaml
        skip: false
    - checkModule:
        moduleSymname: ${this.moduleSymname}
    - if ("${globals.moduleState}" == "uninstalled"):
      - log: ${this.moduleSymname} module is not installed, nothing to do
      - set:
          skip: true
    - else:
      - if ("${this.moduleVersion.print()}"):
        - script: |
            return {"result": 0, "isVersionInstalled": ${globals.installedVersions}.split(",").includes(${this.moduleVersion})}
        - if (! ${response.isVersionInstalled}):
          - log: ${this.moduleSymname}/${this.moduleVersion} is not installed, nothing to do
          - set:
              skip: true
        - else:
          - log: Uninstalling ${this.moduleSymname}/${this.moduleVersion}
          - cmd [proc]: |-
              echo "- uninstallBundle: \"${this.moduleSymname}/${this.moduleVersion}\"" > ${this.provisioningFile}
      - else:
        - if ("${this.stoppedVersions.print()}" == "true") && ("${globals.moduleState}" == "started") && (${globals.installedVersionsCount} > 1):
          - cmd [proc]: |-
              awk 'BEGIN {
                RS = ","
              }
              ($1 !~ /${this.runningVersion}/ && $1 != "") {
                printf("- uninstallBundle: \"${this.moduleSymname}/%s\"\n", $1)
              }
              ' <<<$versions > ${this.provisioningFile}
          - log: "Uninstalling stopped versions of ${this.moduleSymname} module:"
        - else:
          - log: Uninstalling ${this.moduleSymname}
          - cmd [proc]: |-
              echo "- uninstallBundle: \"${this.moduleSymname}\"" > ${this.provisioningFile}
    - if (! ${this.skip}):
      - callProvisioningAPI:
          target: proc
          payload: "@${this.provisioningFile}"
      - checkModule:
          moduleSymname: ${this.moduleSymname}
      - if ("${globals.moduleState}" != "uninstalled"):
        - if ("${response.out}" == "error"):
            - return:
                type: error
                message: "An error occurred during module uninstallation."

  triggerAugSearchFullReindex:
    # Parameters:
    #   asynchronous: true to not wait for the end of the fullReindex, false otherwise. Default true
    cmd[proc]: |-
        __secret__API_TOKEN="${globals.__secret__pat_token}"
        curl -XPOST \
          http://localhost:8080/modules/graphql \
          -H "Authorization: APIToken $__secret__API_TOKEN" \
          -H "Content-Type: application/json" \
          -H "Origin: http://localhost:8080" \
          -d '{"query":"mutation {admin {search {startIndex {jobs {id status project {siteKey} } } } } } "}'

        if [ "${this.asynchronous:true}" == "true" ]; then
          exit 0
        fi

        check_indexation_state() {
          res=$(curl -s -XPOST \
          http://localhost:8080/modules/graphql \
          -H "Authorization: APIToken $__secret__API_TOKEN" \
          -H 'Content-Type: application/json' \
          -H 'Origin: http://localhost:8080' \
          -d '{"query":"{admin {search {listSites {sites {indexationStatus siteKey} } } } } "}')
          echo $res | jq -r '.data.admin.search.listSites.sites | .[] | select(.indexationStatus!="COMPLETED")'
        }

        check=$(check_indexation_state)
        while [ ! -z "$check" ]; do
          sleep 30
          check=$(check_indexation_state)

          # As the operation can take several hours, it's impossible to set a timeout value
          # The manual and "clean" way to abort the package is to create the /tmp/stopWaitingForReindexEnd file.
          if [ -f "/tmp/stopWaitingForReindexEnd" ]; then
            echo "Force quit"
            exit 1
          fi
        done

  setJahiaUserFeedbacksConfig:
    - getJahiaVersion
    - isVersionHigherOrEqual:
        a: ${globals.jahiaVersion}
        b: 8.1.0.0
        res: jahia81plus
    - if (${globals.jahia81plus}):
        - script: |-
            const envName = "${env.envName}";
            orgName = jelastic.env.control.GetNodeGroups(envName, session).object.filter(function (object) {
                                        return object.name == "cp";}).pop().ORGANIZATION_NAME;
            return {
              "result": 0,
              "onAfterReturn": {
                "set": {
                  "organizationName": orgName
                }
              }
            }
        - cmd[proc]: |-
            config_file="/data/digital-factory-data/karaf/etc/org.jahia.services.env.cfg"
            organization_name="${this.organizationName}"
            env_name="${env.envName}"
            if [ ! -f $config_file ]; then
              touch $config_file
              echo "org.jahia.services.env.organization=$organization_name" >> $config_file
              echo "org.jahia.services.env.env_name=$env_name" >> $config_file
            else
              if ( ! grep -q "organization=$organization_name" $config_file ) || ( grep -qc "organization=$organization_name" $config_file | awk -F: '$NF+0 > 1' ); then
                sed -i '/.*organization=.*/d' $config_file
                echo "org.jahia.services.env.organization=$organization_name" >> $config_file
              fi
              if ( ! grep -q "env_name=$env_name" $config_file ) || ( grep -qc "env_name=$env_name" $config_file | awk -F: '$NF+0 > 1' ); then
                sed -i '/.*env_name=.*/d' $config_file
                echo "org.jahia.services.env.env_name=$env_name" >> $config_file
              fi
            fi
            chown tomcat:tomcat $config_file

  generateNodesListFile:
    - set:
        nodesList: "processing.${nodes.proc.first.id}"
    - forEach(nodes.cp):
        - set:
            nodesList: browsing.${@i.id} ${this.nodesList}
    - cmd[proc]: |-
        echo "${this.nodesList}" > /nodesList

  ####################
  # proxysql related #
  ####################
  setupProxysqlCluster:
    - cmd[cp, proc]: |-
          ${globals.proxysql_cli} -e "DELETE FROM proxysql_servers"
    - foreach (nodes.cp):
        - cmd[cp, proc]: |-
            ${globals.proxysql_cli} -e "INSERT INTO proxysql_servers (hostname,port,weight,comment) VALUES ('node${@i.id}-${env.domain}',6032,0,'browsing_$((${@}+1))')"
    - cmd [cp, proc]: |-
        ${globals.proxysql_cli} -e "INSERT INTO proxysql_servers (hostname,port,weight,comment) VALUES ('node${nodes.proc.first.id}-${env.domain}',6032,0,'processing')"
        ${globals.proxysql_cli} -e "LOAD PROXYSQL SERVERS TO RUNTIME"
        ${globals.proxysql_cli} -e "SAVE PROXYSQL SERVERS TO DISK"
    - cmd [cp, proc]: |-
        i=60
        sql="select count(*) from stats_proxysql_servers_metrics where Uptime_s = 0"
        while ! sleep 1 && ${globals.proxysql_cli} -e "$sql" | grep -s 0; do
          ((i--))
          if [ $i -eq 0 ]; then
            echo "[ERROR] ProxySQL cluster is not healthy" 1>&2
            exit 1
          fi
        done
    - if (nodes.sqldb.length == 1):
        configureProxysqlForSingleDBNode

  configureProxysqlForSingleDBNode:
    cmd[cp, proc]: |-
      ${globals.proxysql_cli} -e "DELETE FROM mysql_servers WHERE hostgroup_id!=2;"
      ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections) VALUES (2,'galera_1',3306, 100)"
      ${globals.proxysql_cli} -e "UPDATE mysql_galera_hostgroups SET active=0;"
      ${globals.proxysql_cli} -e "LOAD MYSQL SERVERS TO RUNTIME;"
      ${globals.proxysql_cli} -e "SAVE MYSQL SERVERS TO DISK;"

  setupMysqlServers:
    - cmd[proc]: |-
        ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections) VALUES (2,'galera_1',3306, 100)"
    - if (nodes.sqldb.length > 1):
        - cmd[proc]: |-
            ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections) VALUES (2,'galera_2',3306, 100)"
            ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections, weight) VALUES (2,'galera_3',3306, 100, 1000)"
    - cmd[proc]: |-
        ${globals.proxysql_cli} -e "LOAD MYSQL SERVERS TO RUNTIME"
        ${globals.proxysql_cli} -e "SAVE MYSQL SERVERS TO DISK"

  setupMonitorUser:
    # Parameters:
    # __secret__password: proxysql monitor user password
    - cmd[sqldb]: |-
        __secret__password="${this.__secret__password}"
        mysql -e "CREATE USER IF NOT EXISTS 'proxysql'@'%' IDENTIFIED BY '$__secret__password'"
        mysql -e "GRANT SELECT on sys.* TO 'proxysql'@'%'"
        mysql -e "GRANT SELECT on performance_schema.* TO 'proxysql'@'%'"
        mysql -e "GRANT  PROCESS, REPLICATION CLIENT ON *.* TO 'proxysql'@'%'"

  enableBackendMonitor:
    - cmd[cp, proc]: |-
        ${globals.proxysql_cli} -e "UPDATE global_variables SET variable_value='true' WHERE variable_name='mysql-monitor_enabled'"
        ${globals.proxysql_cli} -e "LOAD MYSQL VARIABLES TO RUNTIME"
        ${globals.proxysql_cli} -e "SAVE MYSQL VARIABLES TO DISK"

  finishProxysqlInstall:
    # Parameters:
    # __secret__monitor_password: proxysql monitor user password
    - updateProxySQLAdminPassword:
        target: "proc, cp"
    - setProxysqlCredsInDatadogConfig:
        target: "proc, cp"

    - setupMonitorUser:
        __secret__password: ${this.__secret__monitor_password}

    - enableBackendMonitor
    - if (nodes.sqldb.length > 1):
        - setupMysqlServers
    - else:
        - disableDatadogCustomChecks
    - setupProxysqlCluster

  updateProxySQLAdminPassword:
    # Parameters
    # target: nodes/nodesGroup to run the script on
    cmd[${this.target}]: |-
      admin_creds_query="set admin-admin_credentials='admin:$PROXYSQL_ADMIN_PASSWORD;cluster1:$PROXYSQL_CLUSTER_PASSWORD';"
      cluster_creds_query="set admin-cluster_password='$PROXYSQL_CLUSTER_PASSWORD';"
      monitor_creds_query="set mysql-monitor_password='$PROXYSQL_MONITORING_PASSWORD';"
      persist_config_query="LOAD ADMIN VARIABLES TO RUNTIME; LOAD MYSQL VARIABLES TO RUNTIME;"
      mysql -h 127.0.0.1 -uadmin -padmin -P6032 -e "$admin_creds_query $cluster_creds_query $monitor_creds_query $persist_config_query" || exit 1
      ${globals.proxysql_cli} -e "SAVE ADMIN VARIABLES TO DISK" || exit 1
      ${globals.proxysql_cli} -e "SAVE MYSQL VARIABLES TO DISK" || exit 1

  setProxysqlCredsInDatadogConfig:
    # Parameters
    # target: nodes/nodesGroup to run the script on
    cmd[${this.target}]: |-
      sed -i "s/password: admin/password: $PROXYSQL_ADMIN_PASSWORD/" /etc/datadog-agent/conf.d/proxysql.d/custom_checks
      sed -i "s/password: admin/password: $PROXYSQL_ADMIN_PASSWORD/" /etc/datadog-agent/conf.d/proxysql.d/conf.yaml

  refreshProxysqlInstancesList:
    - setupProxysqlCluster

  proxysqlSetMariadbBackendStatus:
    - cmd[cp, proc]: |-
        ${globals.proxysql_cli} -e "UPDATE mysql_servers SET status='${this.newStatus}' WHERE hostname='${this.targetHost}';"

  getGaleraMaster:
    # Return:
    #   - galeraMasterIndex
    - cmd[proc]: |-
        ${globals.proxysql_cli} -BNe "select DISTINCT hostname from runtime_mysql_servers where weight = 1000"
    - setGlobals:
        - galeraMasterIndex: ${response.out}

  proxysqlSwitchMaster:
    # Parameter:
    #   - target: galera number (1,2,3)
    - cmd[proc, cp]: |-
        # Check that the future master node is online
        res=$(${globals.proxysql_cli} -BNe "select * from runtime_mysql_servers where hostname='galera_${this.target}' and hostgroup_id=2 and status='ONLINE';")
        if [ -z "$res" ];then
          echo "The new masterNode is not online in proxySQL configuration" >> /var/log/jelastic-packages/proxySqlSwitchMaster.log
          exit 0
        fi

    - cmd[proc, cp]: |-
        ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections, weight) VALUES (2,'galera_1',3306, 100, 1);"
        ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections, weight) VALUES (2,'galera_2',3306, 100, 1);"
        ${globals.proxysql_cli} -e "REPLACE INTO mysql_servers(hostgroup_id,hostname,port, max_connections, weight) VALUES (2,'galera_3',3306, 100, 1);"
        ${globals.proxysql_cli} -e "update mysql_servers set weight=1000 where hostname='galera_${this.target}';"
        ${globals.proxysql_cli} -e "LOAD MYSQL SERVERS TO RUNTIME;"
        ${globals.proxysql_cli} -e "SAVE MYSQL SERVERS TO DISK"
        sleep 15

    - cmd[proc, cp]: |-
        # Check that there is only a single node with a weight of 1000
        res=$(${globals.proxysql_cli} -BNe "select * from runtime_mysql_servers where weight=1000 group by hostname;" | wc -l)
        if [ $res -lt 1 ];then
          echo "There is no master node with a weight of 1000 in proxysql runtime configuration" >> /var/log/jelastic-packages/proxySqlSwitchMaster.log
          exit 1
        elif [ $res -gt 1 ]; then
          echo "There is more than one master node with a weight of 1000 in proxysql runtime configuration" >> /var/log/jelastic-packages/proxySqlSwitchMaster.log
          exit 1
        fi

    - cmd[sqldb]: |-
        my_ip=$(grep $(hostname) /etc/hosts | awk '{print $1}')
        my_node_name_index=$(awk -v my_ip="$my_ip galera" '$0 ~my_ip {print $2}' /etc/hosts)
        if [[ "$my_node_name_index" == "galera_${this.target}" ]]; then
          exit 0
        fi
        timeout=1200 # wait for 1200s
        tries=0
        request="select COUNT(*) from INFORMATION_SCHEMA.PROCESSLIST where db='jahia' and user like 'jahia-db-%'"
        while [[ $(mysql -BNe "$request") -gt 0 ]]; do
          if [[ $tries -ge $timeout ]]; then
            echo "There are still open connections on $my_node_name_index"
            exit 1
          fi
          tries=`expr $tries + 10`
          sleep 10
        done

  disableDatadogCustomChecks:
    - cmd[cp, proc]: |-
        p="/etc/datadog-agent/conf.d"
        for check in proxysql_backend_missing proxysql_connections_discrepancies; do
          [ -h $p/${check}.yaml ] && mv $p/${check}.yaml $p/${check}.yaml_disabled
        done
        if systemctl -q is-active datadog-agent; then
          systemctl restart datadog-agent
        fi
        # In case the agent is not started yet (after a redeploy for example)
        # we don't want the script to fail since the agent will be configured and
        # started later on anyway
        exit 0

  procRedeploy:
    # Parameters:
    #   - upgradeJahia: true or false
    #   - targetDockerTag: jahia version string (eg: 8.0.2.0)
    - set:
        useExistingVolumes: true
    # Do not keep volumes in the case of an upgrade
    - if (${this.upgradeJahia}):
        - set:
            useExistingVolumes: false
    - cmd [proc]:
        - echo 'export JAHIA_UPGRADE="${this.upgradeJahia}"' >> /etc/locale.conf
    - api: environment.control.RedeployContainersByGroup
      nodeGroup: proc
      tag: ${this.targetDockerTag}
      useExistingVolumes: ${this.useExistingVolumes}
      skipReinstall: false
      envName: ${env.envName}
    # restore-module-state is not compatible with rolling redeploy
    - if (${this.upgradeJahia}):
        - cmd [proc]: |-
            rm -fr /data/digital-factory-data/bundles-deployed/*
            sudo -u tomcat touch "/data/digital-factory-data/[persisted-bundles].dorestore"
            echo "restore-module-state have been asked"
            ls -l /data/digital-factory-data/*.dorestore
            touch /data/digital-factory-data/modules/*
            service tomcat start
        - setJahiaUserFeedbacksConfig
    - cmd [proc]:
        - sed '/JAHIA_UPGRADE/d' -i /etc/locale.conf
    - startupJahiaHealthCheck: ${nodes.proc.first.id}
    - callProvisioningAPI:
        target: proc
        payload:
          - karafCommand: "bundle:refresh distributed-sessions server-availability-manager"
          - karafCommand: "cluster:bundle-start default distributed-sessions server-availability-manager"

  browsingNodesBulkRedeploy:
    # Parameters:
    #   - targetDockerTag: jahia version string (eg: 8.0.2.0)
    - api: environment.control.RedeployContainersByGroup
      nodeGroup: cp
      tag: ${this.targetDockerTag}
      useExistingVolumes: false
      skipReinstall: false
      envName: ${env.envName}
    - callProvisioningAPI:
        target: cp
        payload:
          - karafCommand: "bundle:refresh distributed-sessions server-availability-manager"

  browsingNodesRollingRedeploy:
    # Parameters:
    #   - upgradeJahia: true or false
    #   - targetDockerTag: jahia version string (eg: 8.0.2.0)
    - set:
        useExistingVolumes: true
    # Do not keep volumes in the case of an upgrade
    - if (${this.upgradeJahia}):
        - set:
            useExistingVolumes: false
    - forEach (nodes.cp):
        - api: environment.control.RedeployContainerById
          nodeId: ${@i.id}
          tag: ${this.targetDockerTag}
          useExistingVolumes: ${this.useExistingVolumes}
          skipReinstall: false
          envName: ${env.envName}
        - callProvisioningAPI:
            target: ${@i.id}
            payload:
              - karafCommand: "bundle:refresh distributed-sessions server-availability-manager"

  cleanJRLocalRevisionsTable:
    - cmd [${nodes.sqldb.first.id}]: |-
        mysql jahia -Nse "SELECT revision_id FROM JR_J_LOCAL_REVISIONS WHERE journal_id LIKE 'processing.%'"
        mysql jahia -e "TRUNCATE JR_J_LOCAL_REVISIONS"
    - set:
        currentRevision: ${response.out}
    - setEnvNodesAndRevision:
        target: proc
        currentRevision: ${this.currentRevision}
    - forEach(nodes.cp):
        setEnvNodesAndRevision:
          target: ${@i.id}
          currentRevision: ${this.currentRevision}
    - cleanJRJJournalTable:
        batchSize: 100000


  cleanJRJJournalTable:
    # Parameters:
    #   - batchSize: the number of lines to delete per batch
    - cmd [${nodes.sqldb.first.id}]: |-
        batch_size=${this.batchSize}
        query="SELECT MIN(REVISION_ID) FROM JR_J_LOCAL_REVISIONS;"
        min_revision_number=$(mysql jahia -sN -e "$query")
        if [ $? -ne 0 ]; then
          echo "Can't get the minimum revision number, aborting"
          exit 1
        fi
        query="DELETE FROM JR_J_JOURNAL WHERE REVISION_ID < $min_revision_number ORDER BY REVISION_ID LIMIT $batch_size"
        echo "Deleting rows with revision_id < $min_revision_number in table JR_J_JOURNAL. Batch size: $batch_size"
        n=1
        while true; do
          result=$(mysql jahia -vv -e "$query")
          if [ $? -ne 0 ]; then
            echo "Error when trying to delete the rows, aborting"
            exit 1
          fi
          if echo $result | grep -q " 0 rows affected"; then
            exit 0
          fi
          echo "Batch #$n executed"
          ((n+=1))
          sleep 2
        done

  setEnvNodesAndRevision:
    # Parameters:
    #   - target: target node group or id
    #   - currentRevision: Current Revision number
    - cmd [${this.target}]: awk '$1=="cluster.node.serverId" {print $NF; exit}' /opt/tomcat/conf/digital-factory-config/jahia/jahia.node.properties
    - cmd [${nodes.sqldb.first.id}]: |-
        node_name=${response.out}
        current_revision=${this.currentRevision}
        mysql jahia -e "INSERT INTO JR_J_LOCAL_REVISIONS values ('$node_name',$current_revision)"

  setupPat:
    # Parameters:
    # jahia_version: the jahia version
    - generatePatAndKey
    - savePatInVault:
        __secret__pat_token: ${globals.__secret__pat_token}
        __secret__pat_key: ${globals.__secret__pat_key}
    - setPatInJahia:
        __secret__pat_token: ${globals.__secret__pat_token}
        jahia_version: ${this.jahiaVersion}
    - setPatInHaproxy:
        __secret__pat_token: ${globals.__secret__pat_token}

  generatePatAndKey:
    - cmd[proc]: |-
        graph_token=$(cat /dev/urandom | tr -dc '[[:graph:]]' | fold -w 32 | head -n 1)
        token=$(echo -n "$graph_token" | base64)
        key=$(echo -n "$graph_token" | awk '{printf substr($1,0,16)}' | od -A n -t x1 | awk '{printf "%s%s%s%s-%s%s-%s%s-%s%s-%s%s%s%s%s%s",$1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13,$14,$15,$16}')
        printf '{"HideThisOutput": true, "token": "%s", "key": "%s"}' "$token" "$key"

    - script: |-
        __secret__response = ${response.out};
        token = __secret__response["token"];
        key = __secret__response["key"];

        return {
          "result": 0,
          "onAfterReturn": {
            setGlobals: {
              "__secret__pat_token": token,
              "__secret__pat_key": key,
            }
          }
        };

  savePatInVault:
    # Parameters:
    #  - __secret__pat_token: PAT token
    #  - __secret__pat_key: PAT key
    - getVaultData
    - vaultSecretSet:
        secretPath: "paas/customers/${globals.organizationName}/paas_${env.shortdomain}/PAT/root"
        __secret__secretData: '{"token":"${this.__secret__pat_token}","key":"${this.__secret__pat_key}"}'

  setPatInJahia:
    # Parameters:
    # __secret__pat_token: the personal access token
    # jahia_version: the jahia version
    - cmd[proc]: |-
        __secret__pat_token=${this.__secret__pat_token}
        groovy_file_path=/data/digital-factory-data/patches/groovy/pat.groovy

        # Clean up any possible remainder of previous script execution
        rm -f ${groovy_file_path}*

        echo """
        org.jahia.services.content.JCRTemplate.getInstance().doExecuteWithSystemSession({ session ->
          org.jahia.osgi.BundleUtils.getOsgiService(\"org.jahia.modules.apitokens.TokenService\")
              .tokenBuilder(\"/users/root\", \"jahia-cloud-token_admin_${env.envName}\", session)
              .setToken(\"$__secret__pat_token\")
              .setActive(true)
              .create()
          session.save();
        })
        """ >> $groovy_file_path
        chown tomcat:tomcat $groovy_file_path

  setPatInHaproxy:
    # Parameters:
    # __secret__pat_token: the personal access token
    - script: |-
        __secret__patToken = "${this.__secret__pat_token}";
        return jelastic.env.control.AddContainerEnvVars(
          '${env.envName}',
          session,
          nodeGroup='bl',
          vars={"jahia_cfg_healthcheck_token": __secret__patToken}
        );

  getPatTokenAndKey:
    # set token value in __secret__pat_token and key value in __secret__pat_key if not already set
    - if ("HideThisLine" && "${globals.__secret__pat_token.print()}" == "" || "${globals.__secret__pat_key.print()}" == ""):
        - getVaultData
        - vaultSecretReadAllKeysB64:
            secretPath: "paas/customers/${globals.organizationName}/paas_${env.shortdomain}/PAT/root"
        - script: |-
            __secret__pat_creds_base64 = "${globals.__secret__vaultSecretData}";
            pat_creds = JSON.parse(java.lang.String(java.util.Base64.getDecoder().decode(__secret__pat_creds_base64)))
            return {
              "result": 0,
              "onAfterReturn": {
                setGlobals: {
                  "__secret__pat_token": pat_creds["token"],
                  "__secret__pat_key": pat_creds["key"]
                }
              }
            };

  checkPatGroovyScriptExecution:
    - cmd[proc]: |-
        groovy_file_path=/data/digital-factory-data/patches/groovy/pat.groovy
        jahia_running_timeout=360 # 6 minutes
        sleep_interval=5

        while [ -f $groovy_file_path ]; do
          sleep $sleep_interval;
          ((jahia_running_timeout-=sleep_interval))
          if [ $jahia_running_timeout -eq 0 ]; then
            echo "[ERROR] $groovy_file_path is still not executed after 360 seconds"
            exit 1
          fi
        done

        rm -f $groovy_file_path.installed

        if [ -f $groovy_file_path.failed ]; then
            echo "[ERROR] pat.groovy execution failed"
            exit 1
        fi

  isAugSearchEnabled:
    # Returns:
    #   ${globals.isAugSearchEnabled}: true if augmented search is enabled, false otherwise
    - script: |-
        const augsearch = jelastic.env.control.GetNodeGroups("${env.envName}", session).object.filter(function (object) {
            return object.name == "cp";
        }).pop().augsearch;

        resp = {"result": 0}
        resp.onAfterReturn = {
          setGlobals: {
            isAugSearchEnabled: (augsearch != null)
          }
        }

        return resp

  getAugSearchConnectionName:
    # Returns:
    #   - globals.augSearchConnectionName : contains the augmented search connection name configured, empty if there is nothing configured
    - getPatTokenAndKey
    - cmd[proc]: |-
        __secret__API_TOKEN="${globals.__secret__pat_token}"
        get_current_as_connection() {
          curl -fLSs http://localhost:8080/modules/graphql \
            -H "Authorization: APIToken $__secret__API_TOKEN" \
            -H 'Origin: http://localhost:8080' \
            -H 'Content-Type: application/json' \
            -d '{"query":"query { admin { search { currentConnection } } }"}' \
            | jq -r '.data.admin.search.currentConnection'
        }
        current_connection=$(get_current_as_connection)
        if [ "$current_connection" != "null" ]; then
          echo $current_connection
        fi

    - setGlobals:
        augSearchConnectionName: ${response.out}

  removeDefaultESConnection:
    # Delete the jahia-cloud_augmented-search connexion from elasticsearch-connector module
    - getPatTokenAndKey
    - cmd[proc]: |-
        __secret__API_TOKEN="${globals.__secret__pat_token}"

        es_connection_name="jahia-cloud_augmented-search"
        result=$(curl -fLSs -XDELETE \
          -H "Authorization: APIToken $__secret__API_TOKEN" \
          -H 'Origin: http://localhost:8080' \
          -H 'Content-Type: application/json' \
          http://localhost:8080/modules/dbconn/elasticsearch/remove/$es_connection_name | jq -r ".success" )
        if [ "$result" != "Successfully removed ElasticSearch connection" ]; then
          echo "Failed to remove the old connection but may be normal, so we can continue"
        fi


  setDefaultESConnection:
    # Set the jahia-cloud_augmented-search connexion from elasticsearch-connector module
    - getPapiInfoAll
    - cmd[proc]: |-
        __secret__API_TOKEN="${globals.__secret__pat_token}"

        __secret__PAPI_TOKEN="${globals.__secret__papiToken}"
        export PAPI_TOKEN="$__secret__PAPI_TOKEN"
        export PAPI_HOSTNAME="${globals.papiHostname}"
        export PAPI_ENV_ID="${globals.papiEnvId}"
        export PAPI_API_VERSION="${globals.papiApiVersion}"

        environment=$(papi.py -X GET "paas-environment/$PAPI_ENV_ID")
        ec_deployment_id=$(echo $environment | jq -r .ec_deployment_id)
        ec_deployment=$(papi.py -X GET "ec-deployment/$ec_deployment_id")

        es_endpoint=$(echo $ec_deployment | jq -r .es_endpoint | sed 's/https:\/\/\(.*\):.*/\1/g')

        es_connection_name="jahia-cloud_augmented-search"

        __secret__elasticsearch_password="${globals.__secret__elasticsearch_password}"

        result=$(curl -fLSs -XPOST \
          -H 'Authorization: APIToken $__secret__API_TOKEN' \
          -H 'Origin: http://localhost:8080' \
          -H 'Content-Type: application/json' \
          -d "{\"id\": \"$es_connection_name\", \"isConnected\": true, \"host\": \"$es_endpoint\", \"port\": 443, \"user\": \"${env.envName}\", \"password\": \"$__secret__elasticsearch_password\", \"options\": {\"useXPackSecurity\": true, \"useEncryption\": true} }" \
          http://localhost:8080/modules/dbconn/elasticsearch/add
        )
        success=$(echo $result | jq -r .success)
        verified=$(echo $result | jq -r .connectionVerified)
        if [ "$success" != "Connection successfully added" ]; then
          echo "Failed to add the connection"
          exit 1
        elif [ "$verified" != "true" ]; then
          echo "The new connection could not be verified"
          exit 1
        fi

  setAugSearchESConnection:
    - getAugSearchConnectionName
    - if ("${globals.augSearchConnectionName}" != "jahia-cloud_augmented-search"):
        cmd[proc]: |-
          __secret__API_TOKEN="${globals.__secret__pat_token}"

          es_connection_name="jahia-cloud_augmented-search"

          response=$(curl -fLSs -XPOST \
            -H "Authorization: APIToken $__secret__API_TOKEN" \
            -H 'Origin: http://localhost:8080' \
            -H 'Content-Type: application/json' \
            -d '{"query":"mutation { admin { search { setDbConnection(connectionId:\"jahia-cloud_augmented-search\") } } }"}' \
            http://localhost:8080/modules/graphql)
            result=$(echo $response | jq -r '.data.admin.search.setDbConnection')
          if [ "$result" != "Successful" ]; then
            echo "Failed to use the $es_connection_name connection for AS: $response"
            exit 1
          fi

  removeAndCleanAugmentedSearch:
    - removeDefaultESConnection
    - uninstallModule:
        moduleSymname: augmented-search
    - cmd[proc, cp]: |-
        rm -f /data/digital-factory-data/karaf/etc/org.jahia.modules.augmentedsearch.cfg

  #######################
  # jexperience related #
  #######################
  getJexperienceVersion:
    # Get jExperience version to install according to jCustomer env version:
    # - jCustomer 1.x ==> jExperience 2.10.1 & jExperienceDashboards 0.3.0
    # - jCustomer 2.x ==>
    #       - < jahia 8.2 jExperience 3.3.1 & jExperienceDashboards 1.0.0
    #       - >= jahia 8.2 jExperience 3.4.0 & jExperienceDashboards 1.0.0
    # Parameters:
    #   unomi_env_name: name of jCustomer environment
    # Returns:
    #   ${globals.jexperienceVersion}: jExperience version to install according to current Jahia / jCustomer version
    #   ${globals.jexperienceDashboardsVersion}: jExperience-Dashboards version to install according to current Jahia / jCustomer version
    - api: env.control.GetContainerEnvVarsByGroup
      envName: ${this.unomi_env_name}
      nodeGroup: cp
    - set:
        unomiVersion: ${response.object.UNOMI_VERSION}
    - isVersionHigherOrEqual:
        a: ${this.unomiVersion}
        b: 2.0.0
        res: isHigherOrEqual200
    - getJahiaVersion
    - isVersionHigherOrEqual:
        a: ${globals.jahiaVersion}
        b: 8.2.0.0
        res: jahia82Plus
    - if( ${globals.isHigherOrEqual200} ):
        - if(${globals.jahia82Plus}):
            setGlobals:
              jexperienceVersion: 3.4.0
              jexperienceDashboardsVersion: 1.0.0
        - else:
            setGlobals:
              jexperienceVersion: 3.3.1
              jexperienceDashboardsVersion: 1.0.0
    - else:
        - if(${globals.jahia82Plus}):
            return:
              type: error
              message: jahia 8.2 is not compatible with jcustomer 1
        - setGlobals:
            jexperienceVersion: 2.10.1
            jexperienceDashboardsVersion: 0.3.0

  getEnvLinkedJcustomer:
    # Parameters:
    #   - envName: jahia env name to fetch info from
    - script: |
        const envName = "${this.envName}";

        unomi_linked = jelastic.env.control.GetNodeGroups(envName, session).object.filter(function (object) {
                                        return object.name == "cp";}).pop().envLink;

        return unomi_linked?
        {"result": 0, value: unomi_linked, "is_linked": true, "out": "Found a linked env"} :
        {"result": 0, value: "none", "is_linked": false, "out": "No unomi env linked"};
    - setGlobals:
        unomi_env_name: ${response.value}
        unomi_env_linked: ${response.is_linked}

  checkJexperienceCfg:
    # Parameters:
    #   - jcustomerDns: jahia env name to fetch info from
    #   - __secret__jcustomerPwdB64: Jcustomer password (base64 encoded)
    - cmd[proc]: |-
        CONFIG_FILE="/data/digital-factory-data/karaf/etc/org.jahia.modules.jexperience.settings-global.cfg"
        URL=$(grep jexperience.jCustomerURL $CONFIG_FILE | cut -d'=' -f2 | sed 's/ //g' |sed 's/https\?...//g')
        if [ "$URL" != "${this.jcustomerDns}" ]; then
          >&2 echo "error"
        fi
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "jCustomer url is wrong."
    - cmd[proc]: |-
        CONFIG_FILE="/data/digital-factory-data/karaf/etc/org.jahia.modules.jexperience.settings-global.cfg"
        PASSWD=$(sed -n 's/\( *jexperience.jCustomerPassword *= *\)\(.*\) *$/\2/p' $CONFIG_FILE | tr -d '\n' | base64)
        __secret__jcustomerPwdB64="${this.__secret__jcustomerPwdB64}"
        if [ "$PASSWD" != "$(echo -n $__secret__jcustomerPwdB64)" ]; then
          >&2 echo "error"
        fi
    - if ("${response.errOut}" != ""):
        - return:
            type: error
            message: "jCustomer password is wrong."

  removeAndCleanJexperience:
    - cmd[proc, cp]: |-
        JCUSTOMER_CONFIG_FILE="/data/digital-factory-data/karaf/etc/org.jahia.modules.jexperience.settings-global.cfg"
        if [ -f $JCUSTOMER_CONFIG_FILE ]; then
         sed -i 's/\(jexperience.jCustomer.*=\).*/\1/g' $JCUSTOMER_CONFIG_FILE
        fi
    - uninstallModule:
        moduleSymname: jexperience
    - environment.nodegroup.ApplyData [proc, cp]:
        data:
          envLink: null
    - removeKibanaAndJexperienceDashboards


  getKibanaEndpointOfJcustomer:
    # Parameters:
    # - jcustomerEnv
    # Returns:
    # - globals.kibanaEndpoint
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/jcustomer/get-kibana-endpoint.yml",
              envName: "${this.jcustomerEnv}"
            }
          );
    - setGlobals:
        kibanaEndpoint: "${response.successText.fromBase64()}"

  setupKibanaDashboard:
    # Parameters
    # roleAndAccountName: kibana role and es account name
    # __secret__password: es account password
    # jcustomerEnv: target jcustomer environment
    - setGlobalRepoRootUrl

    - getJahiaVersion
    - isVersionHigherOrEqual:
        a: ${globals.jahiaVersion}
        b: 8.1.3.0
        res: isHigherOrEqualThan8130
    - if(${globals.isHigherOrEqualThan8130}):
        - isVersionHigherOrEqual:
            a: ${globals.jahiaVersion}
            b: 8.2.0.0
            res: jahia82Plus
        - if(${globals.jahia82Plus}):
            set:
              kibanaDashboardProviderVersion: 1.3.0
        - else:
            set:
              kibanaDashboardProviderVersion: 1.2.0
    - else:
        set:
          kibanaDashboardProviderVersion: 1.1.0

    - set:
        es_permissions: '{"indices": [{"names": ["${this.jcustomerEnv}_*"], "privileges": ["read"]}]}'
        kibana_permissions: '[{"base": ["all"], "feature": {}, "spaces": ["${this.roleAndAccountName}"]}]'
    - installOrUpgradeModules:
        modules: kibana-dashboards-provider/${this.kibanaDashboardProviderVersion}, jexperience-dashboards/${globals.jexperienceDashboardsVersion}
    - getJexperienceVersion:
        unomi_env_name: ${this.jcustomerEnv}
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/create-kibana-space.yml",
              envName: "${this.jcustomerEnv}",
              settings: {
                'spaceName': '${this.roleAndAccountName}',
              }
            }
          );
    - script: |-
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/create-kibana-role.yml",
              envName: "${this.jcustomerEnv}",
              settings: {
                'roleName': '${this.roleAndAccountName}',
                'esPermissions': '${this.es_permissions}',
                'kibanaPermissions': '${this.kibana_permissions}'
              }
            }
          );
    - script: |-
        var __secret__password = "${this.__secret__password}";
        return api.marketplace.jps.Install(
            {
              jps: "${globals.repoRootUrl}/packages/common/create-elasticsearch-account.yml",
              envName: "${this.jcustomerEnv}",
              settings: {
                'accountName': '${this.roleAndAccountName}',
                'password': __secret__password,
                'rolesList': '${this.roleAndAccountName}',
              }
            }
          );
    - getKibanaEndpointOfJcustomer:
        jcustomerEnv: ${this.jcustomerEnv}
    - cmd[proc]: |-
        __secret__password="${this.__secret__password}"
        cfg_file=/data/digital-factory-data/karaf/etc/org.jahia.modules.kibana_dashboards_provider.cfg
        sed -i "s,.*\(kibana_dashboards_provider.kibanaURL\).*,\1=${globals.kibanaEndpoint},g" $cfg_file
        sed -i 's/.*\(kibana_dashboards_provider.kibanaUser\).*/\1=${this.roleAndAccountName}/g' $cfg_file
        sed -i "s/.*\(kibana_dashboards_provider.kibanaPassword\).*/\1=$__secret__password/g" $cfg_file
        sed -i 's/.*\(kibana_dashboards_provider.kibanaSpace\).*/\1=${this.roleAndAccountName}/g' $cfg_file
        sed -i 's/.*\(kibana_dashboards_provider.kibanaProxy.enable\).*/\1=true/g' $cfg_file
        sed -i 's/.*\(kibana_dashboards_provider.kibanaProxy.cloud\).*/\1=true/g' $cfg_file

  createKibanaCustomerUserAccount:
    # Parameters:
    # - accountName: name of the customer's Kibana user
    # - rolesList: comma-separated list of Kibana roles
    # - __secret__password: password of the customer's Kibana user
    # - jcustomerEnv: target jcustomer environment
  - setGlobalRepoRootUrl
  - script: |-
      var __secret__password = "${this.__secret__password}";
      return api.marketplace.jps.Install(
          {
            jps: "${globals.repoRootUrl}/packages/common/create-elasticsearch-account.yml",
            envName: "${this.jcustomerEnv}",
            settings: {
              'accountName': '${this.accountName}',
              'password': __secret__password,
              'rolesList': '${this.rolesList}',
            }
          }
        );

  removeKibanaAndJexperienceDashboards:
    - cmd [proc]: |-
        cfg_file=/data/digital-factory-data/karaf/etc/org.jahia.modules.kibana_dashboards_provider.cfg
        if [ -f $cfg_file ]; then
          sed -i "s,.*\(kibana_dashboards_provider.\)\(kibanaURL\|kibanaUser\|kibanaPassword\|kibanaSpace\).*,\1\2=,g" $cfg_file
          sed -E 's:(^\s*kibana_dashboards_provider\.)(kibana(URL|User|Password|Space)).*:\1\2=:' -i $cfg_file
        fi
    - uninstallModule:
        moduleSymname: kibana-dashboards-provider
    - uninstallModule:
        moduleSymname: jexperience-dashboards

### Vault related actions ###
  vaultGetIPsecConfB64:
    # Returns:
    #   ${globals.__secret__IPsecConfB64}: the IPsec conf, base64 encoded
    - getVaultData
    - vaultSecretReadKeyB64:
        secretPath: paas/customers/${globals.organizationName}/paas_${env.shortdomain}/ipsec/conn-1
        secretKey: conf
    - setGlobals:
        __secret__IPsecConfB64: ${globals.__secret__vaultSecretData}

  vaultGetIPsecPSKB64:
    # Returns:
    #   ${globals.__secret__IPsecPSKB64}: the PSK, base64 encoded
    - getVaultData
    - vaultSecretReadKeyB64:
        secretPath: paas/customers/${globals.organizationName}/paas_${env.shortdomain}/ipsec/conn-1
        secretKey: secret
    - setGlobals:
        __secret__IPsecPSKB64: ${globals.__secret__vaultSecretData}

  getNexusCredentials:
    # Returns:
    #   ${globals.__secret__nexusLogin}: Nexus login
    #   ${globals.__secret__nexusPassword}: Nexus password
    - getVaultData
    - vaultSecretReadAllKeysB64:
        secretPath: "paas/envs-common/nexus"
    - script: |-
        __secret__vaultResponseBase64 = "${globals.__secret__vaultSecretData}";
        vaultResponse = JSON.parse(java.lang.String(java.util.Base64.getDecoder().decode(__secret__vaultResponseBase64)))
        return {
          "result": 0,
          "onAfterReturn": {
            setGlobals: {
              "__secret__nexusLogin": vaultResponse.login,
              "__secret__nexusPassword": vaultResponse.password
            }
          }
        };

  queryShutdownEndpoint:
    # Parameters:
    # - target: target node id
    - getPatTokenAndKey
    - cmd[${this.target}]: |-
        result=$(curl -SsH "Authorization: APIToken ${globals.__secret__pat_token}" \
            -m 180 \
            -X POST \
            "http://localhost:8080/modules/graphql" \
            -H 'Content-Type: application/json' \
            -H "Origin: http://localhost:8080" \
            -d '{"query":"mutation { admin { jahia { shutdown( dryRun: true timeout: 120 force: false ) } } }"}' \
            | jq -r .data.admin.jahia.shutdown)
        echo $result
    - if ("${response.errOut}" != ""):
        - log: "Unable to query SAM shutdown endpoint before stopping the node."
    - elif ("${response.out}" != "true"):
        - cmd[${this.target}]: |-
            tasklist=$(curl -SsH "Authorization: APIToken ${globals.__secret__pat_token}" \
                  -m 180 \
                  -X POST \
                  "http://localhost:8080/modules/graphql" \
                  -H 'Content-Type: application/json' \
                  -H "Origin: http://localhost:8080" \
                  -d '{"query":"query { admin { jahia { tasks { name, service, started } } } }"}' \
                  | jq -r '.data.admin.jahia.tasks | map(.name) | join(", ")' )
            echo $tasklist
        - if ("${response.errOut}" != ""):
            - log: "Unable to list running tasks before stopping."
        - else:
            - log: "The node was stopped while these tasks were running: ${response.out}"

  setupDatadogAgentStorage:
    - log: "## Finalize Datadog agent setup on ${this}"
    - setGlobalRepoRootUrl
    - installLatestDatadogAgent: ${this}
    - cmd[${this}]: |-
        dd_dir=/etc/datadog-agent
        dd_conf_file=$dd_dir/datadog.yaml
        dd_checks_dir=$dd_dir/checks.d
        custom_metrics_check_name=storage_jahia_custom_metrics
        custom_metrics_check_dir=$dd_dir/conf.d/${custom_metrics_check_name}.d
        sudoers=/etc/sudoers.d/jahia-rules
        NODE_NAME=${HOSTNAME/-*}

        echo "hostname: $(echo $_ROLE| sed 's/_//g').${NODE_NAME#node}" >> $dd_conf_file
        sed -i 's/# logs_enabled: false/logs_enabled: true/' $dd_conf_file
        echo "tags:" >> $dd_conf_file
        echo " - product:jahia" >> $dd_conf_file
        echo " - version:${DX_VERSION}" >> $dd_conf_file
        echo " - envname:${env.envName}" >> $dd_conf_file
        echo " - provide:${_PROVIDE}" >> $dd_conf_file
        echo " - role:${_ROLE}" >> $dd_conf_file

        curl -fLSso /usr/local/bin/set_dd_tags.sh ${globals.repoRootUrl}/assets/common/set_dd_tags.sh || exit 1
        curl -fLSso /etc/cron.d/set_dd_tags_cron ${globals.repoRootUrl}/assets/common/set_dd_tags_cron || exit 1
        chmod u+x /usr/local/bin/set_dd_tags.sh

        grep -q JAHIA_DD_AGENT_CHECK $sudoers || \
          sed -e "/Cmnd_Alias JAHIA_MYSQL_SERVICE/a Cmnd_Alias JAHIA_DD_AGENT_CHECK = \/usr\/bin\/du" \
              -e "/mysql ALL/a dd-agent ALL=NOPASSWD: JAHIA_DD_AGENT_CHECK" \
              -i $sudoers

        [ -d $custom_metrics_check_dir ] || mkdir $custom_metrics_check_dir
        curl -fLSso $dd_checks_dir/${custom_metrics_check_name}.py ${globals.repoRootUrl}/assets/storage/${custom_metrics_check_name}.py || exit 1
        curl -fLSso $custom_metrics_check_dir/${custom_metrics_check_name}.yaml-disabled ${globals.repoRootUrl}/assets/storage/${custom_metrics_check_name}.yaml || exit 1
        chown dd-agent: -R $dd_checks_dir $custom_metrics_check_dir
    - if ("${this}" == "${nodes.storage.first.id}") || ("${this}" == "storage"):
        - cmd [${nodes.storage.first.id}]: |-
            conf_file=/etc/datadog-agent/conf.d/storage_jahia_custom_metrics.d/storage_jahia_custom_metrics.yaml
            [ -f $conf_file ] || mv ${conf_file}-disabled $conf_file
            systemctl restart datadog-agent
    - if (nodes.storage.length > 1):
        - forEach(nodes.storage):
            cmd[${@i.id}]: |-
              if [[ ${@i.id} = ${nodes.storage.first.id} ]]; then
                interval=59
              elif [[ ${@i.id} = ${nodes.storage.last.id} ]]; then
                interval=63
              else
                interval=61
              fi
              echo "dd-agent ALL=(ALL) NOPASSWD:/opt/datadog-agent/embedded/sbin/gstatus" > /etc/sudoers.d/dd-agent
              cat > /etc/datadog-agent/conf.d/glusterfs.d/conf.yaml << EOF
              init_config:
              instances:
                -
                  min_collection_interval: $interval
              logs:
                - type: file
                  path: /var/log/glusterfs/glusterd.log
                  source: glusterfs
                  service: glusterfs
                - type: file
                  path: /var/log/glusterfs/glustershd.log
                  source: glusterfs
                  service: glusterfs
                - type: file
                  path: /var/log/glusterfs/data.log
                  source: glusterfs
                  service: glusterfs
              EOF
              log_dir=/var/log/glusterfs
              [ -d $log_dir ] || mkdir $log_dir
              setfacl -m u:dd-agent:rx /var/log/glusterfs
              setfacl -m u:dd-agent:rx /var/log/glusterfs/*.log
              sed -i '/killall.*glusterd/a \  touch \/var\/log\/glusterfs\/glusterd.log && /usr\/bin\/setfacl -m u:dd-agent:rx \/var\/log\/glusterfs\/*.log' /etc/logrotate.d/glusterfs
              systemctl restart rsyslog crond datadog-agent

  dumpModules:
    # Parameters:
    # - operation: type of operation performed
    # - checkVersion: if true, add versions to validation
    - script: |-
        res = {"result": 0}
        res.onAfterReturn = {
          set: {
            nowDate: new Date().toISOString()
          }
        }
        return res
    - if (${this.checkVersion}):
      - callProvisioningAPI:
          target: "proc, cp"
          payload:
            - karafCommand: cluster:bundle-list -s default | shell:exec awk -F " │" "NR>3 {print \$2 \$6 \$7}" > /opt/tomcat/temp/modulesdump-cluster.${this.operation}.${this.nowDate}
            - karafCommand: bundle:list -s | shell:exec awk -F " │" "NR > 3 {print \$2 \$4 \$5}" > /opt/tomcat/temp/modulesdump-local.${this.operation}.${this.nowDate}
    - else:
       - callProvisioningAPI:
          target: "proc, cp"
          payload:
            - karafCommand: cluster:bundle-list -s default | shell:exec awk -F " │" "NR>3 {print \$2 \$7}" > /opt/tomcat/temp/modulesdump-cluster.${this.operation}.${this.nowDate}
            - karafCommand: bundle:list -s | shell:exec awk -F " │" "NR>3 {print \$2 \$5}" > /opt/tomcat/temp/modulesdump-local.${this.operation}.${this.nowDate}
    - cmd[cp, proc]: |-
        sed -i -E "s/Resolved|Installed/Stopped/" /opt/tomcat/temp/modulesdump-*.${this.operation}.${this.nowDate}

  checkModulesAfterOperation:
    # Parameters:
    # - operation: type of operation performed
    # - checkVersion: if true, add versions to validation
    # - (optional) ignoredModules: list of ignored modules (ex: "module1,module2,module3")
    # Dump modules after operation
    - dumpModules:
        operation: ${this.operation}
        checkVersion: ${this.checkVersion}

    # Clean the dumps of ignored modules if any
    - if ("${this.ignoredModules:}" != ""):
        - cmd[cp, proc]: |-
            # Clean the dumps
            clusterdump_before=$(ls -t /opt/tomcat/temp/modulesdump-cluster* | sed -n '2 p')
            clusterdump_after=$(ls -t /opt/tomcat/temp/modulesdump-cluster* | sed -n '1 p')
            localdump_before=$(ls -t /opt/tomcat/temp/modulesdump-local* | sed -n '2 p')
            localdump_after=$(ls -t /opt/tomcat/temp/modulesdump-local* | sed -n '1 p')
            dumps=($clusterdump_before $clusterdump_after $localdump_before $localdump_after)
            for dump in ${dumps[@]}; do
              sed -i -E "/\s+($(echo "${this.ignoredModules}" | tr ',' '|'))$/d" "$dump"
            done

    # Validate before vs after and log any errors
    - cmd[cp, proc]: |-
        clusterdump_before=$(ls -t /opt/tomcat/temp/modulesdump-cluster.${this.operation}* | sed -n '2 p')
        clusterdump_after=$(ls -t /opt/tomcat/temp/modulesdump-cluster.${this.operation}* | sed -n '1 p')
        localdump_before=$(ls -t /opt/tomcat/temp/modulesdump-local.${this.operation}* | sed -n '2 p')
        localdump_after=$(ls -t /opt/tomcat/temp/modulesdump-local.${this.operation}* | sed -n '1 p')

        # If this is an upgrade, only keep active modules
        dumps_before=($clusterdump_before $localdump_before)
        if [[ ${this.operation} == "upgrade" ]]; then
            for dump in ${dumps_before[@]}; do
              sed -i '/Active/!d' "$dump"
            done
        fi

        # Make sure there's no space issues
        dumps=($clusterdump_before $clusterdump_after $localdump_before $localdump_after)
        for dump in ${dumps[@]}; do
          sed -i 's/  */ /g' "$dump"
        done

        result_file=$(mktemp)
        comm -23 <(sort $clusterdump_before) <(sort $clusterdump_after) | awk '{print $NF}' >> $result_file
        comm -23 <(sort $localdump_before) <(sort $localdump_after) | awk '{print $NF}' >> $result_file

        # Log error if any
        if [ -s "$result_file" ]; then
          logfile=/var/log/jelastic-packages/modulescheck.log
          timestamp=$(date +"%Y-%m-%dT%H:%M:%S")
          errormsg="$timestamp - Errors detected in module states after ${this.operation}. Please check the state of the following module(s): $(sort -u $result_file | paste -s -d,)"
          echo '{"level": "ERROR", "message": "'"$errormsg"'" }' >> $logfile
        fi

  callProvisioningAPI:
    # Parameters:
    #  - target: the node group/id that will perform the call
    #  - payload: the request payload. Can have many formats:
    #     - file path of a script ex: '@install_jexperience.yaml'
    #     - yaml/json string  ex: '- karafCommand: "cluster:bundle-list -s default"'
    #     - raw yaml. Example of parameter:
    #         payload:
    #           - installBundle:
    #               - "mvn:org.jahia.modules/addstuff/2.1.0"
    #             autostart: false
    #             uninstallPreviousVersion: false
    #  - curl_extra_args (optional)
    # Returns
    #  - provisioning_api_call_success - true if curl command succeed, false otherwise.
    #     !!!!! IMPORTANT NOTE !!!!!
    #     You won't be noticed if provided commands were ok,
    #     you have to implement a check in the action calling callProvisioningAPI
    - getPatTokenAndKey
    - cmd[${this.target}]: |-
        __secret__API_TOKEN="${globals.__secret__pat_token}"
        output_file=/tmp/provAPIOutput
        # Always set yaml format because it's necessary for yaml, and not taken into account when json is provided...
        http_code=$(curl -sS -o $output_file -w '%{http_code}' \
                    -H "authorization: APIToken $__secret__API_TOKEN" \
                    http://localhost/modules/api/provisioning ${this.curl_extra_args:} \
                    --form script='${this.payload};type=text/yaml')
        if [ $http_code -lt 200 ] || [ $http_code -gt 299 ]; then
          >&2 echo "Curl command returned wrong http code $http_code"
          >&2 cat $output_file
          rm $output_file
        fi
    - if ("${response.errOut}" == ""):
        setGlobals:
          provisioning_api_call_success: true
    - else:
        setGlobals:
          provisioning_api_call_success: false

  backupKarafConfsDir:
    - cmd[${this}]: |-
        DST_FOLDER=/var/tmp/cloud/karaf_confs_history
        SOURCE_FOLDER=/data/digital-factory-data/karaf/etc/
        TIMESTAMP=$(date +"%Y%m%d%H%M%S")
        [ -d $DST_FOLDER ] || sudo -u tomcat mkdir -p $DST_FOLDER
        sudo -u tomcat tar czf ${DST_FOLDER}/${TIMESTAMP}.tgz -C ${SOURCE_FOLDER} .

  checkKarafConfsChecksums:
    - cmd[${this}]: |-
        out_file_prefix=/opt/tomcat/temp/karaf_configuration_files_checksum_
        before_file=$out_file_prefix"onBeforeRestartNode"
        after_file=$out_file_prefix"onAfterRestartNode"
        cd /var/tmp/cloud/karaf_confs_history
        # perform md5sums from the last file saved in backupKarafConfsDir action. Format output and store it in a tmp file
        tar xf $(ls -1t /var/tmp/cloud/karaf_confs_history| head -1) --to-command 'sh -c "md5sum | sed \"s#-#/data/digital-factory-data/karaf/etc/\$(echo \$TAR_REALNAME | sed \"s%^..%%\" )#\""' > $before_file
        find /data/digital-factory-data/karaf/etc/ -type f -exec md5sum "{}" + > $after_file

        if ! $(cmp -s $before_file $after_file); then
          diff_output=$(diff -W1000 -y --suppress-common-lines $before_file $after_file)
          output="{\"level\": \"ERROR\", \"message\": \"$(date +"%Y-%m-%dT%H:%M:%S") - Errors detected in karaf configuration files after restart. Please check the state of the following configuration files:"
          while IFS= read -r diff; do
            file=$(echo $diff |awk '{print $NF}')
            if [ "$file" = "<" ]; then # Case where the file is now missing, the filename is $2 and not $NF
              file=$(echo $diff |awk '{print $2}')
            fi
            output=$output" "$(echo $file | xargs basename)
          done<<<"$diff_output"
          echo "$output\"}" >> /var/log/jelastic-packages/karafConfigFilescheck.log
        fi

  upgradeDxClusteringTo8109:
    - getJahiaVersion
    - isVersionBetween:
        lower: 8.1.3.0
        lower_may_equal: true
        version: ${globals.jahiaVersion}
        higher: 8.1.7.0
        higher_may_equal: false
        res: canProceed
    - if (${globals.canProceed}):
        - checkModule:
            moduleSymname: org.jahia.bundles.clustering
        - isVersionStrictlyLower:
            a: ${globals.runningVersion}
            b: 8.1.0.9
            res: below8109
        - if (${globals.below8109}):
            # Add repos to configuration
            - getNexusCredentials
            - callProvisioningAPI:
                target: proc
                payload:
                  - addMavenRepository: "https://devtools.jahia.com/nexus/content/groups/enterprise@id=jahia-enterprise"
                    username: ${globals.__secret__nexusLogin}
                    password: ${globals.__secret__nexusPassword}
                  - addMavenRepository: "https://devtools.jahia.com/nexus/content/repositories/thirdparty-releases@id=jahia-thirdparty"
                    username: ${globals.__secret__nexusLogin}
                    password: ${globals.__secret__nexusPassword}
            - if (! ${globals.provisioning_api_call_success}):
                return:
                type: error
                message: "Failed to add the repositories"
            # Uninstall old repo and module version
            - callProvisioningAPI:
                target: proc
                payload:
                  - karafCommand: feature:uninstall -r -v dx-clustering
                    timeout: "5000"
            - if (! ${globals.provisioning_api_call_success}):
                return:
                type: error
                message: "Failed to uninstall dx-clustering on processing node"
            - callProvisioningAPI:
                target: proc
                payload:
                  - karafCommand: feature:repo-remove dx-clustering-${globals.runningVersion}
                    timeout: "5000"
            - if (! ${globals.provisioning_api_call_success}):
                return:
                type: error
                message: "Failed to remove repository on processing node"
            - forEach(nodes.cp):
                - callProvisioningAPI:
                    target: ${@i.id}
                    payload:
                      - karafCommand: feature:uninstall -r -v dx-clustering
                        timeout: "5000"
                - if (! ${globals.provisioning_api_call_success}):
                    return:
                    type: error
                    message: "Failed to uninstall dx-clustering on browsing node ${@i.id}"
                - callProvisioningAPI:
                    target: ${@i.id}
                    payload:
                      - karafCommand: feature:repo-remove dx-clustering-${globals.runningVersion}
                        timeout: "5000"
                - if (! ${globals.provisioning_api_call_success}):
                    return:
                    type: error
                    message: "Failed to remove repository on browsing node ${@i.id}"
            # Give Karaf a minute to catch up
            - sleep:
                milliseconds: 60000
            # Install the new repo and module version
            - callProvisioningAPI:
                target: proc
                payload:
                  - karafCommand: "feature:repo-add mvn:org.jahia.bundles/dx-clustering/8.1.0.9/xml/features"
                    timeout: "5000"
            - if (! ${globals.provisioning_api_call_success}):
                return:
                type: error
                message: "Failed to add repository on processing node"
            - callProvisioningAPI:
                target: proc
                payload:
                  - karafCommand: feature:install -r -v dx-clustering
                    timeout: "5000"
            - if (! ${globals.provisioning_api_call_success}):
                return:
                type: error
                message: "Failed to install dx-clustering on processing node"
            - forEach(nodes.cp):
                - callProvisioningAPI:
                    target: ${@i.id}
                    payload:
                      - karafCommand: "feature:repo-add mvn:org.jahia.bundles/dx-clustering/8.1.0.9/xml/features"
                        timeout: "5000"
                - if (! ${globals.provisioning_api_call_success}):
                    return:
                    type: error
                    message: "Failed to add repository on browsing node ${@i.id}"
                - callProvisioningAPI:
                    target: ${@i.id}
                    payload:
                      - karafCommand: feature:install -r -v dx-clustering
                        timeout: "5000"
                - if (! ${globals.provisioning_api_call_success}):
                    return:
                    type: error
                    message: "Failed to install dx-clustering on browsing node ${@i.id}"
            # Perform a rolling restart
            - install:
                jps: "${globals.repoRootUrl}/packages/jahia/jahia-rolling-restart.yml"
            # Avoid a second restart during migration
            - setGlobals:
                jahiaRollingRestartNeeded: false
            # Check that the module is up and running
            - checkModule:
                moduleSymname: org.jahia.bundles.clustering
            - if ("${globals.moduleState}" != "started"):
                - return:
                  type: error
                  message: "dx-clustering is not running after the module upgrade"
            # Refresh the SAM probe on all nodes
            - checkModule:
                moduleSymname: server-availability-manager
            - getPatTokenAndKey
            - cmd[proc, cp]: |-
                __secret__jahia_cfg_healthcheck_token=${globals.__secret__pat_token}
                response=$(curl -SsH "Authorization: APIToken $__secret__jahia_cfg_healthcheck_token" -XPOST -d "" \
                  "localhost/modules/api/bundles/org.jahia.modules/server-availability-manager/${globals.runningVersion}/_refresh" \
                  | jq -r '.message')
                if [ "$response" != "Operation successful" ]; then
                  msg='Error while trying to refresh SAM module, please try again manually'
                  echo $msg > /var/log/jelastic-packages/refreshSamModule.log
                fi
